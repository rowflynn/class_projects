{"backend_state":"running","connection_file":"/tmp/xdg-runtime-user/jupyter/kernel-7d02d687-4bdd-4a85-adf3-f56090c651a3.json","kernel":"python3","kernel_error":"","kernel_state":"idle","kernel_usage":{"cpu":0,"memory":0},"last_ipynb_save":1727229044904,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"required_libs":[]},"trust":true,"type":"settings"}
{"cell_type":"code","end":1727204561251,"exec_count":3,"id":"6fc554","input":"Names: Rowan, Joey, Jyothi","kernel":"python3","no_halt":true,"output":{"0":{"ename":"SyntaxError","evalue":"invalid syntax (768974057.py, line 1)","traceback":["\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_378/768974057.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    Names: Rowan, Joey, Jyothi\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}},"pos":-1,"start":1727204561226,"state":"done","type":"cell"}
{"cell_type":"code","end":1727204564690,"exec_count":4,"id":"8d58cb","input":"import torch","kernel":"python3","metadata":{"execution":{"iopub.execute_input":"2023-08-18T19:32:55.152236Z","iopub.status.busy":"2023-08-18T19:32:55.151500Z","iopub.status.idle":"2023-08-18T19:32:57.051589Z","shell.execute_reply":"2023-08-18T19:32:57.050409Z"},"origin_pos":6,"tab":["pytorch"]},"no_halt":true,"pos":3,"start":1727204561265,"state":"done","type":"cell"}
{"cell_type":"code","end":1727204564780,"exec_count":5,"id":"4215e4","input":"x = torch.arange(12, dtype=torch.float32)\nx","kernel":"python3","metadata":{"execution":{"iopub.execute_input":"2023-08-18T19:32:57.056039Z","iopub.status.busy":"2023-08-18T19:32:57.055276Z","iopub.status.idle":"2023-08-18T19:32:57.089028Z","shell.execute_reply":"2023-08-18T19:32:57.088195Z"},"origin_pos":14,"tab":["pytorch"]},"no_halt":true,"output":{"0":{"data":{"text/plain":"tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.])"},"exec_count":5}},"pos":6,"start":1727204564742,"state":"done","type":"cell"}
{"cell_type":"code","end":1727204564799,"exec_count":6,"id":"0de979","input":"x.numel()","kernel":"python3","metadata":{"execution":{"iopub.execute_input":"2023-08-18T19:32:57.093138Z","iopub.status.busy":"2023-08-18T19:32:57.092473Z","iopub.status.idle":"2023-08-18T19:32:57.098450Z","shell.execute_reply":"2023-08-18T19:32:57.097452Z"},"origin_pos":21,"tab":["pytorch"]},"no_halt":true,"output":{"0":{"data":{"text/plain":"12"},"exec_count":6}},"pos":9,"start":1727204564795,"state":"done","type":"cell"}
{"cell_type":"code","end":1727204564821,"exec_count":7,"id":"11f7cd","input":"x.shape","kernel":"python3","metadata":{"execution":{"iopub.execute_input":"2023-08-18T19:32:57.102194Z","iopub.status.busy":"2023-08-18T19:32:57.101575Z","iopub.status.idle":"2023-08-18T19:32:57.107424Z","shell.execute_reply":"2023-08-18T19:32:57.106501Z"},"origin_pos":24,"tab":["pytorch"]},"no_halt":true,"output":{"0":{"data":{"text/plain":"torch.Size([12])"},"exec_count":7}},"pos":11,"start":1727204564804,"state":"done","type":"cell"}
{"cell_type":"code","end":1727204564864,"exec_count":8,"id":"247073","input":"X = x.reshape(4, -1)\nprint(\"x=\",x)\nprint(\"X=\",X)\nprint(x[4])\nprint(X[0,0])\nX.numel()\nX.shape","kernel":"python3","metadata":{"execution":{"iopub.execute_input":"2023-08-18T19:32:57.111467Z","iopub.status.busy":"2023-08-18T19:32:57.110749Z","iopub.status.idle":"2023-08-18T19:32:57.117759Z","shell.execute_reply":"2023-08-18T19:32:57.116917Z"},"origin_pos":26,"tab":["pytorch"]},"no_halt":true,"output":{"0":{"name":"stdout","text":"x= tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.])\nX= tensor([[ 0.,  1.,  2.],\n        [ 3.,  4.,  5.],\n        [ 6.,  7.,  8.],\n        [ 9., 10., 11.]])\ntensor(4.)\ntensor(0.)\n"},"1":{"data":{"text/plain":"torch.Size([4, 3])"},"exec_count":8}},"pos":13,"start":1727204564845,"state":"done","type":"cell"}
{"cell_type":"code","end":1727204564900,"exec_count":9,"id":"d5063e","input":"#please write code for question 1 here.\nv=torch.arange(2,17,2,dtype=torch.float32)\nw=v.reshape(2,4)\nprint(v)\nprint(w)","kernel":"python3","no_halt":true,"output":{"0":{"name":"stdout","text":"tensor([ 2.,  4.,  6.,  8., 10., 12., 14., 16.])\ntensor([[ 2.,  4.,  6.,  8.],\n        [10., 12., 14., 16.]])\n"}},"pos":17,"slide":"subslide","start":1727204564874,"state":"done","type":"cell"}
{"cell_type":"code","end":1727204564965,"exec_count":10,"id":"4546cc","input":"torch.zeros((2, 3, 4))","kernel":"python3","metadata":{"execution":{"iopub.execute_input":"2023-08-18T19:32:57.122018Z","iopub.status.busy":"2023-08-18T19:32:57.121194Z","iopub.status.idle":"2023-08-18T19:32:57.128294Z","shell.execute_reply":"2023-08-18T19:32:57.127285Z"},"origin_pos":30,"tab":["pytorch"]},"no_halt":true,"output":{"0":{"data":{"text/plain":"tensor([[[0., 0., 0., 0.],\n         [0., 0., 0., 0.],\n         [0., 0., 0., 0.]],\n\n        [[0., 0., 0., 0.],\n         [0., 0., 0., 0.],\n         [0., 0., 0., 0.]]])"},"exec_count":10}},"pos":19,"start":1727204564961,"state":"done","type":"cell"}
{"cell_type":"code","end":1727204565017,"exec_count":11,"id":"ff8748","input":"q=torch.ones((2, 3, 4))\nprint(q.numel())\nprint(q.shape)","kernel":"python3","metadata":{"execution":{"iopub.execute_input":"2023-08-18T19:32:57.132534Z","iopub.status.busy":"2023-08-18T19:32:57.131716Z","iopub.status.idle":"2023-08-18T19:32:57.139029Z","shell.execute_reply":"2023-08-18T19:32:57.138135Z"},"origin_pos":35,"tab":["pytorch"]},"no_halt":true,"output":{"0":{"name":"stdout","text":"24\ntorch.Size([2, 3, 4])\n"}},"pos":21,"start":1727204564980,"state":"done","type":"cell"}
{"cell_type":"code","end":1727204565068,"exec_count":12,"id":"d24057","input":"torch.randn(3, 4)","kernel":"python3","metadata":{"execution":{"iopub.execute_input":"2023-08-18T19:32:57.143051Z","iopub.status.busy":"2023-08-18T19:32:57.142388Z","iopub.status.idle":"2023-08-18T19:32:57.149695Z","shell.execute_reply":"2023-08-18T19:32:57.148813Z"},"origin_pos":40,"tab":["pytorch"]},"no_halt":true,"output":{"0":{"data":{"text/plain":"tensor([[-0.9378,  0.2723, -1.3325,  0.1529],\n        [-0.4410, -0.7499, -0.0120, -0.5835],\n        [-0.7309, -0.0594,  0.7880,  0.3028]])"},"exec_count":12}},"pos":23,"start":1727204565055,"state":"done","type":"cell"}
{"cell_type":"code","end":1727204565092,"exec_count":13,"id":"aec04e","input":"torch.tensor([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])","kernel":"python3","metadata":{"execution":{"iopub.execute_input":"2023-08-18T19:32:57.153567Z","iopub.status.busy":"2023-08-18T19:32:57.153222Z","iopub.status.idle":"2023-08-18T19:32:57.160436Z","shell.execute_reply":"2023-08-18T19:32:57.159548Z"},"origin_pos":45,"tab":["pytorch"]},"no_halt":true,"output":{"0":{"data":{"text/plain":"tensor([[2, 1, 4, 3],\n        [1, 2, 3, 4],\n        [4, 3, 2, 1]])"},"exec_count":13}},"pos":25,"start":1727204565079,"state":"done","type":"cell"}
{"cell_type":"code","end":1727204565106,"exec_count":14,"id":"8e2057","input":"print(\"X=\", X)\nprint(\"X[-1]=\",X[-1])\nprint(\"X[1:3]=\",X[1:4])","kernel":"python3","metadata":{"execution":{"iopub.execute_input":"2023-08-18T19:32:57.164537Z","iopub.status.busy":"2023-08-18T19:32:57.163812Z","iopub.status.idle":"2023-08-18T19:32:57.171699Z","shell.execute_reply":"2023-08-18T19:32:57.170451Z"},"origin_pos":49,"tab":["pytorch"]},"no_halt":true,"output":{"0":{"name":"stdout","text":"X= tensor([[ 0.,  1.,  2.],\n        [ 3.,  4.,  5.],\n        [ 6.,  7.,  8.],\n        [ 9., 10., 11.]])\nX[-1]= tensor([ 9., 10., 11.])\nX[1:3]= tensor([[ 3.,  4.,  5.],\n        [ 6.,  7.,  8.],\n        [ 9., 10., 11.]])\n"}},"pos":27,"start":1727204565100,"state":"done","type":"cell"}
{"cell_type":"code","end":1727204565124,"exec_count":15,"id":"92679f","input":"#write your answer here\n\nX=torch.arange(12)\nX = X.reshape(4,3)\n\n\ny=X[0::2]\nprint(y)\n","kernel":"python3","no_halt":true,"output":{"0":{"name":"stdout","text":"tensor([[0, 1, 2],\n        [6, 7, 8]])\n"}},"pos":29,"slide":"subslide","start":1727204565112,"state":"done","type":"cell"}
{"cell_type":"code","end":1727204565154,"exec_count":16,"id":"73a7d5","input":"X[1, 2] = 17\nX","kernel":"python3","metadata":{"execution":{"iopub.execute_input":"2023-08-18T19:32:57.176047Z","iopub.status.busy":"2023-08-18T19:32:57.175685Z","iopub.status.idle":"2023-08-18T19:32:57.182893Z","shell.execute_reply":"2023-08-18T19:32:57.181890Z"},"origin_pos":52,"tab":["pytorch"]},"no_halt":true,"output":{"0":{"data":{"text/plain":"tensor([[ 0,  1,  2],\n        [ 3,  4, 17],\n        [ 6,  7,  8],\n        [ 9, 10, 11]])"},"exec_count":16}},"pos":31,"start":1727204565132,"state":"done","type":"cell"}
{"cell_type":"code","end":1727204565166,"exec_count":17,"id":"a434cc","input":"X[:2, :] = 11\nX\nL=torch.zeros(10,50)\nL[0, :5]=12\nL[-1,-5:]=12\nprint(L)","kernel":"python3","metadata":{"execution":{"iopub.execute_input":"2023-08-18T19:32:57.186970Z","iopub.status.busy":"2023-08-18T19:32:57.186270Z","iopub.status.idle":"2023-08-18T19:32:57.193303Z","shell.execute_reply":"2023-08-18T19:32:57.192338Z"},"origin_pos":56,"tab":["pytorch"]},"no_halt":true,"output":{"0":{"name":"stdout","text":"tensor([[12., 12., 12., 12., 12.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0., 12., 12., 12., 12., 12.]])\n"}},"pos":33,"start":1727204565159,"state":"done","type":"cell"}
{"cell_type":"code","end":1727204565183,"exec_count":18,"id":"5cffd3","input":"# Write you answer to Question 3 here\n\n\n# Create the array\nZ=torch.arange(1,17)\n# Form the array into the correct shape\nZ=Z.reshape(4,4)\n# Turn the first two rows to '2's\nZ[: 2]=2\n# Turn the final row to '7's\nZ[3:]=7\n# Print the array\nprint(Z)","kernel":"python3","no_halt":true,"output":{"0":{"name":"stdout","text":"tensor([[ 2,  2,  2,  2],\n        [ 2,  2,  2,  2],\n        [ 9, 10, 11, 12],\n        [ 7,  7,  7,  7]])\n"}},"pos":36,"slide":"subslide","start":1727204565178,"state":"done","type":"cell"}
{"cell_type":"code","end":1727204565199,"exec_count":19,"id":"d6c0e4","input":"x=torch.arange(12)\nx[0:8,]=12\n\nprint(x)\ntorch.exp(x)","kernel":"python3","metadata":{"execution":{"iopub.execute_input":"2023-08-18T19:32:57.197301Z","iopub.status.busy":"2023-08-18T19:32:57.196599Z","iopub.status.idle":"2023-08-18T19:32:57.206136Z","shell.execute_reply":"2023-08-18T19:32:57.205188Z"},"origin_pos":61,"tab":["pytorch"]},"no_halt":true,"output":{"0":{"name":"stdout","text":"tensor([12, 12, 12, 12, 12, 12, 12, 12,  8,  9, 10, 11])\n"},"1":{"data":{"text/plain":"tensor([162754.7969, 162754.7969, 162754.7969, 162754.7969, 162754.7969,\n        162754.7969, 162754.7969, 162754.7969,   2980.9580,   8103.0840,\n         22026.4648,  59874.1406])"},"exec_count":19}},"pos":38,"start":1727204565190,"state":"done","type":"cell"}
{"cell_type":"code","end":1727204565220,"exec_count":20,"id":"78c692","input":"x = torch.tensor([1.0, 2, 4, 8])\ny = torch.tensor([2., 2.,3. , 1.])\nprint(\"x=\",x)\nprint(\"y=\",y)\nprint(\"x+y=\",x + y)\nprint(\"x-y=\", x - y)\nprint(\"x*y=\", x * y)\nprint(\"x/y\", x / y)\nprint(\"x**y\",x ** y)","kernel":"python3","metadata":{"execution":{"iopub.execute_input":"2023-08-18T19:32:57.210417Z","iopub.status.busy":"2023-08-18T19:32:57.209741Z","iopub.status.idle":"2023-08-18T19:32:57.219298Z","shell.execute_reply":"2023-08-18T19:32:57.218318Z"},"origin_pos":66,"tab":["pytorch"]},"no_halt":true,"output":{"0":{"name":"stdout","text":"x= tensor([1., 2., 4., 8.])\ny= tensor([2., 2., 3., 1.])\nx+y= tensor([3., 4., 7., 9.])\nx-y= tensor([-1.,  0.,  1.,  7.])\nx*y= tensor([ 2.,  4., 12.,  8.])\nx/y tensor([0.5000, 1.0000, 1.3333, 8.0000])\nx**y tensor([ 1.,  4., 64.,  8.])\n"}},"pos":40,"start":1727204565213,"state":"done","type":"cell"}
{"cell_type":"code","end":1727204565237,"exec_count":21,"id":"6346f4","input":"X = torch.arange(12, dtype=torch.float32).reshape((3,4))\nprint(\"tensor X:\",X)\nY = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\nprint(\"tensor Y:\",Y)\nprint(\"Concatenating X and Y along axis 0, rows\")\nprint(torch.cat((X, Y), dim=0))\nprint(\"Concatenating X and Y along axis 1, columns\")\nprint(torch.cat((X, Y), dim=1))","kernel":"python3","metadata":{"execution":{"iopub.execute_input":"2023-08-18T19:32:57.223534Z","iopub.status.busy":"2023-08-18T19:32:57.222711Z","iopub.status.idle":"2023-08-18T19:32:57.233166Z","shell.execute_reply":"2023-08-18T19:32:57.232145Z"},"origin_pos":71,"tab":["pytorch"]},"no_halt":true,"output":{"0":{"name":"stdout","text":"tensor X: tensor([[ 0.,  1.,  2.,  3.],\n        [ 4.,  5.,  6.,  7.],\n        [ 8.,  9., 10., 11.]])\ntensor Y: tensor([[2., 1., 4., 3.],\n        [1., 2., 3., 4.],\n        [4., 3., 2., 1.]])\nConcatenating X and Y along axis 0, rows\ntensor([[ 0.,  1.,  2.,  3.],\n        [ 4.,  5.,  6.,  7.],\n        [ 8.,  9., 10., 11.],\n        [ 2.,  1.,  4.,  3.],\n        [ 1.,  2.,  3.,  4.],\n        [ 4.,  3.,  2.,  1.]])\nConcatenating X and Y along axis 1, columns\ntensor([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],\n        [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],\n        [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]])\n"}},"pos":44,"start":1727204565228,"state":"done","type":"cell"}
{"cell_type":"code","end":1727204565299,"exec_count":23,"id":"5c5b61","input":"X == Y","kernel":"python3","metadata":{"execution":{"iopub.execute_input":"2023-08-18T19:32:57.237276Z","iopub.status.busy":"2023-08-18T19:32:57.236485Z","iopub.status.idle":"2023-08-18T19:32:57.243133Z","shell.execute_reply":"2023-08-18T19:32:57.242117Z"},"origin_pos":75,"tab":["pytorch"]},"no_halt":true,"output":{"0":{"data":{"text/plain":"tensor([[False,  True, False,  True],\n        [False, False, False, False],\n        [False, False, False, False]])"},"exec_count":23}},"pos":48,"start":1727204565295,"state":"done","type":"cell"}
{"cell_type":"code","end":1727204565315,"exec_count":24,"id":"579e81","input":"X.sum()","kernel":"python3","metadata":{"execution":{"iopub.execute_input":"2023-08-18T19:32:57.247142Z","iopub.status.busy":"2023-08-18T19:32:57.246480Z","iopub.status.idle":"2023-08-18T19:32:57.253117Z","shell.execute_reply":"2023-08-18T19:32:57.252212Z"},"origin_pos":77,"tab":["pytorch"]},"no_halt":true,"output":{"0":{"data":{"text/plain":"tensor(66.)"},"exec_count":24}},"pos":50,"start":1727204565304,"state":"done","type":"cell"}
{"cell_type":"code","end":1727204565329,"exec_count":25,"id":"95af76","input":"a = torch.arange(3).reshape((3, 1))\nb = torch.arange(2).reshape((1, 2))\na, b","kernel":"python3","metadata":{"execution":{"iopub.execute_input":"2023-08-18T19:32:57.256932Z","iopub.status.busy":"2023-08-18T19:32:57.256264Z","iopub.status.idle":"2023-08-18T19:32:57.263823Z","shell.execute_reply":"2023-08-18T19:32:57.262881Z"},"origin_pos":81,"tab":["pytorch"]},"no_halt":true,"output":{"0":{"data":{"text/plain":"(tensor([[0],\n         [1],\n         [2]]),\n tensor([[0, 1]]))"},"exec_count":25}},"pos":53,"start":1727204565322,"state":"done","type":"cell"}
{"cell_type":"code","end":1727204565345,"exec_count":26,"id":"b7bfd2","input":"a + b","kernel":"python3","metadata":{"execution":{"iopub.execute_input":"2023-08-18T19:32:57.267856Z","iopub.status.busy":"2023-08-18T19:32:57.267172Z","iopub.status.idle":"2023-08-18T19:32:57.273497Z","shell.execute_reply":"2023-08-18T19:32:57.272587Z"},"origin_pos":85,"tab":["pytorch"]},"no_halt":true,"output":{"0":{"data":{"text/plain":"tensor([[0, 1],\n        [1, 2],\n        [2, 3]])"},"exec_count":26}},"pos":56,"start":1727204565342,"state":"done","type":"cell"}
{"cell_type":"code","end":1727204565364,"exec_count":27,"id":"459b19","input":"before = id(Y)\nY = Y + X\nid(Y) == before","kernel":"python3","metadata":{"execution":{"iopub.execute_input":"2023-08-18T19:32:57.277697Z","iopub.status.busy":"2023-08-18T19:32:57.277047Z","iopub.status.idle":"2023-08-18T19:32:57.283549Z","shell.execute_reply":"2023-08-18T19:32:57.282613Z"},"origin_pos":87,"tab":["pytorch"]},"no_halt":true,"output":{"0":{"data":{"text/plain":"False"},"exec_count":27}},"pos":61,"start":1727204565360,"state":"done","type":"cell"}
{"cell_type":"code","end":1727204565378,"exec_count":28,"id":"c1ecbd","input":"Z = torch.zeros_like(Y)\nprint(Z)\nprint('id(Z):', id(Z))\nZ[:] = X + Y\nprint('id(Z):', id(Z))","kernel":"python3","metadata":{"execution":{"iopub.execute_input":"2023-08-18T19:32:57.287695Z","iopub.status.busy":"2023-08-18T19:32:57.286964Z","iopub.status.idle":"2023-08-18T19:32:57.293078Z","shell.execute_reply":"2023-08-18T19:32:57.292048Z"},"origin_pos":92,"tab":["pytorch"]},"no_halt":true,"output":{"0":{"name":"stdout","text":"tensor([[0., 0., 0., 0.],\n        [0., 0., 0., 0.],\n        [0., 0., 0., 0.]])\nid(Z): 140573837240224\nid(Z): 140573837240224\n"}},"pos":64,"start":1727204565369,"state":"done","type":"cell"}
{"cell_type":"code","end":1727204565390,"exec_count":29,"id":"6259a5","input":"before = id(X)\nX += Y\nid(X) == before","kernel":"python3","metadata":{"execution":{"iopub.execute_input":"2023-08-18T19:32:57.296911Z","iopub.status.busy":"2023-08-18T19:32:57.296361Z","iopub.status.idle":"2023-08-18T19:32:57.302754Z","shell.execute_reply":"2023-08-18T19:32:57.301805Z"},"origin_pos":97,"tab":["pytorch"]},"no_halt":true,"output":{"0":{"data":{"text/plain":"True"},"exec_count":29}},"pos":66,"start":1727204565382,"state":"done","type":"cell"}
{"cell_type":"code","end":1727204565402,"exec_count":30,"id":"9de854","input":"A = X.numpy()\nB = torch.from_numpy(A)\ntype(A), type(B)","kernel":"python3","metadata":{"execution":{"iopub.execute_input":"2023-08-18T19:32:57.306812Z","iopub.status.busy":"2023-08-18T19:32:57.306088Z","iopub.status.idle":"2023-08-18T19:32:57.312356Z","shell.execute_reply":"2023-08-18T19:32:57.311478Z"},"origin_pos":103,"tab":["pytorch"]},"no_halt":true,"output":{"0":{"data":{"text/plain":"(numpy.ndarray, torch.Tensor)"},"exec_count":30}},"pos":69,"start":1727204565393,"state":"done","type":"cell"}
{"cell_type":"code","end":1727204565412,"exec_count":31,"id":"328eb4","input":"a = torch.tensor([3.5])\na, a.item(), float(a), int(a)","kernel":"python3","metadata":{"execution":{"iopub.execute_input":"2023-08-18T19:32:57.316471Z","iopub.status.busy":"2023-08-18T19:32:57.315825Z","iopub.status.idle":"2023-08-18T19:32:57.322867Z","shell.execute_reply":"2023-08-18T19:32:57.322007Z"},"origin_pos":108,"tab":["pytorch"]},"no_halt":true,"output":{"0":{"data":{"text/plain":"(tensor([3.5000]), 3.5, 3.5, 3)"},"exec_count":31}},"pos":71,"start":1727204565405,"state":"done","type":"cell"}
{"cell_type":"code","id":"8941d1","input":"","pos":74,"type":"cell"}
{"cell_type":"markdown","id":"0f78f6","input":"---\n**EXAMPLE**\nGiven a tensor of size $n$ and target shape ($h$, $w$), we know that $w = n/h$.\n\n---\n\nTo automatically infer one component of the shape,\nwe can place a `-1` for the shape component\nthat should be inferred automatically.\nIn our case, instead of calling `x.reshape(3, 4)`,\nwe could have equivalently called `x.reshape(-1, 4)` or `x.reshape(3, -1)`.\n\n---","pos":15,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"175520","input":"WRITE YOUR ANSWERS HERE IN MARKDOWN\n\nThe dimension of the final matrix will be 3x2. y is only a 1x2 matrix so it will be copied to create a matrix with three identical rows.\nThe resulting matrix will be:\n\n[[11, 22],\n[13, 24],\n[15, 26]]\n\nThe dimension of the final matrix will be 3x2. y is only a 1x2 matrix so it will be copied to create a matrix with three identical rows.\nThe resulting matrix will be:\n\n[[10, 40],\n[30, 80],\n[50, 120]]\n\n","pos":58,"slide":"subslide","state":"done","type":"cell"}
{"cell_type":"markdown","id":"188fb4","input":"","pos":75,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"1a2dc8","input":"## Conversion to Other Python Objects\n","metadata":{"origin_pos":99},"pos":67,"slide":"slide","state":"done","type":"cell"}
{"cell_type":"markdown","id":"1a9c01","input":"#### Method `reshape`\n\nWe can [**change the shape of a tensor\nwithout altering its size or values**],\nby invoking `reshape`.\n\nFor example, we can transform \nour vector `x` whose shape is (12,) \nto a matrix `X`  with shape (3, 4).\nThis new tensor retains all elements\nbut reconfigures them into a matrix.\nNotice that the elements of our vector\nare laid out one row at a time and thus\n`x[3] == X[0, 3]`.\n","metadata":{"origin_pos":25},"pos":12,"slide":"subslide","state":"done","type":"cell"}
{"cell_type":"markdown","id":"1d2873","input":"To (**convert a size-1 tensor to a Python scalar**),\nwe can invoke the `item` function or Python's built-in functions.\n","metadata":{"origin_pos":106},"pos":70,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"37d101","input":"[**If the value of `X` is not reused in subsequent computations,\nwe can also use `X[:] = X + Y` or `X += Y`\nto reduce the memory overhead of the operation.**]\n","metadata":{"origin_pos":95,"tab":["pytorch"]},"pos":65,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"3c06b4","input":"**QUESTION 5)**\n\n![image](question5.png)\n\n","pos":57,"slide":"subslide","state":"done","type":"cell"}
{"cell_type":"markdown","id":"4004ff","input":"This might be undesirable for two reasons.\n* First, we do not want to run around allocating memory unnecessarily all the time. In machine learning, we often have hundreds of megabytes of parameters and update all of them multiple times per second. Whenever possible, we want to perform these updates *in place*.\n* Second, we might point at the  same parameters from multiple variables. If we do not update in place,  we must be careful to update all of these references, lest we spring a memory leak  or inadvertently refer to stale parameters.\n","metadata":{"origin_pos":88},"pos":62,"slide":"subslide","state":"done","type":"cell"}
{"cell_type":"markdown","id":"4354b7","input":"#### Method `cat`\n\nWe can also [***concatenate* multiple tensors,**]\nstacking them end-to-end to form a larger one.\nWe just need to provide a list of tensors\nand tell the system along which axis to concatenate.\n","pos":42,"slide":"subslide","state":"done","type":"cell"}
{"cell_type":"markdown","id":"4461c2","input":"Likewise, we denote *binary* scalar operators,\nwhich map pairs of real numbers\nto a (single) real number\nvia the signature \n$f: \\mathbb{R}\\times \\mathbb{R} \\rightarrow \\mathbb{R}$.\n\nGiven any two vectors $\\mathbf{u}$ \nand $\\mathbf{v}$ *of the same shape*,\nand a binary operator $f$, we can produce a vector\n$\\mathbf{c} $\nby setting $c_i = f(u_i, v_i)$ for all $i$,\nwhere $c_i, u_i$, and $v_i$ are the $i^\\textrm{th}$ elements\nof vectors $\\mathbf{c}, \\mathbf{u}$, and $\\mathbf{v}$.\n\nWe can write $\\mathbf{c}=F(\\mathbf{u},\\mathbf{v})$, were $F: \\mathbb{R}^d\\times \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$ is created by *lifting* the binary scalar function $f$ to a vector operation.\n\nThe common standard arithmetic operators\nfor addition (`+`), subtraction (`-`), \nmultiplication (`*`), division (`/`), \nand exponentiation (`**`)\nhave all been *lifted* to elementwise operations\nfor identically-shaped tensors of arbitrary shape.\n","metadata":{"origin_pos":64},"pos":39,"slide":"subslide","state":"done","type":"cell"}
{"cell_type":"markdown","id":"485531","input":"#### Method `shape`\n\n\n(**We can access a tensor's *shape***) \n(the length along each axis)\nby inspecting its `shape` attribute.\nBecause we are dealing with a vector here,\nthe `shape` contains just a single element\nand is identical to the size.\n","metadata":{"origin_pos":23},"pos":10,"slide":"subslide","state":"done","type":"cell"}
{"cell_type":"markdown","id":"4abbe3","input":"#### Methods `zeros` and `ones`\n\nPractitioners often need to work with tensors\ninitialized to contain all 0s or 1s.\n[**We can construct a tensor with all elements set to 0**] \nand a shape of (2, 3, 4) via the `zeros` function.\n","pos":18,"slide":"subslide","state":"done","type":"cell"}
{"cell_type":"markdown","id":"4d7703","input":"Similarly, we can create a tensor \nwith all 1s by invoking `ones`.\n","metadata":{"origin_pos":33},"pos":20,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"5508b9","input":" ","pos":59,"slide":"subslide","state":"done","type":"cell"}
{"cell_type":"markdown","id":"57fe88","input":"Beyond reading them, (**we can also *write* elements of a matrix by specifying indices.**)\n","metadata":{"origin_pos":50,"tab":["pytorch"]},"pos":30,"slide":"subslide","state":"done","type":"cell"}
{"cell_type":"markdown","id":"5a464e","input":"\n\n## Getting Started\n\n(**To start, we import the PyTorch library.\nNote that the package name is `torch`.**)\n","metadata":{"origin_pos":3,"tab":["pytorch"]},"pos":2,"slide":"slide","state":"done","type":"cell"}
{"cell_type":"markdown","id":"5f1563","input":"**END OF WORKSHEET**\n\nMake sure that you answered all the questions on time. This completed `Jupyter Notebook` will be collected and graded. \n\nOnce the `Jupyter Notebook` is collected it can not be modified.","pos":73,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"63e110","input":"## Saving Memory\n\n[**Running operations can cause new memory to be\nallocated to host results.**]\nFor example, if we write `Y = X + Y`,\nwe dereference the tensor that `Y` used to point to\nand instead point `Y` at the newly allocated memory.\n\n> We can demonstrate this issue with Python's `id()` function,\nwhich gives us the exact address \nof the referenced object in memory.\n\nNote that after we run `Y = Y + X`,\n`id(Y)` points to a different location.\nThat is because Python first evaluates `Y + X`,\nallocating new memory for the result \nand then points `Y` to this new location in memory.\n","metadata":{"origin_pos":86},"pos":60,"slide":"slide","state":"done","type":"cell"}
{"cell_type":"markdown","id":"693092","input":"Fortunately, (**performing in-place operations**) is easy.\nWe can assign the result of an operation\nto a previously allocated array `Y`\nby using slice notation: `Y[:] = <expression>`.\n\nTo illustrate this concept, \nwe overwrite the values of tensor `Z`,\nafter initializing it, using `zeros_like`,\nto have the same shape as `Y`.\n","metadata":{"origin_pos":89,"tab":["pytorch"]},"pos":63,"slide":"subslide","state":"done","type":"cell"}
{"cell_type":"markdown","id":"6bab9d","input":"## Indexing and Slicing\n\nAs with  Python lists,\nwe can access tensor elements \nby indexing (starting with 0).\nTo access an element based on its position\nrelative to the end of the list,\nwe can use negative indexing.\n\nFinally, we can access whole ranges of indices \nvia slicing (e.g., `X[start:stop]`), \nwhere the returned value includes \nthe first index (`start`) *but not the last* (`stop`).\nFinally, when only one index (or slice)\nis specified for a $k^\\textrm{th}$-order tensor,\nit is applied along axis 0.\nThus, in the following code,\n[**`[-1]` selects the last row and `[1:3]`\nselects the second and third rows**].\n","metadata":{"origin_pos":48},"pos":26,"slide":"slide","state":"done","type":"cell"}
{"cell_type":"markdown","id":"731e39","input":"#### Summing a tensor\n\n[**Summing all the elements in the tensor**] yields a tensor with only one element.\n","metadata":{"origin_pos":76},"pos":49,"slide":"subslide","state":"done","type":"cell"}
{"cell_type":"markdown","id":"73a840","input":"## Operations in Tensors\n\nNow that we know how to construct tensors\nand how to read from and write to their elements,\nwe can begin to manipulate them\nwith various mathematical operations.\n\nAmong the most useful of these \nare the *elementwise* operations.\nThese apply a standard scalar operation\nto each element of a tensor.\n\nFor functions that take two tensors as inputs,\nelementwise operations apply some standard binary operator\non each pair of corresponding elements.\n\nWe can create an elementwise function \nfrom any function that maps \nfrom a scalar to a scalar.\n\nIn mathematical notation, we denote such\n*unary* scalar operators (taking one input)\nby the signature \n$f: \\mathbb{R} \\rightarrow \\mathbb{R}$.\nThis just means that the function maps\nfrom any real number onto some other real number.\nMost standard operators, including unary ones like $e^x$, can be applied elementwise.\n","metadata":{"origin_pos":59},"pos":37,"slide":"slide","state":"done","type":"cell"}
{"cell_type":"markdown","id":"74a786","input":"The example below shows what happens when we concatenate\ntwo matrices along rows (axis 0)\ninstead of columns (axis 1).We can see that the first output's axis-0 length ($6$)\nis the sum of the two input tensors' axis-0 lengths ($3 + 3$);\nwhile the second output's axis-1 length ($8$)\nis the sum of the two input tensors' axis-1 lengths ($4 + 4$).","pos":43,"slide":"subslide","state":"done","type":"cell"}
{"cell_type":"markdown","id":"753b98","input":"#### Constructing Tensors Directly\n\nFinally, we can construct tensors by\n[**supplying the exact values for each element**] \nby supplying (possibly nested) Python list(s) \ncontaining numerical literals.\nHere, we construct a matrix with a list of lists,\nwhere the outermost list corresponds to axis 0,\nand the inner list corresponds to axis 1.\n","metadata":{"origin_pos":43},"pos":24,"slide":"subslide","state":"done","type":"cell"}
{"cell_type":"markdown","id":"7bd8c1","input":"Broadcasting produces a larger $3\\times2$ matrix \nby replicating matrix `a` along the columns\nand matrix `b` along the rows\nbefore adding them elementwise.\n\n$$\\text{new } a=\\begin{bmatrix} 0 & 0\\\\ 1  & 1 \\\\ 2 & 2\\end{bmatrix}$$\n$$\\text{ new }b=\\begin{bmatrix} 0 & 1\\\\ 0& 1 \\\\0 & 1\\end{bmatrix}$$\n","pos":55,"slide":"subslide","state":"done","type":"cell"}
{"cell_type":"markdown","id":"7ce84f","input":"> **A tensor represents a (possibly multidimensional) array of numerical values.**\n\nIn the one-dimensional case, i.e., when only one axis is needed for the data,\na tensor is called a *vector*.\n\nWith two axes, a tensor is called a *matrix*.\n\nWith $k > 2$ axes, we drop the specialized names\nand just refer to the object as a $k^\\textrm{th}$-*order tensor*.\n\n![Example of Tensors](tensors.png)","metadata":{"origin_pos":9},"pos":4,"slide":"subslide","state":"done","type":"cell"}
{"cell_type":"markdown","id":"7feeda","input":"`PyTorch` provides a variety of functions \nfor creating new tensors \nprepopulated with values. \n\n#### Method  `arange`\n\nFor example, by invoking `arange(n)`,\nwe can create a vector of evenly spaced values,\nstarting at 0 (included) \nand ending at `n` (not included).\nBy default, the interval size is $1$.\nUnless otherwise specified, \nnew tensors are stored in main memory \nand designated for CPU-based computation.\n","metadata":{"origin_pos":11,"tab":["pytorch"]},"pos":5,"slide":"subslide","state":"done","type":"cell"}
{"cell_type":"markdown","id":"8e4ed5","input":"#### Method `randn`\n\nWe often wish to \n[**sample each element randomly (and independently)**] \nfrom a given probability distribution.\nFor example, the parameters of neural networks\nare often initialized randomly.\nThe following snippet creates a tensor \nwith elements drawn from \na standard Gaussian (normal) distribution\nwith mean 0 and standard deviation 1.\n\n![Normal Distribution](normal.png)","metadata":{"origin_pos":38},"pos":22,"slide":"subslide","state":"done","type":"cell"}
{"cell_type":"markdown","id":"903a46","input":"## Summary\n\nThe tensor class is the main interface for storing and manipulating data in deep learning libraries.\nTensors provide a variety of functionalities including construction routines; indexing and slicing; basic mathematics operations; broadcasting; memory-efficient assignment; and conversion to and from other Python objects.\n","metadata":{"origin_pos":111},"pos":72,"slide":"slide","state":"done","type":"cell"}
{"cell_type":"markdown","id":"90625b","input":"## Broadcasting\n:label:`subsec_broadcasting`\n\nBy now, you know how to perform \nelementwise binary operations\non two tensors of the same shape. \n\nUnder certain conditions,\neven when shapes differ, \nwe can still [**perform elementwise binary operations\nby invoking the *broadcasting mechanism*.**]\n\nBroadcasting works according to \nthe following two-step procedure:\n\n1. expand one or both arrays by copying elements along axes with length 1 so that after this transformation, the two tensors have the same shape;\n2. perform an elementwise operation on the resulting arrays.\n","metadata":{"origin_pos":79},"pos":51,"slide":"slide","state":"done","type":"cell"}
{"cell_type":"markdown","id":"9571c5","input":"**Question 1**\nPlease write code below that will define first the following tensor:\n$$v=[2.0,4.0, 6.0, 8.0, 10.0, 12.0, 14.0, 16.0]$$ and then will reshape it, using the -1 option, into the following tensor:\nthe following matrix:\n$$\nw=\\begin{bmatrix} 2.0 & 4.0 & 6.0 & 8.0\\\\\n10 & 12 & 14 & 16\n\\end{bmatrix}\n$$\nYour code should print $v$ and $w$. You should explore the options of the method `arange` by doing an internet search and use it in the code below.","pos":16,"slide":"subslide","state":"done","type":"cell"}
{"cell_type":"markdown","id":"b4784c","input":"","pos":58.5,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"b60e44","input":"**QUESTION 4)**\nConsider two PyTorch tensors defined as follows:\n$$E1=\\begin{bmatrix} 1 & 2 &3 \\\\ 10 & 11 & 12\\\\ \\end{bmatrix}$$\n$$E2=\\begin{bmatrix} 20 & 22 &23\\\\3 & 4 &5\\end{bmatrix}$$\n\nWrite **Python** code using `PyTorch` operations to perform the following element-wise operations on tensors E1 and E2:\n\n1. coordinatewise Addition (E1+ E1)\n2. coordinatewise Multiplication (E2 * E2)\n3. Coordinatewise Subtraction (E2 - E2)\n4. coordinatewise Division (E1 / E1) \nAdditionally, perform concatenation of tensors E1 and E2 along:\n* Axis 0 (vertical stacking)\n* Axis 1 (horizontal stacking)\nYour code must be commented and must print all the important results.","pos":45,"slide":"subslide","state":"done","type":"cell"}
{"cell_type":"markdown","id":"b968e6","input":"[**Converting to a NumPy tensor (`ndarray`)**], or vice versa, is easy.\nThe torch tensor and NumPy array \nwill share their underlying memory, \nand changing one through an in-place operation \nwill also change the other.\n","metadata":{"origin_pos":101,"tab":["pytorch"]},"pos":68,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"bc46c2","input":"---\n\n**IMPORTANT** : You need to answer the questions on this notebook during class time. \n\nWhen the lesson is over these notebooks will be collected and graded. Once the notebooks are collected you can not modify them anymore.\n\n---\n\n# Data Manipulation\n:label:`sec_ndarray`\n\n> In order to get anything done,  we need some way to store and manipulate data.\n\nGenerally, there are two important things \nwe need to do with data: \n\n* acquire them; \n* and process them once they are inside the computer. \n\nThere is no point in acquiring data  without some way to store it, so to start, let's get our hands dirty with $n$-dimensional arrays,  which we also call *tensors*.\n\n> If you already know the `NumPy` scientific computing package,  this will be a breeze.\n\n\n","metadata":{"origin_pos":1},"pos":0,"slide":"slide","state":"done","type":"cell"}
{"cell_type":"markdown","id":"c52c48","input":"","pos":34,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"c9c85f","input":"#### Constructing Binary Tensors\n\nSometimes, we want to \n[**construct a binary tensor via *logical statements*.**]\nTake `X == Y` as an example.\nFor each position `i, j`, if `X[i, j]` and `Y[i, j]` are equal, \nthen the corresponding entry in the result takes value `1`,\notherwise it takes value `0`.\n","metadata":{"origin_pos":74},"pos":47,"slide":"subslide","state":"done","type":"cell"}
{"cell_type":"markdown","id":"cacb33","input":"Since `a` and `b` are $3\\times1$ \nand $1\\times2$ matrices, respectively,\ntheir shapes do not match up.\n\nSo, for example, we can not add them up....unless we use broadcasting.\n","metadata":{"origin_pos":84},"pos":54,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"d1dee5","input":"**QUESTION 2)**\n\nFor the tensor $X$ above, write code that will extract the array $[0.,1.,2.]$ and the array $[6.,7.,8.]$ using ONLY `reshape` and slicing.","pos":28,"slide":"subslide","state":"done","type":"cell"}
{"cell_type":"markdown","id":"d3d21d","input":"####  Method `numel`\n\nWe can inspect the total number of elements \nin a tensor via its `numel` method.","pos":8,"slide":"subslide","state":"done","type":"cell"}
{"cell_type":"markdown","id":"e65582","input":"> Note that specifying every shape component to `reshape` is redundant.\n\nBecause we already know our tensor's size, we can work out one component of the shape given the rest.\n\n","metadata":{"origin_pos":28},"pos":14,"slide":"subslide","state":"done","type":"cell"}
{"cell_type":"markdown","id":"e86c6c","input":"For all modern deep learning frameworks, the *tensor class* (\n`Tensor` in `PyTorch`) \nresembles NumPy's `ndarray`,\nwith a few killer features added.\n\nFirst, the tensor class supports *automatic differentiation*. Second, it leverages GPUs to accelerate numerical computation, whereas `NumPy` only runs on `CPUs`.\n\nThese properties make neural networks both easy to code and fast to run.","pos":1,"slide":"subslide","state":"done","type":"cell"}
{"cell_type":"markdown","id":"e884c9","input":"**QUESTION 3)**\n\nCreate first the following tensor:\n\n$$\nZ=\\begin{bmatrix} 1 & 2 & 3 & 4 \\\\ 5 & 6 & 7 & 8 \\\\ 9 & 10 & 11 & 12 \\\\ 13 & 14 &15 & 16 \n\\end{bmatrix}\n$$\n\nUsing **only** the tool discussed above, create this tensor so that now it is:\n$$\nZ=\\begin{bmatrix} 2 & 2 & 2  & 2\\\\ 2 & 2& 2 &2 \\\\9 & 10 & 11 & 12\\\\ 7& 7& 7 & 7\n\\end{bmatrix}\n$$\nyour code should print your results for the tensor Z.","pos":35,"slide":"subslide","state":"done","type":"cell"}
{"cell_type":"markdown","id":"ed1ef2","input":"If we want [**to assign multiple elements the same value,\nwe apply the indexing on the left-hand side \nof the assignment operation.**]\nFor instance, `[:2, :]`  accesses \nthe first and second rows,\nwhere `:` takes all the elements along axis 1 (column).\nWhile we discussed indexing for matrices,\nthis also works for vectors\nand for tensors of more than two dimensions.\n","metadata":{"origin_pos":55},"pos":32,"slide":"subslide","state":"done","type":"cell"}
{"cell_type":"markdown","id":"f07cfa","input":"Each of these values is called\nan *element* of the tensor.\nThe tensor `x` contains 12 elements.\n\n\n","metadata":{"origin_pos":18,"tab":["pytorch"]},"pos":7,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"f36e7b","input":"In addition to elementwise computations,\nwe can also perform linear algebraic operations,\nsuch as dot products and matrix multiplications.\nWe will elaborate on these\nin :numref:`sec_linear-algebra`.\n\n","metadata":{"origin_pos":69},"pos":41,"slide":"subslide","state":"done","type":"cell"}
{"cell_type":"markdown","id":"fee16e","input":"### Example\n\nConsider two matrices:\n$$a=\\begin{bmatrix} 0 \\\\ 1 \\\\ 2\\end{bmatrix}$$\n$$b=\\begin{bmatrix} 0 & 1\\end{bmatrix}$$","pos":52,"slide":"subslide","state":"done","type":"cell"}
{"end":1727204565284,"exec_count":22,"id":"7ee647","input":"# Write your answer to Question 4) here\nE1 = torch.arange(1,13).reshape(-1,3)[::3]\nE2 = torch.tensor([[20,22,23],[3,4,5]])\n\n\nprint(E1 + E1)\nprint(E2 * E2)\nprint(E2 - E2)\nprint(E1 / E1)\n\nprint(torch.cat(E1,E2), dim =0)\nprint(torch.cat((E1,E2), dim =1))\n\n","kernel":"python3","no_halt":true,"output":{"0":{"name":"stdout","text":"tensor([[ 2,  4,  6],\n        [20, 22, 24]])\ntensor([[400, 484, 529],\n        [  9,  16,  25]])\ntensor([[0, 0, 0],\n        [0, 0, 0]])\ntensor([[1., 1., 1.],\n        [1., 1., 1.]])\n"},"1":{"ename":"TypeError","evalue":"cat() received an invalid combination of arguments - got (Tensor, Tensor), but expected one of:\n * (tuple of Tensors tensors, int dim, *, Tensor out)\n * (tuple of Tensors tensors, name dim, *, Tensor out)\n","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_378/1497783577.py\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mE1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mE1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mE1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mE2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mE1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mE2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: cat() received an invalid combination of arguments - got (Tensor, Tensor), but expected one of:\n * (tuple of Tensors tensors, int dim, *, Tensor out)\n * (tuple of Tensors tensors, name dim, *, Tensor out)\n"]}},"pos":46,"start":1727204565243,"state":"done","type":"cell"}
{"id":0,"time":1727202909161,"type":"user"}
{"last_load":1725996814738,"type":"file"}