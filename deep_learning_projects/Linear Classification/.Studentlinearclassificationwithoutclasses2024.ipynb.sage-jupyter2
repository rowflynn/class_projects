{"backend_state":"ready","connection_file":"/tmp/xdg-runtime-user/jupyter/kernel-1738afac-9661-47d1-a99b-365f47dc5c62.json","kernel":"python3","kernel_error":"","kernel_state":"idle","kernel_usage":{"cpu":0,"memory":0},"last_backend_state":1734212435848,"last_ipynb_save":1734247993894,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"trust":true,"type":"settings"}
{"cell_type":"code","end":1733425991530,"exec_count":3,"id":"b4fa5d","input":"import torch #Imports the PyTorch library.\nimport torch.nn as nn\n# Imports the `nn` submodule from the PyTorch library as `nn`. This submodule specifically focuses on neural network modules and functionalities, including building blocks like linear layers, convolutional layers, activation functions,\n# and loss functions.\n# By importing it as `nn`, you create a convenient alias for referencing these elements more concisely throughout your code, enhancing readability and maintainability.\nimport torch.optim as optim\n# Imports the `optim` submodule from the PyTorch library as `optim`.This submodule provides optimization algorithms essential for training neural networks. It includes various optimizers like Stochastic Gradient Descent (SGD), Adam, and RMSprop, which are used to iteratively update model parameters based on the calculated gradients during the training process.\n# By importing it as `optim`, you gain access to these optimization tools for effectively training your models and enhancing their performance.\nfrom torchvision import transforms, datasets\n# Imports two submodules from the torchvision library:\n# - `transforms`: This module contains various data transformation functions commonly used in computer vision tasks. These functions are essential for preprocessing images,such as resizing, normalization, and data augmentation, before feeding them into your models.\n# - `datasets`: This module provides access to pre-downloaded and ready-to-use image datasets commonly used for learning and benchmarking visual recognition models. Loading a dataset through this module saves you the effort of manually downloading and formatting the data, allowing you to focus on building and training your models.\n# By importing both submodules together, you gain convenient access to these powerful tools for preparing and managing your computer vision datasets.\nfrom torch.utils.data import DataLoader\n#Imports the `DataLoader` class from the `torch.utils.data` submodule. This class serves as a powerful tool for efficiently loading and managing datasets during the training process. It streamlines the data loading workflow by:\n# - Batching data: It automatically groups samples into batches, which is crucial for feeding data efficiently to your model and utilizing GPU memory effectively.\n# - Shuffling data (optional): It can shuffle the data within each epoch, promoting better generalization by exposing your model to diverse examples in different orders.\n# - Multiprocessing (optional): It can leverage multiple CPU cores or GPUs for parallel data loading, significantly accelerating the training process on hardware capable of such parallelization.\n# By importing `DataLoader`, you gain access to this versatile tool for optimizing your data loading pipeline and accelerating training, especially for large datasets.\nfrom torch.nn.functional import cross_entropy\n# Imports the `cross_entropy` function from the `torch.nn.functional` submodule as `cross_entropy`. This function efficiently calculates the cross-entropy loss, a commonly used loss function in classification tasks. It measures the difference between the predicted probability distribution of your model and the true (one-hot encoded) labels of the data.\n# By importing it as `cross_entropy`, you have a convenient way to define and calculate this loss function within your code, facilitating evaluation and optimization of your classification models.\n# Note that using this function directly allows for more flexibility in customizing the loss calculation compared to using pre-defined loss modules like `nn.CrossEntropyLoss`, although it requires manual management of some aspects like reduction (mean or sum) and weight assignment.\nimport matplotlib.pyplot as plt#for plotting our graphs","kernel":"python3","last":26793,"pos":9,"slide":"subslide","start":1733425977567,"state":"done","type":"cell"}
{"cell_type":"code","end":1733426003127,"exec_count":4,"id":"96e9b7","input":"############################################################\n# Data preparation\n############################################################\nresize = (32, 32)# values of resizing the size of the images in the database\n# Define the transform to rezise the data and transform it into a collection of tensors.\ntrans = transforms.Compose([transforms.Resize(resize), transforms.ToTensor()])\ntraining_data = datasets.FashionMNIST(root=\"./data\", train=True, download=True, transform=trans)# This Download and load the training part of the FashionMNIST dataset \nvalidation_data = datasets.FashionMNIST(root=\"./data\", train=False, download=True, transform=trans)# This Download and load the training part of the FashionMNIST dataset \nbatch_size = 64#This defines the batch size of our training\n#we generate the data loaders for the training and validation data\ntraindataloader = DataLoader(training_data, batch_size, shuffle=True)\nvalidatedataloader = DataLoader(validation_data, batch_size, shuffle=True)\n# The batchs generated by the iterator validatedataloader can be accessed as:\nbatch = next(iter(validatedataloader)) \n#The generated batch is made of two components\ndata, labels=batch\n#data is the collection of all 64 images, only 1 channel and 32 by 32 images\nprint(\"data in batch: \",data.size())\n#labels are the assocaited labels to each data size\nprint(\"data in batch: \",labels.size())","kernel":"python3","last":6470,"output":{"0":{"name":"stdout","text":"data in batch:  torch.Size([64, 1, 32, 32])\ndata in batch:  torch.Size([64])\n"}},"pos":19,"slide":"subslide","start":1733426002883,"state":"done","type":"cell"}
{"cell_type":"code","end":1733426007324,"exec_count":5,"id":"b6ae34","input":"############################################################\n# Data visualization\n############################################################\n#Return text labels for the provided indices.\ndef text_labels(indices):\n    \"\"\"Return text labels.\"\"\"\n    labels = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat',\n              'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']\n    return [labels[int(i)] for i in indices]\n\n#Plot the given image with its corresponding label.\ndef plot_image_label(image, label):\n    # we create and image from the tensor, specifying axis, heigth etc \n    Newimage=image.numpy().transpose((1,2,0))\n    #We update the color palette\n    Newimage=Newimage*255\n    #We resize the image so it is easier for humans to distinguish the object\n    matplotlib.pyplot.figure(figsize=(2,2))\n    matplotlib.pyplot.imshow(Newimage)\n    #we process the associated label.\n    #newlabel=text_label(label.item())\n    list=[label]\n    newlabel=text_labels(list)\n    matplotlib.pyplot.title(f'Label: {newlabel}')\n    matplotlib.pyplot.show()","kernel":"python3","last":4,"pos":22,"slide":"subslide","start":1733426007321,"state":"done","type":"cell"}
{"cell_type":"code","end":1733426009950,"exec_count":6,"id":"1cdd8c","input":"############################################################\n# Example how to retrieve and visualize training data\n############################################################\n#We obtain the next batch from the variable train dataloader\nbatch1=next(iter(traindataloader))\n#batch1 is a list made of 2 elements\nprint(\"size of the item coming from dataloader:\", len(batch1),\": images and labels\")\n#The batch for the training dataloader comes in the form of two tensors: images and labels\nimages1, labels1=batch1\n#We print the dimensions and values of the tensors obtained\nprint(\"Size of the images in the batch:\", images1.size())# 64 images\nprint(\"Size of the labels in the batch:\", labels1.size())# 64 numbers\nprint(\"************************************************************************\")\nprint(\"Here are the contents of the first image of the batch as a tensor:\")\nprint(\"size of the first image of the batch:\", images1[0].size())\nprint(images1[0])\n#We print the corresponding label using the function text_lables. We need first to \n# transform label1[0] into a list.\n#list=[labels1[0]]\nprint(\"Value of the corresponding numerical value of the label\", labels1[0].item())\n# we use the function plot_image_label to visualize the content of images1[0]\nprint(\"Here is the associated image and label\")\nplot_image_label(images1[0],labels1[0])","kernel":"python3","last":223,"output":{"0":{"name":"stdout","text":"size of the item coming from dataloader: 2 : images and labels\nSize of the images in the batch: torch.Size([64, 1, 32, 32])\nSize of the labels in the batch: torch.Size([64])\n************************************************************************\nHere are the contents of the first image of the batch as a tensor:\nsize of the first image of the batch: torch.Size([1, 32, 32])\ntensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         ...,\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.]]])\nValue of the corresponding numerical value of the label 5\nHere is the associated image and label\n"},"1":{"data":{"image/png":"704c75b4316f8acf354f1a81c74042a32489663e","text/plain":"<Figure size 200x200 with 1 Axes>"},"metadata":{"image/png":{"height":219,"width":201}}}},"pos":24,"start":1733426009442,"state":"done","type":"cell"}
{"cell_type":"code","end":1733426015588,"exec_count":7,"id":"c624c0","input":"############################################################\n#Model definition\n############################################################\nnum_inputs = 1024 # Inputs of the neural networtk\nnum_outputs = 10# output of the neural network\n#Define the classification_model\nclassification_model = nn.Sequential(nn.Flatten(), nn.Linear(num_inputs, num_outputs))\n#Initialization phase\nsigma = 0.01#Define standard deviation\n#Initialize weights randomly\nclassification_model[1].weight.data = torch.normal(0, sigma, size=(num_outputs, num_inputs), requires_grad=True)\n#Initialize bias data\nclassification_model[1].bias.data = torch.zeros(num_outputs, requires_grad=True)","kernel":"python3","last":55,"pos":31,"slide":"subslide","start":1733426015518,"state":"done","type":"cell"}
{"cell_type":"code","end":1733426018493,"exec_count":8,"id":"c5a445","input":"############################################################\n# Example Forward pass of model\n############################################################\n#Disable gradient calculator and set neural network to evaluation mode\nwith torch.no_grad():\n    # Input of the neural network\n    input=images1[0]\n    label= labels1[0]# This is not needed, it is just the label associated with the input in the data.\n    print(\"Input image to neural network\")\n    plot_image_label(input,label)\n    print(\"Label value\", label)\n    #input of neural network\n    print(\"input size:\", input.shape)\n    #output of the neural network.\n    output=classification_model(input)# This is the forward pass.\n    print(\"Output of neural network\\n\")\n    print(output)\n    #we can select the highest value of the output using the function argmax\n    # Find the index with the highest value using argmax\n    highest_index = torch.argmax(output)\n    # Print the index\n    print(\"predicted index: \", highest_index)  # Output: 2","kernel":"python3","last":676,"output":{"0":{"name":"stdout","text":"Input image to neural network\n"},"1":{"data":{"image/png":"704c75b4316f8acf354f1a81c74042a32489663e","text/plain":"<Figure size 200x200 with 1 Axes>"},"metadata":{"image/png":{"height":219,"width":201}}},"2":{"name":"stdout","text":"Label value tensor(5)\ninput size: torch.Size([1, 32, 32])\nOutput of neural network\n\ntensor([[ 0.0221,  0.0042, -0.0186, -0.0316,  0.0649, -0.0235,  0.0642, -0.0387,\n         -0.0346,  0.0802]])\npredicted index:  tensor(9)\n"}},"pos":34,"slide":"subslide","start":1733426018226,"state":"done","type":"cell"}
{"cell_type":"code","end":1733426484069,"exec_count":14,"id":"fc3d9c","input":"######################################\n# Loss function\n#####################################\ndef loss(Y_hat, Y):#Y_hat is the predicted probability distribution, Y is the labels\n    Y_hat = Y_hat.reshape((-1, Y_hat.shape[-1]))\n    Y = Y.reshape((-1,))\n    return cross_entropy(Y_hat, Y, reduction=\"mean\")","kernel":"python3","last":6,"pos":48,"slide":"subslide","start":1733426484063,"state":"done","type":"cell"}
{"cell_type":"code","end":1733426967701,"exec_count":18,"id":"019da3","input":"\n# Define the input tensor\ninput_array = torch.tensor([2.0, -3.1, 4.0, 6.0])\n\n# Calculate the softmax\nsoftmax_output = torch.softmax(input_array, dim=0)\n\n# Print the result\nprint(softmax_output)","kernel":"python3","last":2435,"output":{"0":{"name":"stdout","text":"tensor([1.5875e-02, 9.6784e-05, 1.1730e-01, 8.6673e-01])\n"}},"pos":40,"slide":"subslide","start":1733426967696,"state":"done","type":"cell"}
{"cell_type":"code","end":1733428945843,"exec_count":49,"id":"a32c87","input":"############################################################\n# Example using the Loss function\n############################################################\n#We test the function loss with the following batch of tensor values and the associated target\n# labels\noutputs = torch.tensor([[1.0, 1.0, 1.0],\n                       [2.0, 2.0, 2.0],\n                       [3.0, 3.0, 3.0]], dtype=torch.float)\nprint(\"size of  tensor outputs=\", outputs.size())\n#We use the following target of tensor of the corresponding labels of the data \ntargets = torch.tensor([0, 0, 1], dtype=torch.long)#It is important that the\n# dtype here is long.\nprint(\"size of targets=\", targets.size())\nprint(\"Target probability distribution:\",targets)\n# Calculate cross-entropy loss using our newly defined function\nprint(\"Cross-entropy loss:\", loss(outputs, targets))# this loss function value is \n#the average of the loss function for the three elements of the batch","kernel":"python3","last":46,"output":{"0":{"name":"stdout","text":"size of  tensor outputs= torch.Size([3, 3])\nsize of targets= torch.Size([3])\nTarget probability distribution: tensor([0, 0, 1])\nCross-entropy loss: tensor(1.0986)\n"}},"pos":51,"slide":"subslide","start":1733428945834,"state":"done","type":"cell"}
{"cell_type":"code","end":1733429117575,"exec_count":50,"id":"fa710c","input":"############################################################\n# Defining the Optimizer\n############################################################\n# Instantiate an SGD optimizer for training the classification model:\nclassoptimizer = optim.SGD(classification_model.parameters(), lr=0.06)","kernel":"python3","last":8,"pos":54,"start":1733429117562,"state":"done","type":"cell"}
{"cell_type":"code","end":1733430610844,"exec_count":52,"id":"11c1f9","input":"######################################\n# Visualization of loss and accuracy\n#####################################\nwith torch.no_grad():# makes sure to be in evaluation mode \n    # we plot now the graph of total_loss values against the epochs.\n    plt.plot(total_epoch_loss, label='Training Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.show()\n    #We plot now the accuracy of the neural network\n    plt.plot(final_accuracy, label='Training Accuracy')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    plt.show()","kernel":"python3","output":{"0":{"data":{"image/png":"a06f87e3a01dc10a1cff075f1a52e1028482625b","text/plain":"<Figure size 1200x700 with 1 Axes>"},"metadata":{"image/png":{"height":602,"width":1019}}},"1":{"data":{"image/png":"94992d51b5014866a1060efdc9fb791ff428a333","text/plain":"<Figure size 1200x700 with 1 Axes>"},"metadata":{"image/png":{"height":602,"width":996}}}},"pos":59,"slide":"subslide","start":1733430609944,"state":"done","type":"cell"}
{"cell_type":"code","exec_count":16,"id":"3539d6","input":"######################################\n# Example classifier in action\n#####################################\nwith torch.no_grad():# makes sure to be in evaluation mode\n    data, target in validatedataloader # Selects the first batch of the validation data\n    for i in range(0,10):#Read the first 10 data elements of the batch\n        plot_image_label(data[i],target[i])#we plot the ith image of the batch\n        labellist=torch.tensor([target[i]])# we process the label of the ith image as text\n        print(\"target label:\", text_labels(labellist))\n        output = classification_model(data[i])# forward pass for the data in the batch\n        predicted = torch.argmax(output, dim=1)#calculate batch the labels predicted\n        labelpredicted=torch.tensor([predicted])#we transform the number into a tensor becasue the function text_label only accepts tensors as inputs\n        print(\"Label predicted:\",text_labels(labelpredicted))#We calculate the text assocaited with the predicted label\n        print(\"*********************************\")            ","output":{"0":{"data":{"image/png":"bb5608c707a41e9c74ce216c087cd816f044476d","text/plain":"<Figure size 200x200 with 1 Axes>"},"exec_count":16,"metadata":{"image/png":{"height":219,"width":201}},"output_type":"execute_result"},"1":{"name":"stdout","output_type":"stream","text":"target label: ['sandal']\nLabel predicted: ['sandal']\n*********************************\n"},"10":{"data":{"image/png":"4fac12bf0a99f0c20dcd82a45b984ea2b2f7cd69","text/plain":"<Figure size 200x200 with 1 Axes>"},"exec_count":16,"metadata":{"image/png":{"height":219,"width":201}},"output_type":"execute_result"},"11":{"name":"stdout","output_type":"stream","text":"target label: ['sandal']\nLabel predicted: ['sandal']\n*********************************\n"},"12":{"data":{"image/png":"97fc8fe8f4103d8cb1843380efcff0deebfc3a47","text/plain":"<Figure size 200x200 with 1 Axes>"},"exec_count":16,"metadata":{"image/png":{"height":219,"width":201}},"output_type":"execute_result"},"13":{"name":"stdout","output_type":"stream","text":"target label: ['sneaker']\nLabel predicted: ['sneaker']\n*********************************\n"},"14":{"data":{"image/png":"a702047bc853349b91912d5f35f7301f6f24f521","text/plain":"<Figure size 200x200 with 1 Axes>"},"exec_count":16,"metadata":{"image/png":{"height":219,"width":201}},"output_type":"execute_result"},"15":{"name":"stdout","output_type":"stream","text":"target label: ['pullover']\nLabel predicted: ['pullover']\n*********************************\n"},"16":{"data":{"image/png":"badb5e955ddf153d7894c908945b81b00976703f","text/plain":"<Figure size 200x200 with 1 Axes>"},"exec_count":16,"metadata":{"image/png":{"height":219,"width":201}},"output_type":"execute_result"},"17":{"name":"stdout","output_type":"stream","text":"target label: ['shirt']\nLabel predicted: ['shirt']\n*********************************\n"},"18":{"data":{"image/png":"01290e50ad8b74004fcb20dcff797519e7329443","text/plain":"<Figure size 200x200 with 1 Axes>"},"exec_count":16,"metadata":{"image/png":{"height":219,"width":201}},"output_type":"execute_result"},"19":{"name":"stdout","output_type":"stream","text":"target label: ['coat']\nLabel predicted: ['coat']\n*********************************\n"},"2":{"data":{"image/png":"613ba6ad6cd0beedaf4827a8b3040e0490629387","text/plain":"<Figure size 200x200 with 1 Axes>"},"exec_count":16,"metadata":{"image/png":{"height":219,"width":201}},"output_type":"execute_result"},"3":{"name":"stdout","output_type":"stream","text":"target label: ['t-shirt']\nLabel predicted: ['t-shirt']\n*********************************\n"},"4":{"data":{"image/png":"6b8613c330e601aaca6c2d51812a6ff3be670905","text/plain":"<Figure size 200x200 with 1 Axes>"},"exec_count":16,"metadata":{"image/png":{"height":219,"width":201}},"output_type":"execute_result"},"5":{"name":"stdout","output_type":"stream","text":"target label: ['shirt']\nLabel predicted: ['t-shirt']\n*********************************\n"},"6":{"data":{"image/png":"b6294e49d2861ffab7a1424c3e6531815ddfae7b","text/plain":"<Figure size 200x200 with 1 Axes>"},"exec_count":16,"metadata":{"image/png":{"height":219,"width":201}},"output_type":"execute_result"},"7":{"name":"stdout","output_type":"stream","text":"target label: ['shirt']\nLabel predicted: ['shirt']\n*********************************\n"},"8":{"data":{"image/png":"64629284b9131786ddc52e6d814308c165488df9","text/plain":"<Figure size 200x200 with 1 Axes>"},"exec_count":16,"metadata":{"image/png":{"height":219,"width":201}},"output_type":"execute_result"},"9":{"name":"stdout","output_type":"stream","text":"target label: ['sandal']\nLabel predicted: ['ankle boot']\n*********************************\n"}},"pos":62,"slide":"subslide","type":"cell"}
{"cell_type":"code","exec_count":19,"id":"454593","input":"# Evaluation of our neural network on the test set\ncorrect = 0  # Initialize counter for correct predictions\ntotal = 0    # Initialize counter for total test samples\ninput_size = 28 * 28  # Each image is 28x28 pixels\n\n# Disable gradient calculation to save memory and computations during evaluation\nwith torch.no_grad():\n    # Iterate through batches of images and labels in the test set\n    for images, labels in  validatedataloader:\n        #####Forward pass#####\n        output = classification_model(images)#Calculates the output of the neural network with the input from the batch.\n        predicted = torch.argmax(output, dim=1)# Calculate the predicted outputs as labels\n        #Calculate the total numbers of cases when the predicted and target labels are the same,\n        #and add that number to the variable correct.\n        correct += (predicted == labels).sum().item()\n        #Calculate the total number of elements in the batch\n        total += labels.size(0)#updates by adding the total number of elements in the batch\n        #####End Batch Loop#####\n        \n        \nprint(f\"Accuracy of the model on the 10,000 test images: {100 * correct / total} %\")","output":{"0":{"name":"stdout","output_type":"stream","text":"Accuracy of the model on the 10,000 test images: 79.01 %\n"}},"pos":64,"slide":"subslide","type":"cell"}
{"cell_type":"code","exec_count":21,"id":"73761f","input":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass LinearClassificationModel(nn.Module):\n    \"\"\"\n    Linear classification model using nn.Linear.\n    This extends the regression model to output multiple classes.\n    \"\"\"\n    def __init__(self, input_features, output_classes):\n        super(LinearClassificationModel, self).__init__()\n        # Define a fully connected layer\n        self.linear = nn.Linear(input_features, output_classes)\n    \n    def forward(self, X):\n        # Linear forward pass\n        logits = self.linear(X)\n        # Apply softmax to get probabilities\n        probs = F.softmax(logits, dim=1)\n        return probs","pos":70,"slide":"subslide","type":"cell"}
{"cell_type":"code","exec_count":22,"id":"0650de","input":"","output":{"0":{"name":"stdout","output_type":"stream","text":"Output probabilities: "},"1":{"name":"stdout","output_type":"stream","text":"tensor([[0.0532, 0.2344, 0.0668, 0.1017, 0.1219, 0.0494, 0.1422, 0.0405, 0.0665,\n         0.1233],\n        [0.1215, 0.0423, 0.0522, 0.1041, 0.2720, 0.1068, 0.0950, 0.0419, 0.1262,\n         0.0381],\n        [0.0626, 0.1865, 0.1017, 0.0750, 0.0279, 0.1163, 0.0385, 0.2088, 0.1273,\n         0.0554],\n        [0.0784, 0.1875, 0.0560, 0.1378, 0.0556, 0.1331, 0.1191, 0.0854, 0.0711,\n         0.0759],\n        [0.1889, 0.0738, 0.0403, 0.0691, 0.0366, 0.0425, 0.1711, 0.2646, 0.0761,\n         0.0371],\n        [0.1673, 0.0462, 0.1186, 0.1283, 0.0597, 0.0562, 0.1095, 0.0525, 0.2000,\n         0.0618],\n        [0.3445, 0.0731, 0.0964, 0.0433, 0.0289, 0.1095, 0.0611, 0.0803, 0.0675,\n         0.0954],\n        [0.1506, 0.0517, 0.0843, 0.0817, 0.1140, 0.1005, 0.0609, 0.2511, 0.0595,\n         0.0457],\n        [0.0624, 0.0522, 0.1231, 0.0661, 0.0911, 0.1617, 0.0635, 0.1316, 0.0692,\n         0.1791],\n        [0.1335, 0.0746, 0.0577, 0.0760, 0.0504, 0.0579, 0.2582, 0.1185, 0.1044,\n         0.0687],\n        [0.1342, 0.1251, 0.1098, 0.0661, 0.0344, 0.1456, 0.0610, 0.0454, 0.0770,\n         0.2013],\n        [0.0837, 0.1064, 0.0527, 0.0639, 0.0779, 0.1847, 0.0676, 0.1673, 0.0495,\n         0.1461],\n        [0.0483, 0.1125, 0.1057, 0.1748, 0.1498, 0.0924, 0.1307, 0.0734, 0.0822,\n         0.0301],\n        [0.0925, 0.1994, 0.1636, 0.0373, 0.0507, 0.0688, 0.0812, 0.1051, 0.1286,\n         0.0728],\n        [0.1061, 0.1875, 0.1045, 0.0672, 0.1440, 0.0885, 0.0521, 0.0398, 0.1222,\n         0.0881],\n        [0.0910, 0.0416, 0.1891, 0.2218, 0.0329, 0.0819, 0.1094, 0.0583, 0.1050,\n         0.0692],\n        [0.1043, 0.1356, 0.0678, 0.0565, 0.1024, 0.0363, 0.0475, 0.0925, 0.0691,\n         0.2881],\n        [0.1477, 0.1272, 0.0501, 0.0437, 0.1238, 0.0345, 0.1228, 0.1389, 0.1144,\n         0.0969],\n        [0.0933, 0.1198, 0.1158, 0.1052, 0.0745, 0.0738, 0.1557, 0.1052, 0.0967,\n         0.0601],\n        [0.0660, 0.0899, 0.1191, 0.3237, 0.0450, 0.0392, 0.1193, 0.1265, 0.0461,\n         0.0251],\n        [0.0647, 0.2477, 0.1190, 0.1166, 0.0984, 0.1125, 0.0510, 0.0374, 0.0780,\n         0.0747],\n        [0.0292, 0.0593, 0.1433, 0.0808, 0.0737, 0.2380, 0.0805, 0.0596, 0.0472,\n         0.1884],\n        [0.0650, 0.1104, 0.1690, 0.0609, 0.0641, 0.0426, 0.1884, 0.0457, 0.1361,\n         0.1179],\n        [0.0804, 0.2033, 0.0670, 0.0553, 0.0880, 0.0666, 0.1090, 0.1856, 0.1116,\n         0.0332],\n        [0.1031, 0.0483, 0.0371, 0.0896, 0.1915, 0.0934, 0.1776, 0.0427, 0.0765,\n         0.1402],\n        [0.1426, 0.1403, 0.0748, 0.0996, 0.1089, 0.1004, 0.1089, 0.0363, 0.1005,\n         0.0877],\n        [0.0565, 0.0483, 0.0567, 0.0537, 0.0842, 0.1422, 0.0728, 0.2160, 0.0578,\n         0.2120],\n        [0.0549, 0.2598, 0.1247, 0.0698, 0.0500, 0.0628, 0.0386, 0.1609, 0.1388,\n         0.0398],\n        [0.0315, 0.0461, 0.0668, 0.2384, 0.0356, 0.1054, 0.1533, 0.0425, 0.0550,\n         0.2254],\n        [0.0920, 0.1036, 0.0934, 0.0634, 0.1808, 0.2397, 0.0819, 0.0531, 0.0578,\n         0.0342],\n        [0.0471, 0.0972, 0.0764, 0.1230, 0.1219, 0.0799, 0.0708, 0.0824, 0.0536,\n         0.2477],\n        [0.1657, 0.1042, 0.2746, 0.0900, 0.0330, 0.0302, 0.0936, 0.0830, 0.0824,\n         0.0433],\n        [0.0977, 0.1331, 0.0999, 0.0448, 0.1321, 0.0756, 0.0834, 0.0365, 0.2146,\n         0.0824],\n        [0.1614, 0.1088, 0.3145, 0.0922, 0.0406, 0.0411, 0.0735, 0.0693, 0.0528,\n         0.0457],\n        [0.2754, 0.0733, 0.0567, 0.0930, 0.0932, 0.0423, 0.0318, 0.2212, 0.0838,\n         0.0293],\n        [0.1405, 0.1201, 0.0498, 0.0506, 0.0502, 0.0708, 0.2972, 0.1206, 0.0312,\n         0.0691],\n        [0.0498, 0.0951, 0.0705, 0.0233, 0.2391, 0.1632, 0.0861, 0.1384, 0.0913,\n         0.0432],\n        [0.1024, 0.0627, 0.2918, 0.0457, 0.0668, 0.0473, 0.1116, 0.1197, 0.0563,\n         0.0957],\n        [0.1387, 0.1561, 0.0376, 0.1770, 0.1089, 0.0105, 0.1786, 0.0480, 0.1261,\n         0.0184],\n        [0.0848, 0.0134, 0.0479, 0.1624, 0.1103, 0.0788, 0.1443, 0.1148, 0.0708,\n         0.1725],\n        [0.0675, 0.1824, 0.0886, 0.0266, 0.0633, 0.1582, 0.1172, 0.0556, 0.1156,\n         0.1250],\n        [0.2143, 0.2479, 0.0303, 0.0707, 0.0241, 0.0717, 0.0923, 0.0943, 0.0418,\n         0.1126],\n        [0.0656, 0.0866, 0.1257, 0.0545, 0.0672, 0.0660, 0.1039, 0.2025, 0.1870,\n         0.0410],\n        [0.0750, 0.1008, 0.0603, 0.0902, 0.1425, 0.0618, 0.1149, 0.1977, 0.0442,\n         0.1127],\n        [0.0824, 0.0875, 0.0613, 0.1189, 0.0425, 0.0836, 0.0607, 0.2711, 0.1380,\n         0.0538],\n        [0.1341, 0.1386, 0.0641, 0.0848, 0.0551, 0.1296, 0.1299, 0.1109, 0.0619,\n         0.0910],\n        [0.0454, 0.0679, 0.0249, 0.1139, 0.1831, 0.0480, 0.0870, 0.1745, 0.1168,\n         0.1385],\n        [0.1466, 0.1385, 0.0984, 0.1589, 0.1099, 0.0686, 0.0585, 0.0992, 0.0773,\n         0.0441],\n        [0.2542, 0.1457, 0.0782, 0.1107, 0.0637, 0.0361, 0.1161, 0.0204, 0.0455,\n         0.1295],\n        [0.0439, 0.1139, 0.0442, 0.1232, 0.1272, 0.1948, 0.0387, 0.1205, 0.0850,\n         0.1085],\n        [0.0508, 0.1173, 0.0353, 0.1009, 0.0395, 0.0290, 0.2957, 0.0834, 0.1565,\n         0.0917],\n        [0.0650, 0.0289, 0.0570, 0.2045, 0.1005, 0.0915, 0.2715, 0.0364, 0.0825,\n         0.0622],\n        [0.0177, 0.2297, 0.0391, 0.0884, 0.0338, 0.0635, 0.2509, 0.0270, 0.0708,\n         0.1790],\n        [0.1738, 0.0564, 0.1308, 0.1225, 0.0426, 0.0506, 0.0889, 0.0742, 0.1349,\n         0.1253],\n        [0.0359, 0.2688, 0.0787, 0.0627, 0.0363, 0.2187, 0.1726, 0.0487, 0.0486,\n         0.0288],\n        [0.1167, 0.0595, 0.2306, 0.1772, 0.0278, 0.0156, 0.0475, 0.0896, 0.0951,\n         0.1403],\n        [0.1224, 0.2879, 0.0940, 0.0454, 0.0535, 0.1361, 0.0524, 0.0355, 0.1211,\n         0.0518],\n        [0.1483, 0.1532, 0.1052, 0.0480, 0.1122, 0.0664, 0.0447, 0.1064, 0.1318,\n         0.0839],\n        [0.0926, 0.0517, 0.1792, 0.0938, 0.0722, 0.0179, 0.2170, 0.1312, 0.1012,\n         0.0433],\n        [0.2164, 0.1884, 0.1615, 0.0651, 0.0479, 0.0616, 0.0285, 0.0466, 0.1187,\n         0.0653],\n        [0.0625, 0.0985, 0.0379, 0.0604, 0.1622, 0.1007, 0.2073, 0.1560, 0.0682,\n         0.0463],\n        [0.0328, 0.0455, 0.0524, 0.3354, 0.0565, 0.1604, 0.0726, 0.0418, 0.0949,\n         0.1078],\n        [0.0746, 0.0795, 0.1544, 0.0639, 0.0285, 0.0765, 0.1270, 0.1464, 0.1603,\n         0.0890],\n        [0.0607, 0.0788, 0.1795, 0.1484, 0.0575, 0.0498, 0.0860, 0.1194, 0.1280,\n         0.0919]], grad_fn=<SoftmaxBackward0>)\nShape of output: torch.Size([64, 10])\n"}},"pos":74,"slide":"subslide","type":"cell"}
{"cell_type":"code","exec_count":23,"id":"8d20c5","input":"class CrossEntropyLossWrapper:\n    \"\"\"\n    Wrapper class for CrossEntropyLoss.\n    \"\"\"\n    def __init__(self):\n        self.criterion = nn.CrossEntropyLoss()\n    \n    def __call__(self, predictions, targets):\n        return self.criterion(predictions, targets)","pos":79,"slide":"subslide","type":"cell"}
{"cell_type":"code","exec_count":24,"id":"72b7f2","input":"# Create an instance of CrossEntropyLossWrapper\n\n\n# Example predictions (logits) and true labels\n # 64 samples, 10 classes (logits before softmax)\n # 64 true labels in the range [0, 9]\n\n# Calculate the loss\n","output":{"0":{"name":"stdout","output_type":"stream","text":"Cross-entropy loss: 2.69079327583313\n"}},"pos":83,"type":"cell"}
{"cell_type":"code","exec_count":25,"id":"a4bf51","input":"\nclass SGDOptimizer:\n    \"\"\"\n    Wrapper class for SGD optimizer.\n    \"\"\"\n    def __init__(self, model_params, lr=0.01):\n        self.optimizer = torch.optim.SGD(model_params, lr=lr)\n    \n    def zero_grad(self):\n        self.optimizer.zero_grad()\n    \n    def step(self):\n        self.optimizer.step()","pos":87,"type":"cell"}
{"cell_type":"code","exec_count":26,"id":"4c2b47","input":"# Assume 'model' is an instance of a neural network with parameters to optimize\n\n# Create an instance of SGDOptimizer\n\n\n# Example training loop for one batch\n  # Perform forward pass\n # Calculate loss\n\n # Zero the gradients\n  # Backpropagate the gradients\n  # Update the parameters","pos":91,"type":"cell"}
{"cell_type":"code","exec_count":27,"id":"379148","input":"from torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\n\n# Step 1: Data preparation\ntransform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n\ntrain_dataset = datasets.FashionMNIST(root='./data', train=True, transform=transform, download=True)\ntest_dataset = datasets.FashionMNIST(root='./data', train=False, transform=transform, download=True)\n\ntrain_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\ntest_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)\n\n# Step 2: Define model, loss, and optimizer\ninput_size = 28 * 28  # Each image is 28x28 pixels\nnum_classes = 10  # 10 classes in FashionMNIST\n\nmodel = LinearClassificationModel(input_size, num_classes)\nloss_fn = CrossEntropyLossWrapper()\noptimizer = SGDOptimizer(model.parameters(), lr=0.01)\n\n# Step 3: Training loop\nepochs = 5\nfor epoch in range(epochs):\n    total_loss = 0.0\n    for images, labels in train_loader:\n        images = images.view(-1, input_size)  # Flatten images\n        \n        # Forward pass\n        outputs = model(images)\n        loss = loss_fn(outputs, labels)\n        \n        # Backward pass and optimization\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item()\n    #we append the average loss to the list of epoch losses\n    total_epoch_loss.append(epoch_loss)\n    \n    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {total_loss/len(train_loader)}\")\n    \n# Step 4: Evaluation on test set\ncorrect = 0\ntotal = 0\nwith torch.no_grad():\n    for images, labels in test_loader:\n        images = images.view(-1, input_size)\n        outputs = model(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\nprint(f\"Accuracy of the model on the 10,000 test images: {100 * correct / total} %\")","output":{"0":{"name":"stdout","output_type":"stream","text":"Epoch [1/5], Loss: 1.9454757168348917\n"},"1":{"name":"stdout","output_type":"stream","text":"Epoch [2/5], Loss: 1.8205210187796081\n"},"2":{"name":"stdout","output_type":"stream","text":"Epoch [3/5], Loss: 1.7951230845217512\n"},"3":{"name":"stdout","output_type":"stream","text":"Epoch [4/5], Loss: 1.7826595516092996\n"},"4":{"name":"stdout","output_type":"stream","text":"Epoch [5/5], Loss: 1.724891846113876\n"},"5":{"name":"stdout","output_type":"stream","text":"Accuracy of the model on the 10,000 test images: 77.22 %\n"}},"pos":95,"type":"cell"}
{"cell_type":"code","exec_count":30,"id":"9ac314","input":"######################################\n# Visualization of loss and accuracy\n#####################################\nwith torch.no_grad():# makes sure to be in evaluation mode \n    # we plot now the graph of total_loss values against the epochs.\n    plt.plot(total_epoch_loss, label='Training Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.show()","output":{"0":{"data":{"image/png":"9304185635f9d74156e4a54f798718ff4a69fe74","text/plain":"<Figure size 1200x700 with 1 Axes>"},"exec_count":30,"metadata":{"image/png":{"height":602,"width":1019}},"output_type":"execute_result"}},"pos":96,"type":"cell"}
{"cell_type":"code","exec_count":53,"id":"06e8b1","input":"######################################\n# Training\n#####################################\n#####Initializing parameters#####\nmax_epochs = 5\ntotal_epoch_loss=[]\nfinal_accuracy=[]\nfor epoch in range(max_epochs):# Iterate for every epoch\n    ###########Initialization batch loop#################\n    correct = 0# Intializes the counter of the number of correct predictions in the epoch\n    total = 0## stores total number of elements in the batch\n    total_loss=0.0#Total loss per epoch\n    for data, target in traindataloader:#Go over all the batches\n        classoptimizer.zero_grad()#ensures that the new gradients calculated in the current iteration accurately reflects\n    #the loss at the current parameter values.\n        #####Forward pass#####\n        output = classification_model(data)#Calculates the output of the neural network with the input from the batch.\n        #####Calculate loss#####\n        batch_loss = loss(output, target)#Calculate the entropy loss for this batch\n        #####Backpropagation and optimization#####\n        batch_loss.backward()#Performs the calculation of the gradient of the function batch_loss\n        classoptimizer.step()# updates the values of the weights using the results of the gradient\n        #####Updates values of total errors and # of correct predictions#####\n        predicted = torch.argmax(output, dim=1)# Calculate the predicted outputs as labels\n        #Calculate the total numbers of cases when the predicted and target labels are the same,\n        #and add that number to the variable correct.\n        correct += (predicted == target).sum().item()\n        #Calculate the total number of elements in the batch\n        total += target.size(0)#updates by adding the total number of elements in the batch\n        total_loss+=batch_loss.item()#Updates the error from this batch\n        #####End Batch Loop#####\n    #######We calculate now the average loss and the accuracy for the epoch#######\n    epoch_loss= total_loss/len(traindataloader)\n    #we append the average loss to the list of epoch losses\n    total_epoch_loss.append(epoch_loss)\n    # Calculate and print epoch accuracy\n    epoch_accuracy = 100. * correct / total\n    final_accuracy.append(epoch_accuracy)\n    print(f\"Epoch: [{epoch+1}/{max_epochs}] | Training Accuracy: {epoch_accuracy:.2f}%\")\n    #print the epoch error\n    print(\"Epoch error:\", epoch_loss)\n    ###########End of epoch loop#################","kernel":"python3","output":{"0":{"name":"stdout","text":"Epoch: [1/50] | Training Accuracy: 84.68%\nEpoch error: 0.44641929374002953\n"},"1":{"name":"stdout","text":"Epoch: [2/50] | Training Accuracy: 84.92%\nEpoch error: 0.4394832910981768\n"},"10":{"name":"stdout","text":"Epoch: [11/50] | Training Accuracy: 85.65%\nEpoch error: 0.41709958236100575\n"},"11":{"ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[53], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\u001b[38;5;66;03m## stores total number of elements in the batch\u001b[39;00m\n\u001b[1;32m     12\u001b[0m total_loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m\u001b[38;5;66;03m#Total loss per epoch\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data, target \u001b[38;5;129;01min\u001b[39;00m traindataloader:\u001b[38;5;66;03m#Go over all the batches\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     classoptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\u001b[38;5;66;03m#ensures that the new gradients calculated in the current iteration accurately reflects\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#the loss at the current parameter values.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m#####Forward pass#####\u001b[39;00m\n","File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n","File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n","File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/mnist.py:146\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    143\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img\u001b[38;5;241m.\u001b[39mnumpy(), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 146\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    149\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n","File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n","File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[1;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:176\u001b[0m, in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    174\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mpermute((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mByteTensor):\n\u001b[0;32m--> 176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_float_dtype\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdiv(\u001b[38;5;241m255\u001b[39m)\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]},"2":{"name":"stdout","text":"Epoch: [3/50] | Training Accuracy: 85.08%\nEpoch error: 0.4366028039599024\n"},"3":{"name":"stdout","text":"Epoch: [4/50] | Training Accuracy: 85.10%\nEpoch error: 0.43207618058808067\n"},"4":{"name":"stdout","text":"Epoch: [5/50] | Training Accuracy: 85.26%\nEpoch error: 0.4292257860430014\n"},"5":{"name":"stdout","text":"Epoch: [6/50] | Training Accuracy: 85.29%\nEpoch error: 0.42661761050857205\n"},"6":{"name":"stdout","text":"Epoch: [7/50] | Training Accuracy: 85.54%\nEpoch error: 0.42241306075536367\n"},"7":{"name":"stdout","text":"Epoch: [8/50] | Training Accuracy: 85.48%\nEpoch error: 0.42044029486522494\n"},"8":{"name":"stdout","text":"Epoch: [9/50] | Training Accuracy: 85.55%\nEpoch error: 0.4197475165128708\n"},"9":{"name":"stdout","text":"Epoch: [10/50] | Training Accuracy: 85.62%\nEpoch error: 0.41827124596309306\n"}},"pos":57,"slide":"subslide","state":"done","type":"cell"}
{"cell_type":"markdown","id":"0085ea","input":"We can illustrate the precision of this classification tool by visualizing the accuracy of the trained model for a few training images and comparing the calculated labels with the true labels.","pos":61,"slide":"subslide","type":"cell"}
{"cell_type":"markdown","id":"015cc1","input":"**VARIABLES**\n\n| Variable Name   | Data Type   | Nature of Variable  |\n|------------|------------|------------|\n|` traindataloader` | `DataLoader` | Data Loader for the data |\n|` image1` | `torch.Tensor` | Data preparation |\n| `num_inputs` | `int`| Model Definition: number of inputs |\n|  `num_outputs` | `int`| Model Definition: number of outputs |\n|  `classification_model` | `torch.nn.Sequential` | Model Definition: The model itself |\n|  `loss` | `float` | The cross entropy loss function |\n|  `classoptimizer` | `optim.SGD` | stochastic gradient optimizer variable |\n|  `lr` | `float` | learning rate for the optimization process |","pos":55,"slide":"subslide","type":"cell"}
{"cell_type":"markdown","id":"0223b9","input":"### Testing the Neural Network\n\nWe tested te accuracy of our neural network with respect to the training data. THis should not be done. \n\n > The accuracy of the neural network must be tested with a different data set from the one used for training.\n\nFor the Fashion MNist data sets, we have a testing data set available.","pos":63,"slide":"slide","type":"cell"}
{"cell_type":"markdown","id":"06ed77","input":"**VARIABLES**\n\n| Variable Name   | Data Type   | Nature of Variable  |\n|------------|------------|------------|\n|` traindataloader` | `DataLoader` | Data Loader for the data |\n|` image1` | `torch.Tensor` | Data preparation |\n| `num_inputs` | `int`| Model Definition: number of inputs |\n|  `num_outputs` | `int`| Model Definition: number of outputs |\n|  `classification_model` | `torch.nn.Sequential` | Model Definition: The model itself |\n|  `loss` | `float` | The cross entropy loss function |\n|  `classoptimizer` | `optim.SGD` | stochastic gradient optimizer variable |\n|  `lr` | `float` | learning rate for the optimization process |\n|  `max_epochs` | `int` | Number of epochs |","pos":60,"slide":"subslide","type":"cell"}
{"cell_type":"markdown","id":"0f0d1a","input":"Observe that the evaluation accuracy is less than the one obtained with the training set.","pos":65,"slide":"subslide","type":"cell"}
{"cell_type":"markdown","id":"10c281","input":"## Data Visualization\n\nThe categories of Fashion-MNIST have human-understandable names. The following convenience method `text_labels` converts between numeric labels and their names.\n\nWe also define a method `plot_image_label` that takes the tensor of the images obtained from  the iterator `traindataloaded` and visualize them using the library `matplotlib`.\n","pos":21,"slide":"subslide","type":"cell"}
{"cell_type":"markdown","id":"13eb26","input":"## The Softmax\n:label:`subsec_softmax_operation`\n\nWe are interested in **predictions** for linear classification model, so given the output $\\mathbf{o}=(o_1,o_2,\\ldots,0_q)$ we want:\n\n1. to transforms the outputs $\\mathbf{o}$ into $\\mathbf{p}$, a  **probability distributions**, that is,  a collection of probabilities that tells us what are the probabilities of belonging to each class. \n\n2. Then we want to use these probabilities to decide which one of the label is the one being predicted.\n\nHow to achieve these steps?","pos":36,"slide":"subslide","type":"cell"}
{"cell_type":"markdown","id":"266c8e","input":"Here are some examples of popular datasets included in the `torchvision.datasets` module:\n\n* MNIST: A dataset of handwritten digits\n\n* CIFAR-10: A dataset of 60,000 32x32 color images in 10 classes\n\n* ImageNet: A large-scale dataset of over 15 million images in over 20,000 classes\n\nThe `torchvision.datasets` module is a valuable resource for computer vision practitioners. It provides a wide range of pre-processed datasets and makes it easy to load and use data for training and evaluating machine learning models.\n\nWe already imported that module with the line:\n\n```python \nfrom torchvision import transforms, datasets\n```","pos":14,"slide":"subslide","type":"cell"}
{"cell_type":"markdown","id":"274f21","input":"**EXAMPLE**\n\n* Does this email belong in the spam folder or the inbox?\n* Is this customer more likely to sign up\n  or not to sign up for a subscription service?\n* Does this image depict a donkey, a dog, a cat, or a rooster?\n* Which movie is Aston most likely to watch next?\n* Which section of the book are you going to read next?\n","pos":2,"slide":"subslide","type":"cell"}
{"cell_type":"markdown","id":"2be2c8","input":"**QUESTION 2**:\n\nWrite a PyTorch code snippet that demonstrates how to declare, initialize, and use the `LinearClassificationModel` class for a dataset with 784 input features and 10 output classes.","pos":73,"slide":"subslide","type":"cell"}
{"cell_type":"markdown","id":"2edfef","input":"### Transforming the Outputs into Probabilities\n\nFirst, we need to decide how to make the outputs $\\mathbf{o}$ of the linear network into probability distribution $\\mathbf{p}$. \n\nRecall:\n\n---\n\n**PROBABILITY DISTRIBUTION**\n\nA finite probability distribution are vectors $\\mathbf{p}=(p_1,p_2,\\ldots, p_n)$ such that:\n1. Every $p_i$ is a number between 0 and 1 \n2. $\\sum_{i=1}^n p_i=1$.\n\n---\n* So for example, the output $(-1,2,5)$ is not a set of probability distributions, nor is the output $(0.1, 0.3, 0.3)$ since the sum of the probabilities is not one.","pos":37,"slide":"subslide","type":"cell"}
{"cell_type":"markdown","id":"326e15","input":"**QUESTION 3**:\n\nExplain how the *softmax* function is used in the `LinearClassificationModel` class and what its role is in classification tasks.\n","pos":76,"slide":"subslide","type":"cell"}
{"cell_type":"markdown","id":"35a0ad","input":"## Loading the Dataset\n\nSince the Fashion-MNIST dataset is so useful, all major frameworks provide preprocessed versions of it. We can download and read it into memory using built-in framework utilities.\n\nWe can access the data in the  Fashion-MNIST data set using the class `torchvision.datasets` in **Pytorch**.\n\n#### The `torchvision.datasets`\n\nIn **PyTorch**, the `torchvision.datasets` module provides a collection of built-in datasets for computer vision tasks. These datasets are pre-processed and ready to be used for training and evaluating machine learning models. The module also includes utility classes for building custom datasets.\n","pos":12,"slide":"subslide","type":"cell"}
{"cell_type":"markdown","id":"37eda0","input":"Consequently, our weights constitute a $10\\times1024$ matrix plus a $10\\times1$ row vector for the biases.\n\nWe are going to construct this neural network in **Pytorch**, using the class `nn.Sequential`.","pos":27,"slide":"subslide","type":"cell"}
{"cell_type":"markdown","id":"38dadc","input":"**VARIABLES**\n\n| Variable Name   | Data Type   | Nature of Variable  |\n|------------|------------|------------|\n|` traindataloader` | `DataLoader` | Data Loader for the data |\n|` image1` | `torch.Tensor` | Data preparation |\n| `num_inputs` | `int`| Model Definition: number of inputs |\n|  `num_outputs` | `int`| Model Definition: number of outputs |\n|  `classification_model` | `torch.nn.Sequential` | Model Definition: The model itself |","pos":32,"type":"cell"}
{"cell_type":"markdown","id":"41bb01","input":"## Solving the FashionMNIST Problem\n\nYou can now use the classes defined above and train them on `FashionMNIST` following the same structure as in the training of the linear regresson networks with classes.","pos":94,"type":"cell"}
{"cell_type":"markdown","id":"41e63e","input":">  Note, however, that the outputs $\\mathbf{o}=(o_0,o_1,o_2)$ could be of the form $(1,0,0)$ which corresponds to the label $0$, but what if  $\\mathbf{0}=(0.3,-2.1, 0.4)$? how do we quantify the error between $(0.3, -2.1,0.4)$ and $(1,0,0)$? \n\nWe will discuss how to measure this error later in this notebook.\n","pos":5,"slide":"subslide","type":"cell"}
{"cell_type":"markdown","id":"42b052","input":"\n**DEFINITION**\n\n**Cross-entropy** is a measure of the difference between two probability distributions for a given random variable or set of events.\n\n**Cross entropy** loss is a metric used in machine learning to measure how well a classification model performs. The loss (or error) is measured as a number between 0 and 1, with 0 being a perfect model. The goal is generally to get your model as close to 0 as possible.\n\n\n* **Cross entropy** loss measures the difference between the discovered probability distribution of a machine learning classification model and the predicted distribution. \n\n* All possible values for the prediction are stored so, for example, if you were looking for the odds in a coin toss, it would store that information at 0.5 and 0.5 (heads and tails).\n\nGiven a probability distribution  $\\hat{p}=(\\hat{p}_1,\\hat{p}_2,\\ldots, \\hat{p}_n)$ obtained via **Softmax** from the running of a neural network over a set of inputs, and a target probability distribution $\\mathbf{l}=(l_1,l_2,\\ldots,l_n)$, the **Cross-Entropy loss** is given by:\n$$\nL=\\sum_{i} l_i\\log(\\hat{p}_i).\n$$\n\n---\n\n","pos":42,"slide":"subslide","type":"cell"}
{"cell_type":"markdown","id":"42c4cd","input":"\n\n* *Interpretability*: `nn.Sequential` models are relatively interpretable due to their clear layer-based structure. This interpretability allows for understanding the decision-making process of the model and identifying potential biases or overfitting issues.\n\n> In summary, `nn.Sequential` models offer a balance between simplicity, flexibility, efficiency, and interpretability, making them a preferred choice for implementing linear classification problems. Their ease of use and adaptability allow for rapid prototyping and optimization, while their interpretability facilitates model evaluation and refinement.\n\n>  As with linear regression, we initialize the weights `W` with Gaussian noise. The biases are initialized as zeros.\n","pos":29,"slide":"subslide","type":"cell"}
{"cell_type":"markdown","id":"42d7fa","input":"**QUESTION 9**:\n\nWhy do we call `zero_grad` before calling `backward` in the optimization process?","pos":92,"type":"cell"}
{"cell_type":"markdown","id":"443bc5","input":"\n### The nn.Sequential class\n\n`nn.Sequential` models are well-suited for implementing linear classification problems due to their simplicity and flexibility. They are particularly effective for tasks involving a fixed number of input features and a single output, such as classifying images into predefined categories.\n\n* *Simplicity*: `nn.Sequential` models are constructed by sequentially stacking layers, making them straightforward to design and implement. This simplicity allows for easy experimentation with different layer combinations and hyperparameters to optimize model performance.\n\n* *Flexibility*: `nn.Sequential` models can accommodate a variety of layer types, including linear layers, activation functions, and pooling layers. This flexibility enables the creation of models tailored to specific classification tasks. For instance, adding more linear layers can increase model complexity to capture more intricate patterns in the data.\n\n* *Efficiency*: `nn.Sequential` models are computationally efficient, particularly for smaller datasets. Their straightforward structure simplifies forward and backward propagation during training, reducing computational overhead compared to more complex network architectures.","pos":28,"slide":"subslide","type":"cell"}
{"cell_type":"markdown","id":"4515e3","input":"# Data Preparation\n\n## The Image Classification Dataset\n\nOne widely used dataset for image classification is the  [MNIST dataset](https://en.wikipedia.org/wiki/MNIST_database) :cite:`LeCun.Bottou.Bengio.ea.1998` of handwritten digits. \n\n* At the time of its release in the 1990s it posed a formidable challenge to most machine learning algorithms, consisting of 60,000 images of $28 \\times 28$ pixels resolution (plus a test dataset of 10,000 images). \n\n* To put things into perspective, back in 1995, a Sun SPARCStation 5 with a whopping 64MB of RAM and a blistering 5 MFLOPs was considered state of the art equipment for machine learning at AT&T Bell Laboratories.\n\n* Achieving high accuracy on digit recognition was a key component in automating letter sorting for the USPS in the 1990s. Deep networks such as LeNet-5 :cite:`LeCun.Jackel.Bottou.ea.1995`, support vector machines with invariances :cite:`Scholkopf.Burges.Vapnik.1996`, and tangent distance classifiers :cite:`Simard.LeCun.Denker.ea.1998` all could reach error rates below 1%. \n","pos":10,"slide":"slide","type":"cell"}
{"cell_type":"markdown","id":"4517e4","input":"### Transforming the Dataset\n\nBefore accessing the dataset, we need to **transform** it to suit our needs.\n\n#### The `transforms` submodule of `torchvision`\n\nThe `transforms` module in `torchvision` provides a wide range of built-in transformations for various image processing tasks. These transformations can be used to resize, crop, augment, and normalize images, making them suitable for training computer vision models.\n","pos":15,"slide":"subslide","type":"cell"}
{"cell_type":"markdown","id":"483369","input":"This will output a tensor of size [64, 10] representing the probability distribution over 10 classes for each of the 64 input samples.","pos":75,"slide":"subslide","type":"cell"}
{"cell_type":"markdown","id":"49c718","input":"### Class `LinearClassificationModel`\n\n\nWe'll reuse the structure of the `LinearRegressionModel` but adapt it for classification.","pos":69,"slide":"subslide","type":"cell"}
{"cell_type":"markdown","id":"4bf686","input":"Even as we pivot towards classification, most of the plumbing remains the same: \n\n1. loading the data, \n    \n2. passing it through the model,\n\n3. generating output, \n\n4. calculating the loss, \n\n5. taking gradients with respect to weights, and updating the model.\n\n>  However, the precise form of the targets, the parametrization of the output layer, and the choice of loss function will adapt to suit the classification setting.\n\nIn this section, we focus on *classification* problems where we put aside *how much?* questions and instead focus on *which category?* questions.","pos":1,"slide":"subslide","type":"cell"}
{"cell_type":"markdown","id":"4d5ddb","input":"### The Theory Behind the Cross-Entropy Loss\n\n> Using **cross-entropy**, we can measure the error (or difference) between two probability distributions. \n\nFor example, in the case of Binary Classification problem with two classification values with probabilities $0$ and $1$, in other words with  $\\mathbf{l}=(l_1,l_2)=(0,1)$, and the output of a neural network, transformed into a probability distribution $\\hat{p}=(p_1,p_2)$, the  **cross-entropy** is given by:\n$$\nL=(l_1\\log(p_1)+l_2\\log(p_2)).\n$$\n\nIn this case, \n$$L=(0\\log(p_1)+1\\log(p_2))$$\n$$L=\\log(p_2)$$\n\n\n","pos":43,"slide":"subslide","type":"cell"}
{"cell_type":"markdown","id":"4ee7ca","input":"\n### The code\n\nThe code below makes the training data set and the evaluation dataset for Fashion-MNIST accesible to our neural network. \n\nSince our training method will be the Stochastic Gradient Descent method, we need to define the batch size for the dataloader variables.\n\nStudy it with care.","pos":18,"slide":"subslide","type":"cell"}
{"cell_type":"markdown","id":"56c10c","input":"\nWe now want to compare that vector $\\hat{p}=$`softmax(output_data)` ( a sofmax generated probability distribution) with a target probability distribution with label class $1$. This corresponds to a tensor probability ditribution $y=$ `target=[0,1,0]`. We calculate the Cross Entropy loss using the formula:\n$$\nL=\\sum_{i} y_i\\log(\\hat{y}_i),\n$$\nas follows:\n$$\nL=-\\left(0\\ln(\\frac{1}{3})+0\\ln(\\frac{1}{3})+1.0\\ln (\\frac{1}{3})\\right)=-\\ln(\\frac{1}{3})=1.0986.\n$$\nSimilarly, if the target label has value $0$, the the corresponding tensor probability distribution is $y=$ `target=[1,0,0]` and the Cross Entropy Loss is given by:\n$$\nL=-\\left(1.0\\ln(\\frac{1}{3})+0\\ln(\\frac{1}{3})+0\\ln (\\frac{1}{3})\\right)=-\\ln(\\frac{1}{3})=1.0986.\n$$\n","pos":46,"slide":"subslide","type":"cell"}
{"cell_type":"markdown","id":"59670e","input":"### Class `SGDOptimizer`\n\nWe can reuse the `SGDoptimizer` class from the linear regresssion classes without modification.","pos":86,"type":"cell"}
{"cell_type":"markdown","id":"5c6b40","input":"Here is an example of visualizing the data in the iterator `traindataloader`.","pos":23,"slide":"subslide","type":"cell"}
{"cell_type":"markdown","id":"5d1132","input":"The `torchvision.transforms.Compose()` function is used to create a list of data transformations. In this case, two transformations are applied: resizing the images to the specified resize dimensions and converting the images to tensors.\n\nWe already imported the submodule transform with the line:\n\n```python \nfrom torchvision import transforms, datasets\n```\n\n","pos":17,"slide":"subslide","type":"cell"}
{"cell_type":"markdown","id":"6105f6","input":"**QUESTION 5**:\n\nWrite a PyTorch code snippet that demonstrates how to declare, initialize, and use the `CrossEntropyLossWrapper` class to calculate the loss between predicted outputs and target labels.","pos":82,"type":"cell"}
{"cell_type":"markdown","id":"6537bd","input":"**END OF WORKSHEET**\n\nMake sure that you answered all the questions on time. This completed `Jupyter Notebook` will be collected and graded. \n\nOnce the `Jupyter Notebook` is collected it can not be modified.","pos":97,"slide":"subslide","type":"cell"}
{"cell_type":"markdown","id":"653c0e","input":"Answer: \n\n","pos":77,"slide":"subslide","type":"cell"}
{"cell_type":"markdown","id":"753ea8","input":"* For over a decade, MNIST served as *the* point of reference for comparing machine learning algorithms. \n\n* While it had a good run as a benchmark dataset, even simple models by today's standards achieve classification accuracy over 95%, making it unsuitable for distinguishing between strong models and weaker ones. \n\n* Even more, the dataset allows for *very* high levels of accuracy, not typically seen in many classification problems. This skewed algorithmic development towards specific families of algorithms that can take advantage of clean datasets, such as active set methods and boundary-seeking active set algorithms.\n\n* Today, MNIST serves as more of a sanity check than as a benchmark. ImageNet :cite:`Deng.Dong.Socher.ea.2009` poses a much  more relevant challenge.\n\n* Unfortunately, ImageNet is too large for many of the examples and illustrations in this book, as it would take too long to train to make the examples interactive. \n\n* As a substitute we will focus our discussion in the coming sections on the qualitatively similar, but much smaller Fashion-MNIST dataset :cite:`Xiao.Rasul.Vollgraf.2017` which was released in 2017. It contains images of 10 categories of clothing at $28 \\times 28$ pixels resolution.","pos":11,"slide":"subslide","type":"cell"}
{"cell_type":"markdown","id":"770f04","input":"# The Loss Function\n\nAs mentioned before, defining the loss function for a classification problem is different from defining it for a regression problem. \n\nTo understand the differences, we need to introduce some key concepts.","pos":35,"slide":"slide","type":"cell"}
{"cell_type":"markdown","id":"7b0092","input":"\nOne way to accomplish this goal (and to ensure nonnegativity) is to use an exponential function $P(y = i) \\propto \\exp o_i$ (transforms each one of the outputs into a positive number) and a process of normalization (dividing every value by the sum of all the values).\n\nPutting these two pieces together gives us the *softmax* function:\n$$\n\\hat{\\mathbf{p}}=(\\hat{p}_1, \\hat{p}_2,\\ldots, \\hat{p}_n) =\n$$\n$$\n\\mathrm{softmax}(\\mathbf{o}) \\quad \\textrm{where}\\quad  \\hat{p}_i = \\frac{\\exp(o_i)}{\\sum_j \\exp(o_j)}. \\tag{eq-softmax-y-and-o}\n$$\n\n* Note that the largest coordinate of $\\mathbf{o}$ corresponds to the most likely class according to $\\hat{\\mathbf{p}}$.\n\n* Moreover, because the `softmax` operation preserves the ordering among its arguments, we do not need to compute the `softmax` to determine which class has been assigned the highest probability. \n\nThus,\n$$\n\\operatorname*{argmax}_j \\hat p_j = \\operatorname*{argmax}_j o_j.\n$$\n\n\nThe idea of a softmax dates back to :citet:`Gibbs.1902`, who adapted ideas from physics.","pos":38,"slide":"subslide","type":"cell"}
{"cell_type":"markdown","id":"7ba110","input":"# Defining the Optimizer\nIn this step, we decide of to optimize our model. We will use the Stochastic Gradient Descent method, and will define the value of the parameter $lr$ to be $0.06$.","pos":53,"slide":"slide","type":"cell"}
{"cell_type":"markdown","id":"7efea0","input":"**QUESTION 8**:\n\nWrite a PyTorch code snippet that demonstrates how to declare, initialize, and use the `SGDOptimizer` class to optimize a model.","pos":90,"type":"cell"}
{"cell_type":"markdown","id":"81cd81","input":"## Definition of  the Classes\n","pos":68,"slide":"subslide","type":"cell"}
{"cell_type":"markdown","id":"86f512","input":"Answer: The `SGDOptimizer` class has:\n\n","pos":89,"type":"cell"}
{"cell_type":"markdown","id":"91806f","input":"Once our loss function is defined we illustrate how to use it. ","pos":50,"slide":"subslide","type":"cell"}
{"cell_type":"markdown","id":"95aca1","input":"**Question 6**:\n\nWhat is the purpose of using cross-entropy loss in classification tasks?\n","pos":84,"type":"cell"}
{"cell_type":"markdown","id":"98376f","input":"Answer: \n\n","pos":85,"type":"cell"}
{"cell_type":"markdown","id":"98e6cd","input":"### Linear Model\n\nIn order to define a neural network for our classification problems, we need a model with multiple outputs, as many as (number of labels -1).\n\nIn our case, since we have 4 inputs and 3 possible output categories,\nwe need 12 scalars to represent the weights ($w$ with subscripts),\nand 3 scalars to represent the biases ($b$ with subscripts). This yields:\n\n$$\n\\begin{aligned}\no_1 &= x_1 w_{11} + x_2 w_{12} + x_3 w_{13} + x_4 w_{14} + b_1,\\\\\no_2 &= x_1 w_{21} + x_2 w_{22} + x_3 w_{23} + x_4 w_{24} + b_2,\\\\\no_3 &= x_1 w_{31} + x_2 w_{32} + x_3 w_{33} + x_4 w_{34} + b_3.\n\\end{aligned}\n$$\n\nThe corresponding neural network diagram\nis shown in :numref:`fig_softmaxreg`.\n\nJust as in linear regression,\nwe use a single-layer neural network.\n\nAnd since the calculation of each output, $o_1, o_2$, and $o_3$,\ndepends on every input, $x_1$, $x_2$, $x_3$, and $x_4$,\nthe output layer can also be described as a *fully connected layer*.\n\n![Softmax regression is a single-layer neural network.](../img/softmaxreg.svg)\n:label:`fig_softmaxreg`\n","pos":6,"slide":"subslide","type":"cell"}
{"cell_type":"markdown","id":"9adbbc","input":"**VARIABLES**\n\n| Variable Name   | Data Type   | Nature of Variable  |\n|------------|------------|------------|\n|` traindataloader` | `DataLoader` | Data Loader for the data |\n|` image1` | `torch.Tensor` | Data preparation |","pos":25,"type":"cell"}
{"cell_type":"markdown","id":"9ae627","input":"# Training a Neural Network in Linear Classification \n\n> Now that you have worked through all of the mechanics of simple neural networks, you are ready to apply the skills you have learned to broader kinds of tasks.\n","pos":0,"slide":"slide","type":"cell"}
{"cell_type":"markdown","id":"9b5a3c","input":"Recall the classes that we defined for the linear regression problem:\n\n\n| Class Name | Purpose|\n| -------- | ------- |\n| `LinerRegressionDataset` | Data Generation |\n| `LinearRegressionModel` | Linear Model |\n| `MSELossWrapper` | Loss Function |\n| `SGDOptimizer` | Optimizer |\n\nUsing these classes,  we will create the following classes for a linear classifier:\n\n`ClassificationDataset`: This class will handle the dataset for classification, similar to LinearRegressionDataset.\n\n`ClassificationModel`: This will inherit from nn.Module, and handle the structure of the neural network, using a fully connected layer for classification.\n\n`CrossEntropyLossWrapper`: A wrapper around the nn.CrossEntropyLoss function for classification loss.\n\n`SGDOptimizer:` The same class can be reused for handling optimization during training.\n","pos":67,"slide":"subslide","type":"cell"}
{"cell_type":"markdown","id":"9f4827","input":"**QUESTION 4**:\n\nHow many methods and fields does the `CrossEntropyLossWrapper` class have? What is the purpose of each?\n","pos":80,"slide":"subslide","type":"cell"}
{"cell_type":"markdown","id":"a4158d","input":"> **NOTE**: Observe that the output of the pytorch function `cross-entropy` use in our definition of the function `los` is a tensor of dimension 1, not a number.\n\n**VARIABLES**\n\n| Variable Name   | Data Type   | Nature of Variable  |\n|------------|------------|------------|\n|` traindataloader` | `DataLoader` | Data Loader for the data |\n|` image1` | `torch.Tensor` | Data preparation |\n| `num_inputs` | `int`| Model Definition: number of inputs |\n|  `num_outputs` | `int`| Model Definition: number of outputs |\n|  `classification_model` | `torch.nn.Sequential` | Model Definition: The model itself |\n|  `loss` | `float` | The cross entropy loss function |\n","pos":52,"slide":"subslide","type":"cell"}
{"cell_type":"markdown","id":"a8f141","input":"Answer: \n\n","pos":72,"slide":"subslide","type":"cell"}
{"cell_type":"markdown","id":"ab6559","input":"Study the following code with care.\nIt defines our model as a linear classification model using Pytorch.\n\nNote in particular the line:\n```Python\nclassification_model = nn.Sequential(nn.Flatten(), nn.Linear(num_inputs, num_outputs))\n```\n---\n<details>\n  <summary><b>Detailed Explanation of the Code Below</b></summary>\nHere it is:\n    <ol>\n        <li> `nn.Sequential()`. Purpose: This container allows you to chain together a sequence of layers or operations. The output of one layer becomes the input for the next.\nIn this case: Two layers are defined inside the nn.Sequential container: nn.Flatten() and nn.Linear(num_inputs, num_outputs).\n    \t<li>`nn.Flatten()`. Purpose: The `nn.Flatten()` layer reshapes the input tensor to a 1D tensor (a flat vector). It's typically used to prepare multi-dimensional input data (like images) for a fully connected layer.\nInput and output: If the input is a batch of images (e.g., `shape (batch_size, channels, height, width))`, `nn.Flatten()` will convert it to shape (batch_size, channels * height * width). This transformation ensures that the subsequent `nn.Linear` layer can process the data.\n    \t<li>V `nn.Linear(num_inputs, num_outputs)`. Purpose: This is a fully connected layer (also called a dense layer). It performs a linear transformation of the input data using learned weights and biases.\nnum_inputs: The number of input features (i.e., the size of the flattened input).\nnum_outputs: The number of output features, which usually corresponds to the number of classes in a classification task.\n    </ol>\n  \n\n</details>\n\n---\n","pos":30,"slide":"subslide","type":"cell"}
{"cell_type":"markdown","id":"af8766","input":"## Brief Explanation of the Classification Problems\n:label:`subsec_classification-problem`\n\nTo get our feet wet, let's start with a simple image classification problem. \n\n---\n\n**THE PROBLEM** \n\nHere, each input consists of a $2\\times2$ grayscale image. \n* We can represent each pixel value with a single scalar, giving us four features $\\mathbf{x}=(x_1, x_2, x_3, x_4)$. \n* Further, let's assume that each image belongs to one among the categories \"cat\", \"chicken\", and \"dog\".\n\n* We can imagine that the training dataset is made of data items of the form $(\\mathbf{x}, l)$, where $\\mathbf{x}$ is the image and $l$ is the label that can be $0,1$ or $2$ and corresponds to the categories \"cat\", \"chicken\", \"dog\".\n\n* We can imagine that our neural network takes imputs of the form $\\mathbf{x}$ and outputs vectors $\\mathbf{o}=(o_0,o_2,o_3)$.\n\n---","pos":4,"slide":"slide","type":"cell"}
{"cell_type":"markdown","id":"b5347b","input":"* The value of the loss $L$ thus depends on the probability $p_2$ obtained from the neural network. \n\n* Therefore, our loss function will reward the model for giving a correct prediction (high value of $p_2$, i.e $p_2$ close to 1) with a low loss. However, if the probability is lower, the value of the error will be high (bigger negative value), and therefore it penalizes the model for a wrong outcome.\n\n<div class=\"alert alert-block alert-warning\">\n<b>OBSERVATION</b> If $\\hat{\\mathbf{p}}$ is a probability vector then for every single entry of $\\hat{p}$ we have that $0\\leq p_i\\leq 1$, which implies that $0\\leq -\\log(p_i)$. \n    Note also that $L(\\mathbf{l}, \\hat{\\mathbf{p}}) = 0$ only if we predict\nthe actual label with\"certainty\" ($p_i=1$).This can never happen for any finite setting of the weights\nbecause taking a softmax output towards $1$\nrequires taking the corresponding input $o_i$ to infinity\n(or all other outputs $o_j$ for $j \\neq i$ to negative infinity).\nEven if our model could assign an output probability of $0$,\nany error made when assigning such high confidence\nwould incur infinite loss ($-\\log 0 = \\infty$).\n</div>","pos":44,"slide":"subslide","type":"cell"}
{"cell_type":"markdown","id":"b679d1","input":"###  Cross-Entropy\n\nNow that we have a mapping from features $\\mathbf{x}$\nto probabilities $\\mathbf{\\hat{p}}$, using **Softmax**, \nwe need a way measure the error between the output probability distribution $\\mathbf{\\hat{p}}$ and the labels coming from the training data.\n\nThis requires us to defined a loss function for the probabilities observed ($\\mathbf{\\hat{p}}$) compared to the probabilities given by the data ($\\mathbf{l}$). We call the probabilities on the data and the probabilities obtained via Softmax **probability distributions**.\n\n<div class=\"alert alert-block alert-info\">\n<b>NOTE</b> The output of the linear classification model, $\\mathbf{o}$, before they are transformed into probability distributions, are called **logits**. See <a href=\"https://datascience.stackexchange.com/questions/31041/what-does-logits-in-machine-learning-mean\">here</a>\n\n</div>\n","pos":41,"slide":"subslide","type":"cell"}
{"cell_type":"markdown","id":"b7a414","input":"Answer: \n\n","pos":81,"slide":"subslide","type":"cell"}
{"cell_type":"markdown","id":"bbf620","input":"## Structure of the Linear Classification Process\n\nAs in the previous case, our Linear Classification Process is made of the following steps:\n\n1. Data Preparation\n2. Model Definition\n3. The Loss Function\n4. Defining the Optimizer\n5. Training the Model\n\nIn the rest of this notebook we will discuss and implement each one of these steps.\n\nWe begin by importing the required modules.","pos":8,"slide":"slide","type":"cell"}
{"cell_type":"markdown","id":"bdab69","input":"### Class `CrossEntropyLossWrapper`\n\nWe'll create a simple wrapper for the `CrossEntropyLoss`, similar to the loss wrapper from the regression example.","pos":78,"slide":"subslide","type":"cell"}
{"cell_type":"markdown","id":"bdb7be","input":"# Model Definition \n\nRecall the structure of our classification problem for the images in FashionMNIST. \n\n* Each **input** to the neural network in each instance will be represented by a fixed-length vector.\nSince the raw data here consists of $32 \\times 32$ pixel images, we *flatten* each image,\ntreating them as vectors of length $32\\times 32=1024$.\n\n* The **output** is a vector of size ten, which we call $\\mathbf{o}$ wich will give us the **probabilities** that the image is in one of the ten categories.\n\nSo the structure of the neural network can be imagined as the following matrix operations:\n$$\n\\mathbf{0}=\\mathbf{W}\\cdot \\mathbf{x}+ \\mathbf{b}\n$$\n$$\n\\begin{bmatrix}\no_{1} \\\\ o_{2} \\\\ \\ldots \\\\  o_{10}\n\\end{bmatrix}=\n\\begin{bmatrix} w_{1,1} & w_{1,2} & \\ldots & w_{1,1024}\\\\ w_{2,1} & w_{2,2} & \\ldots & w_{2,1024}\\\\ \\ldots\\\\w_{10,1} & w_{10,2} & \\ldots & w_{10,1024}\\end{bmatrix}\\cdot \n\\begin{bmatrix} x_{1}  \\\\\nx_{2}  \\\\\n\\ldots \\\\\n x_{1024} \n\\end{bmatrix}\\\n+\\begin{bmatrix} b_1 \\\\ b_2 \\\\ \\ldots \\\\ b_{10}\\end{bmatrix}\n$$\n","pos":26,"slide":"slide","type":"cell"}
{"cell_type":"markdown","id":"c67d55","input":"## Using  `F.cross_entropy`\n\nIn **pytorch** the function `cross_entropy` can be used to calculate the cross entropy loss.\n\nWe ilustrate this with an example.\n\nConsider the following output tensor: `output_data`=[1,1,1].\n\nTo transform it into a probability distribution using **Softmax** we first generate `exp(output_data)`=$[e^1,e^1,e^1]$, and then calculate:\n$$\n\\hat{y}=\\text{softmax(output\\_data)}=[\\frac{e^1}{e^1+e^1+e^1}, \\frac{e^1}{e^1+e^1+e^1}, \\frac{e^1}{e^1+e^1+e^1}]=[\\frac{1}{3},\\frac{1}{3},\\frac{1}{3}]\n$$","pos":45,"slide":"subslide","type":"cell"}
{"cell_type":"markdown","id":"d043be","input":"Answer: \n\n","pos":93,"type":"cell"}
{"cell_type":"markdown","id":"d4fdfb","input":"### Example of Softmax Calculation\n\n\nWe will illustrate a Softmax calculation in **Pytorch** by selecting a simple example, and using the **Pytorch** function `torch.nn.functional.softmax`, which is an implementation of the Softmax formula.\n\nConsider the following tensor: `output_data`=[1,1,1].\n\nTo transform it into a **Softmax** probability distribution we first generate `exp(output_data)`=$[e^1,e^1,e^1]$, and then calculate:\n\n$$\n\\text{softmax(output\\_data)}=\n$$\n$$[\\frac{e^1}{e^1+e^1+e^1}, \\frac{e^1}{e^1+e^1+e^1}, \\frac{e^1}{e^1+e^1+e^1}]=[\\frac{1}{3},\\frac{1}{3},\\frac{1}{3}]\n$$","pos":39,"slide":"subslide","type":"cell"}
{"cell_type":"markdown","id":"d69b63","input":"# The Training\nWe are now ready to train our model. The next code segment not only trains the model, it also graphs the error and accuracy of the model in every epoch.","pos":56,"slide":"slide","type":"cell"}
{"cell_type":"markdown","id":"d7561b","input":"# OOD in Linear Classfication\n\n## Introduction\n\nWe will extend the classes defined in the linear regression notebook to create a neural network that can handle classification tasks. Here's an overview of the process:","pos":66,"slide":"slide","type":"cell"}
{"cell_type":"markdown","id":"d9fff4","input":"We can illustrate the training by plotting the accuracy in the training data and the errors as plots, per each epoch.","pos":58,"slide":"subslide","type":"cell"}
{"cell_type":"markdown","id":"dce253","input":"## Forward Pass of the Model\n\nAlthough not necessary for the training, we illustrate how we can calculate the forward pass of this model.","pos":33,"slide":"slide","type":"cell"}
{"cell_type":"markdown","id":"dd29a5","input":"**QUESTION 1**:\n\nHow many attributes (fields) does the `LinearClassificationModel` class have? What is the purpose of each attribute?\n","pos":71,"slide":"subslide","type":"cell"}
{"cell_type":"markdown","id":"df5a51","input":"Reshaping Predicted Pobabilities (Y_hat):\n\n```python\nY_hat = Y_hat.reshape((-1, Y_hat.shape[-1]))\n```\n**Purpose**: The reshaping is performed to ensure that the shape of `Y_hat` is suitable for calculating the loss.\n\n**Explanation**:\n`Y_hat.shape[-1]` retrieves the last dimension of the tensor, which typically corresponds to the number of classes in a classification problem.\n``(-1, Y_hat.shape[-1])`` reshapes `Y_hat` to have an unspecified number of rows (which will be automatically determined based on the total number of elements) and a fixed number of columns equal to the number of classes.\nResult: This transformation ensures that `Y_hat` has a shape of `(N,C)`, where N is the number of samples and C is the number of classes.\n\nReshaping Labels (Y):\n\n```python\nY = Y.reshape((-1,))\n```\n**Purpose**: Similar to `Y_hat`, this reshapes the label tensor `Y.`\n\n**Explanation**:\n(-1,) reshapes Y into a one-dimensional tensor (flattening it). This is necessary because the labels should be a one-dimensional tensor where each entry corresponds to the true class index for the respective sample.\nResult: This ensures that Y has a shape of (N,)","pos":49,"slide":"subslide","type":"cell"}
{"cell_type":"markdown","id":"ed185b","input":"\nHere are some of the key features of the `torchvision.datasets` module:\n\n* Wide range of datasets: The module provides a diverse selection of datasets for various computer vision tasks, including image classification, object detection, and image segmentation.\n\n* Pre-processing: The datasets are pre-processed, which means they are already normalized, resized, and augmented if necessary. This saves time and effort when preparing data for training models.\n\n* Data loading: The module provides convenient data loaders that allow you to iterate over the datasets in batches. This makes it easy to feed data to your models during training.\n\n* Custom datasets: The module includes utility classes for building custom datasets. This allows you to create datasets tailored to your specific needs.\n","pos":13,"slide":"subslide","type":"cell"}
{"cell_type":"markdown","id":"f5d0ae","input":"**QUESTION 7**:\n\nHow many methods and fields are in the `SGDOptimizer` class? What does each method do?\n","pos":88,"type":"cell"}
{"cell_type":"markdown","id":"f5fad6","input":"For a more concise notation we use vectors and matrices:\n$$\n\\begin{bmatrix}o_1\\\\o_2\\\\o_3  \\end{bmatrix}=\\mathbf{o} = \\mathbf{W} \\mathbf{x} + \\mathbf{b}=\\begin{bmatrix} w_{11} & w_{12} & w_{13} & w_{14} \\\\w_{21} & w_{22} & w_{23} & w_{24} \\\\ w_{31} & w_{32} & w_{33} & w_{34}\\end{bmatrix}\\begin{bmatrix} x_1\\\\x_2\\\\x_3\\\\x_4\\end{bmatrix}+\\begin{bmatrix} b_1\\\\b_2\\\\b_3\\\\b_4\\end{bmatrix}\n$$ \nwhich are  much better suited for mathematics and code.\nNote that we have gathered all of our weights $\\mathbf{W}$ into a $3 \\times 4$ matrix and all biases\n$\\mathbf{b} \\in \\mathbb{R}^3$ in a vector.","pos":7,"slide":"subslide","type":"cell"}
{"cell_type":"markdown","id":"f8350a","input":"### Using the loss function\n\n\nLet us illustrate the use of the function `F.cross_entropy` by calculating the cross entropy loss obtained in the previous example, with the output data batch of tensors given by: `output_data`=[1.0,1.0,1.0],[1.0,1.0,1.0],[1.0,1.0,1.0]  and the target probability distributions given by  `labels:0,0,1`.\nIn the manual calculation done in the previous example we obtained $l=1.0986$.\n\nwe define first our function loss for this training problem using the function `F.cross_entropy` as shown below.","pos":47,"slide":"subslide","type":"cell"}
{"cell_type":"markdown","id":"f92dc6","input":"\nColloquially, machine learning practitioners overload the word *classification* to describe two subtly different problems:\n* those where we are interested only in hard assignments of examples to categories (classes);and\n* those where we wish to make soft assignments,i.e., to assess the probability that each category applies.\n\n> The distinction tends to get blurred, in part, because often, even when we only care about hard assignments, we still use models that make soft assignments.\n\nEven more, there are cases where more than one label might be true.\n\nFor instance, a news article might simultaneously cover the topics of entertainment, business, and space flight, but not the topics of medicine or sports.\n\nThus, categorizing it into one of the above categories on their own would not be very useful.\n\nThis problem is commonly known as [multi-label classification](https://en.wikipedia.org/wiki/Multi-label_classification).\nSee :citet:`Tsoumakas.Katakis.2007` for an overview and :citet:`Huang.Xu.Yu.2015` for an effective algorithm when tagging images.\n","pos":3,"slide":"subslide","type":"cell"}
{"cell_type":"markdown","id":"fb0614","input":"**VARIABLES**\n\n| Variable Name   | Data Type   | Nature of Variable  |\n|------------|------------|------------|\n|` traindataloader` | `DataLoader` | Data Loader for the data |","pos":20,"type":"cell"}
{"cell_type":"markdown","id":"fca39c","input":"\nHere are some of the common uses of transforms in computer vision:\n\n* Data Augmentation: Augmenting data involves creating new variations of existing data to increase the training dataset's size and diversity. This helps the model learn more robust features and generalize better to unseen data.\n\n* Image Normalization: Normalizing images ensures that all images have a consistent mean and standard deviation of pixel values. This helps stabilize training and prevents the model from being biased towards images with higher or lower pixel intensities.\n\n* Image Resizing and Cropping: Resizing and cropping images can be used to standardize the size of input images and focus on specific regions of interest.\n\n* Color Augmentations: Color augmentations can be used to modify the color balance, brightness, and contrast of images, adding more variability to the training data.","pos":16,"slide":"subslide","type":"cell"}
{"id":0,"time":1734212424200,"type":"user"}
{"last_load":1733253448824,"type":"file"}