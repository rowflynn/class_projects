{"args":["--to","cocalc-html"],"start":1733420044763,"state":"done","time":1733420044912,"type":"nbconvert"}
{"backend_state":"running","connection_file":"/tmp/xdg-runtime-user/jupyter/kernel-4ac40b74-8b09-4c03-aa36-4a0accadc7ea.json","kernel":"python3","kernel_error":"","kernel_state":"idle","kernel_usage":{"cpu":0,"memory":0},"last_backend_state":1733419887468,"last_ipynb_save":1733441840371,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"trust":true,"type":"settings"}
{"cell_type":"code","end":1733419889868,"exec_count":3,"id":"0fd026","input":"# Import necessary modules\nfrom sklearn.datasets import fetch_california_housing\n\n# Load the California Housing dataset\ndata = fetch_california_housing()\nX, y = data.data, data.target\n\n# Define the attribute names\nfeature_names = [\n    \"MedInc\", \"HouseAge\", \"AveRooms\", \"AveBedrms\", \n    \"Population\", \"AveOccup\", \"Latitude\", \"Longitude\"\n]\n\n# Function to print a sample data item with attribute names\ndef print_sample_data(X, y, feature_names, sample_index=0):\n    # Select the sample at the specified index\n    X_sample = X[sample_index]\n    y_sample = y[sample_index]\n    \n    # Print each feature with its attribute name\n    print(\"Sample data item with attributes:\")\n    for feature_name, value in zip(feature_names, X_sample):\n        print(f\"{feature_name}: {value}\")\n    \n    print(f\"\\nTarget (Median House Value): {y_sample}\")\n\n# Print the first sample data item\nprint_sample_data(X, y, feature_names, sample_index=0)\nprint(\"*******************************\")\n# Print the second sample data item\nprint_sample_data(X, y, feature_names, sample_index=1)","kernel":"python3","last":20,"no_halt":true,"output":{"0":{"name":"stdout","text":"Sample data item with attributes:\nMedInc: 8.3252\nHouseAge: 41.0\nAveRooms: 6.984126984126984\nAveBedrms: 1.0238095238095237\nPopulation: 322.0\nAveOccup: 2.5555555555555554\nLatitude: 37.88\nLongitude: -122.23\n\nTarget (Median House Value): 4.526\n*******************************\nSample data item with attributes:\nMedInc: 8.3014\nHouseAge: 21.0\nAveRooms: 6.238137082601054\nAveBedrms: 0.9718804920913884\nPopulation: 2401.0\nAveOccup: 2.109841827768014\nLatitude: 37.86\nLongitude: -122.22\n\nTarget (Median House Value): 3.585\n"}},"pos":2,"start":1733419887555,"state":"done","type":"cell"}
{"cell_type":"code","end":1733419893999,"exec_count":4,"id":"a474b2","input":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport torch\nfrom torch.utils.data import TensorDataset, DataLoader\n# Standardize the features and target variable\nscaler_X = StandardScaler()\nscaler_y = StandardScaler()\nX = scaler_X.fit_transform(X)\ny = scaler_y.fit_transform(y.reshape(-1, 1)).flatten()\n# Convert to PyTorch tensors\nX_tensor = torch.tensor(X, dtype=torch.float32)\ny_tensor = torch.tensor(y, dtype=torch.float32)\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42)\n\n# Create TensorDataset and DataLoader for training and testing\ntrain_ds = TensorDataset(X_train, y_train)\ntest_ds = TensorDataset(X_test, y_test)\ntrain_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\ntest_loader = DataLoader(test_ds, batch_size=32)","kernel":"python3","last":11,"no_halt":true,"pos":4,"start":1733419889877,"state":"done","type":"cell"}
{"cell_type":"code","end":1733419894019,"exec_count":5,"id":"100185","input":"# Write your code here\n\n# Retrieve the first batch of data\nX_batch, y_batch = next(iter(train_loader))\n#Print the shape of X_batch and of y_batch\nprint(\"Shape of X_batch: \",X_batch.shape)\nprint(\"Shape of y_batch: \",y_batch.shape)","kernel":"python3","last":8,"no_halt":true,"output":{"0":{"name":"stdout","text":"Shape of X_batch:  torch.Size([32, 8])\nShape of y_batch:  torch.Size([32])\n"}},"pos":6,"start":1733419894010,"state":"done","type":"cell"}
{"cell_type":"code","end":1733419894034,"exec_count":6,"id":"771ce4","input":"# Write your code here\n\nfrom torch import nn\n\nclass LinearRegressionModel(nn.Module):\n    def __init__(self, input_features, output_features=1):\n        # Define the initializator using the corresponding one for nn.Linear\n        super().__init__()\n        self.linear = nn.LazyLinear(output_features)\n\n    def forward(self, x):\n        #Define the forwarrd pass with input x\n        return self.linear(x)\n    \n","kernel":"python3","last":3,"no_halt":true,"pos":8,"start":1733419894024,"state":"done","type":"cell"}
{"cell_type":"code","end":1733419894050,"exec_count":7,"id":"37bf00","input":"# Write your code here\n\n# Initialize the model\ninput_features = X.shape[1]\nmodel = LinearRegressionModel(input_features)\n\n# Pass a batch through the model to verify output shape\nX = X_batch\ny_hat = model(X)\n\n# Print the output shape and the output batch\nprint(y_hat.shape)\nprint(y_hat)\n","kernel":"python3","last":9,"no_halt":true,"output":{"0":{"name":"stdout","text":"torch.Size([32, 1])\ntensor([[-0.3866],\n        [-0.0542],\n        [ 0.1352],\n        [ 0.2114],\n        [ 0.3163],\n        [ 0.4983],\n        [-0.1779],\n        [-0.8946],\n        [-0.3630],\n        [ 0.0704],\n        [-0.8580],\n        [-0.5582],\n        [-0.3896],\n        [-0.4940],\n        [-0.3690],\n        [-0.2837],\n        [-0.8333],\n        [ 0.3048],\n        [ 0.9007],\n        [-0.5305],\n        [ 0.3927],\n        [-0.3488],\n        [ 0.4452],\n        [-0.5494],\n        [-0.3636],\n        [ 2.0084],\n        [-0.7371],\n        [ 0.7144],\n        [-0.4066],\n        [-0.3889],\n        [ 0.6491],\n        [-0.0892]], grad_fn=<AddmmBackward0>)\n"}},"pos":10,"start":1733419894041,"state":"done","type":"cell"}
{"cell_type":"code","end":1733419895541,"exec_count":8,"id":"f87afd","input":"#Write your code here\n\nimport torch.optim as optim\n\n# Define the loss function, call it criterion and define the optimizer\ncriterion = nn.MSELoss()\nparams = model.parameters()\nlr = 0.001\noptimizer = optim.SGD(params, lr)\n    ","kernel":"python3","last":3,"no_halt":true,"pos":12,"start":1733419894058,"state":"done","type":"cell"}
{"cell_type":"code","end":1733419895556,"exec_count":9,"id":"ce858d","input":"# Write your code here\n\n#Calculate the output of the model with input X.batch. Call the output prediction\nprediction = y_hat\n#calculate the loss using the criterion function (loss function) evaluated with prediction and y-batch\nloss = criterion(prediction.reshape(-1,1), y_batch.reshape(-1,1))\n#print the value of the loss just calculated\nprint(loss)","kernel":"python3","last":5,"no_halt":true,"output":{"0":{"name":"stdout","text":"tensor(1.0932, grad_fn=<MseLossBackward0>)\n"}},"pos":14,"start":1733419895552,"state":"done","type":"cell"}
{"cell_type":"code","end":1733419943254,"exec_count":10,"id":"6c791d","input":"# Write your code here\nimport matplotlib.pyplot as plt\n\n# Set the number of epochs and set train-losses to the empty array.\nepochs = 100\ntrain_losses = []  # List to record the training loss for each epoch\n\n# Training loop\nfor epoch in range(epochs):\n    running_loss = 0.0  # Initialize running loss for each epoch\n    \n    # Loop over batches in the train_loader\n    for inputs, targets in train_loader:\n                      # Clear the gradients from the previous batch\n        optimizer.zero_grad()\n                    # Forward pass: compute model predictions\n        y_hat = model(inputs)\n                      # Compute the loss for this batch\n        loss = criterion( targets.reshape(-1,1),y_hat.reshape(-1,1))\n                     # Backward pass: calculate gradients\n        loss.backward()\n                     # Update model parameters based on gradients\n        optimizer.step()\n                      # Accumulate loss for this epoch\n        running_loss += loss.item()\n    #end of inner loop\n    # Calculate and record the average loss for the current epoch\n    average_loss = running_loss / len(train_loader)\n    train_losses.append(average_loss)\n\n    # Print the loss every 10 epochs\n    if (epoch + 1) % 10 == 0:\n        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {average_loss:.4f}\")\n\n# Plot the training loss over epochs\nplt.plot(train_losses, label=\"Training Loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()","kernel":"python3","last":45953,"no_halt":true,"output":{"0":{"name":"stdout","text":"Epoch [10/100], Loss: 0.4130\n"},"1":{"name":"stdout","text":"Epoch [20/100], Loss: 0.3948\n"},"10":{"data":{"image/png":"71d6f6aa51a5671cbfa173bfac3f8aa194724172","text/plain":"<Figure size 1200x700 with 1 Axes>"},"metadata":{"image/png":{"height":602,"width":1010}}},"2":{"name":"stdout","text":"Epoch [30/100], Loss: 0.3906\n"},"3":{"name":"stdout","text":"Epoch [40/100], Loss: 0.3899\n"},"4":{"name":"stdout","text":"Epoch [50/100], Loss: 0.3896\n"},"5":{"name":"stdout","text":"Epoch [60/100], Loss: 0.3896\n"},"6":{"name":"stdout","text":"Epoch [70/100], Loss: 0.3895\n"},"7":{"name":"stdout","text":"Epoch [80/100], Loss: 0.3895\n"},"8":{"name":"stdout","text":"Epoch [90/100], Loss: 0.3894\n"},"9":{"name":"stdout","text":"Epoch [100/100], Loss: 0.3895\n"}},"pos":16,"start":1733419895570,"state":"done","type":"cell"}
{"cell_type":"code","end":1733419943350,"exec_count":11,"id":"fa1b8b","input":"# Write answer here\nfrom sklearn.metrics import mean_squared_error\n\n# Set the model to evaluation mode to disable any training-specific features\nmodel.eval()\n\n# Disable gradient computation to save memory and computation during inference\nwith torch.no_grad():\n    # Generate predictions on the test data\n                  # Flatten to ensure consistency with target shape\n    y_predicted = model(X_test)\n    y_predicted.flatten()\n    # Calculate Mean Squared Error (MSE) on the test set\n    error = mean_squared_error(y_test, y_predicted)\n#end of loop\n# Print the evaluation metrics\nprint(error)","kernel":"python3","last":6,"no_halt":true,"output":{"0":{"name":"stdout","text":"0.41983175\n"}},"pos":19,"start":1733419943264,"state":"done","type":"cell"}
{"cell_type":"code","end":1733419991526,"exec_count":12,"id":"dc5632","input":"#Write your code here for the learning rate 0.01\n\n#Redefine the optimizer with the new learning rate\nparams = model.parameters()\nlr = 0.01\noptimizer1 = optim.SGD(params, lr)\n\n# rerun your training loop with the new learning rate above.\nimport matplotlib.pyplot as plt\nmodel.train()\n# Set the number of epochs and set train-losses to the empty array.\nepochs = 100\ntrain_losses = []  # List to record the training loss for each epoch\n\n# Training loop\nfor epoch in range(epochs):\n    running_loss = 0.0  # Initialize running loss for each epoch\n    \n    # Loop over batches in the train_loader\n    for inputs, targets in train_loader:\n                      # Clear the gradients from the previous batch\n        optimizer1.zero_grad()\n                    # Forward pass: compute model predictions\n        y_hat = model(inputs)\n                      # Compute the loss for this batch\n        loss = criterion( targets.reshape(-1,1),y_hat.reshape(-1,1))\n                     # Backward pass: calculate gradients\n        loss.backward()\n                     # Update model parameters based on gradients\n        optimizer1.step()\n                      # Accumulate loss for this epoch\n        running_loss += loss.item()\n    #end of inner loop\n    # Calculate and record the average loss for the current epoch\n    average_loss = running_loss / len(train_loader)\n    train_losses.append(average_loss)\n\n    # Print the loss every 10 epochs\n    if (epoch + 1) % 10 == 0:\n        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {average_loss:.4f}\")\n\n# Plot the training loss over epochs\nplt.plot(train_losses, label=\"Training Loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()","kernel":"python3","last":44243,"no_halt":true,"output":{"0":{"name":"stdout","text":"Epoch [10/100], Loss: 2372.6659\n"},"1":{"name":"stdout","text":"Epoch [20/100], Loss: 73409383480.4806\n"},"10":{"data":{"image/png":"8478d378a581cad3d2be8ab3bf3c6957afe3f034","text/plain":"<Figure size 1200x700 with 1 Axes>"},"metadata":{"image/png":{"height":617,"width":988}}},"2":{"name":"stdout","text":"Epoch [30/100], Loss: 7592126244774516736.0000\n"},"3":{"name":"stdout","text":"Epoch [40/100], Loss: 6915894929682333947658240.0000\n"},"4":{"name":"stdout","text":"Epoch [50/100], Loss: 6918676630764683450846289076420608.0000\n"},"5":{"name":"stdout","text":"Epoch [60/100], Loss: inf\n"},"6":{"name":"stdout","text":"Epoch [70/100], Loss: inf\n"},"7":{"name":"stdout","text":"Epoch [80/100], Loss: inf\n"},"8":{"name":"stdout","text":"Epoch [90/100], Loss: inf\n"},"9":{"name":"stdout","text":"Epoch [100/100], Loss: inf\n"}},"pos":21,"start":1733419943361,"state":"done","type":"cell"}
{"cell_type":"code","end":1733420040123,"exec_count":13,"id":"8d1c67","input":"#Write your code here for the learning rate 0.0001\n\n#Redefine the optimizer with the new learning rate\nparams = model.parameters()\nlr = 0.0001\noptimizer2 = optim.SGD(params, lr)\n\n# rerun your training loop with the new learning rate above.\nimport matplotlib.pyplot as plt\nmodel.train()\n# Set the number of epochs and set train-losses to the empty array.\nepochs = 100\ntrain_losses = []  # List to record the training loss for each epoch\n\n# Training loop\nfor epoch in range(epochs):\n    running_loss = 0.0  # Initialize running loss for each epoch\n    \n    # Loop over batches in the train_loader\n    for inputs, targets in train_loader:\n                      # Clear the gradients from the previous batch\n        optimizer2.zero_grad()\n                    # Forward pass: compute model predictions\n        y_hat = model(inputs)\n                      # Compute the loss for this batch\n        loss = criterion( targets.reshape(-1,1),y_hat.reshape(-1,1))\n                     # Backward pass: calculate gradients\n        loss.backward()\n                     # Update model parameters based on gradients\n        optimizer2.step()\n                      # Accumulate loss for this epoch\n        running_loss += loss.item()\n    #end of inner loop\n    # Calculate and record the average loss for the current epoch\n    average_loss = running_loss / len(train_loader)\n    train_losses.append(average_loss)\n\n    # Print the loss every 10 epochs\n    if (epoch + 1) % 10 == 0:\n        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {average_loss:.4f}\")\n\n# Plot the training loss over epochs\nplt.plot(train_losses, label=\"Training Loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()","kernel":"python3","last":45419,"no_halt":true,"output":{"0":{"name":"stdout","text":"Epoch [10/100], Loss: inf\n"},"1":{"name":"stdout","text":"Epoch [20/100], Loss: inf\n"},"10":{"data":{"image/png":"5a250677cb80d48b23a7bb747ab9c30f5c1bcc4a","text/plain":"<Figure size 1200x700 with 1 Axes>"},"metadata":{"image/png":{"height":602,"width":1021}}},"2":{"name":"stdout","text":"Epoch [30/100], Loss: inf\n"},"3":{"name":"stdout","text":"Epoch [40/100], Loss: inf\n"},"4":{"name":"stdout","text":"Epoch [50/100], Loss: inf\n"},"5":{"name":"stdout","text":"Epoch [60/100], Loss: inf\n"},"6":{"name":"stdout","text":"Epoch [70/100], Loss: inf\n"},"7":{"name":"stdout","text":"Epoch [80/100], Loss: inf\n"},"8":{"name":"stdout","text":"Epoch [90/100], Loss: inf\n"},"9":{"name":"stdout","text":"Epoch [100/100], Loss: inf\n"}},"pos":22,"start":1733419991540,"state":"done","type":"cell"}
{"cell_type":"code","id":"5d1bb3","input":"","pos":25,"type":"cell"}
{"cell_type":"markdown","id":"0e2280","input":"## Dataset Information\n\nThe California Housing dataset provides information on median house values in various districts in California, along with other demographic and geographic data. \n\nThis dataset, introduced by the U.S. Census, is often used for regression problems and is available in the `sklearn.datasets` module of Pytorch.\n\n### Dataset Overview\n\nThe California Housing dataset contains the following attributes:\n\n* `MedInc`: Median income in the district (in tens of thousands of dollars).\n* `HouseAge`: Median house age in the district.\n* `AveRooms`: Average number of rooms per household.\n* `AveBedrms`: Average number of bedrooms per household.\n* `Population`: Population in the district.\n* `AveOccup`: Average number of occupants per household.\n* `Latitude`: Latitude of the district's location.\n* `Longitude`: Longitude of the district's location.\n\n#### Target Variable\n\nThe target variable, which we aim to predict using linear regression, is:\n\n`MedHouseVal`: Median house value for households within a district (in hundreds of thousands of dollars).\n\n#### Pytorch Implementation of the Data Set\n\nWe need to import `fetch_california_housing `from `sklearn.datasets` to load the California Housing dataset.\n\nHere is a sample code that imports the required data into Pytorch","pos":1,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"17ef17","input":"### Curating the Data\n\nBefore using the dataset, we need to process it: transform it into a batch of tensors.\n\n#### Why Normalize the Data?\n\nNormalization is essential for the following reasons:\n\n* Consistent Scale: Each feature (e.g., income, population, latitude) has different scales. Standardizing these features to a mean of 0 and a standard deviation of 1 ensures that each feature contributes proportionally to the modelâ€™s learning, preventing some variables from dominating others.\n\n* Improved Training Stability: Neural networks, especially those using gradient-based optimization, converge faster and more reliably when inputs are normalized. Without normalization, larger values can lead to large gradients that destabilize the training process.\n\n* Reduced Bias in Coefficients: Normalizing also helps ensure that each feature is treated equally by the model, preventing any inherent bias toward features with larger values.\n\n##### Pytorch Code\n\n* We will Import `train_test_spli`t from `sklearn.model_selection` to split the data into training and testing sets.\nWe will Import `Standard Scaler`t from `sklearn.preprocessing` to split the data into training and testing sets.\n\n##### Why Splitting the Data?\n\nSplitting the data into training and testing sets is a critical step in machine learning, as it allows us to evaluate the modelâ€™s performance on unseen data. Here are the key reasons for splitting the California Housing dataset in this context:\n\n1. Model Evaluation on Unseen Data\n   - A fundamental goal of machine learning is to generalize well on new, unseen data. By splitting the dataset, we train the model on one portion (the training set) and assess its performance on another (the test set). This helps us estimate how well the model might perform on real-world data.\n\n2. Avoiding Overfitting\n   - Without a test set, thereâ€™s a risk that the model will \"memorize\" the training data instead of learning meaningful patterns (a problem called overfitting). Testing on a separate dataset lets us verify that the model captures the underlying trends rather than just fitting to the noise in the training data.\n\n3. Hyperparameter Tuning and Model Selection\n   - Often, we need to try different model architectures, hyperparameters, or regularization methods to find the best performing model. Using a test set allows us to evaluate these choices objectively, helping us choose the model and settings that generalize well.\n\n4. Performance Metrics\n   - A dedicated test set provides unbiased performance metrics. For instance, in regression tasks like this housing dataset, we might want metrics like Mean Squared Error (MSE) or Mean Absolute Error (MAE). These metrics calculated on the test set are good indicators of expected performance in real-world applications.\n\n#### Transforming dataset into Tensors\n\nThis last step is required to feed the data to a neural network.\n\n#### Practical Example in the Provided Code\n\nIn this code:\n- `train_test_split` with `test_size=0.2` splits 80% of the data for training and 20% for testing.\n- `train_loader` and `test_loader` then serve as data sources for training and evaluating the model separately.\n\nThis separation enables a realistic assessment of model performance and facilitates selecting the best model and settings.","pos":3,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"1f0804","input":"**END OF WORKSHEET**\n\nMake sure that you answered all the questions on time. This completed `Jupyter Notebook` will be collected and graded. \n\nOnce the `Jupyter Notebook` is collected it can not be modified.","pos":24,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"555724","input":"# Project: Linear Regression Model for Predicting California House Prices\n\n> > ðŸ‘‰ðŸ¾ Objective: Develop a linear regression neural network using PyTorch to predict house prices based on various features in the California Housing dataset.","pos":0,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"5af84d","input":"## Define the Loss Function and Optimizer\n\nIn this section, you'll define the loss function and optimizer, which are essential for training the linear regression model.\n\n### Question 4\n\n1. **Define the Loss Function:**\n   * Use `nn.MSELoss` as the loss function, which is commonly used for regression tasks.\n   * The Mean Squared Error (MSE) loss measures the average squared difference between the predicted and actual values, which helps in minimizing prediction errors during training.\n   * you can check the API of the `nn.MSEloss` function [here](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html).\n\n2. **Set Up the Optimizer:**\n   * Use `torch.optim.SGD` as the optimizer for updating the model parameters during training.\n   * Set the learning rate (`lr`) to 0.001, which controls how much the optimizer adjusts the weights with each step.\n   * Initialize the optimizer to work with the parameters of your model, which we defined in Question 3 above as the variable `model`, which you can access with `model.parameters()`in the `optim.SGD call`.\n\n* **Code Outline:**\n  - Import `torch.optim` as `optim`.\n  - Define `criterion` as `nn.MSELoss()`.\n  - Define `optimizer` as a call to `optim.SGD()` with the right parameters.\n  \n\nThis setup will prepare the model for training, allowing it to minimize the MSE loss through gradient descent.\n\nWrite your code below\n\n","pos":11,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"5ca034","input":"#### Explanation of the Results\n\nWrite your explanation here.\n\nIn this example, the learning rate of 0.01 quickly approaches infinity rather than converging to 0. The learning rate of 0.0001 approaches 0 but slower than 0.001 so 0.001 seems to be the ideal rate for this specific example.\n","pos":23,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"6c03cc","input":"## Evaluate the Model\n\nIn this section, you'll evaluate the performance of your trained model on a test dataset.\n\n### Question 7\n\nWe will evaluate the modelâ€™s total loss on the test dataset using the mean_square_error() from the module sklearn.metrics (find how to do this on the internet). Do not forget to import it.\n\n1. **Set the Model to Evaluation Mode:**\n   * Use `model.eval()` to set the model to evaluation mode. This ensures that any layers that behave differently during training (like dropout) are correctly configured for inference.\n\n2. **Disable Gradient Computation:**\n   * Use `torch.no_grad()` to disable gradient calculations, which saves memory and speeds up computation since we are only performing inference.\n\n3. **Generate Predictions on the Test Data:**\n   * Pass `X_test` (your test dataset inputs) through the model to get predictions.\n   * Use `.flatten()` on the output to ensure the predictions have the same shape as `y_test` (your target values), which is necessary for calculating mean squared error.\n\n4. **Calculate Mean Squared Error (MSE):**\n   * Use `mean_squared_error()` from `sklearn.metrics` to calculate the modelâ€™s MSE on the test dataset. This function provides an easy way to compute MSE between the predictions and true labels (`y_test`).\n   * If you're unfamiliar with `mean_squared_error()` or `sklearn.metrics`, try searching online for \"sklearn mean_squared_error\" for usage details and examples.\n\n5. **Display the Test MSE:**\n   * Use `print` to output the test MSE, rounding to four decimal places to keep the results concise.\n\nWrite your answer below.","pos":18,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"7688ee","input":"## Define the class LinearRegressionModel\n\n> In this section we define the class `LinearRegressionModel` class.\n\n\n### Question 2\n\nIn this exercise, you'll define a class `LinearRegressionModel` that inherits from `torch.nn.Module`. This class will represent a simple linear regression model using PyTorch's neural network module.\n\n* **Objective:** Use PyTorch's `nn.Linear` layer to create a linear regression model that can take in a set of input features and map them to a single output value. This is useful for predicting continuous values (e.g., predicting house prices based on size, age, location, etc.).\n\nRecall some basic info about torch.nn.Linear\n* Class: `torch.nn.Linear`\n* Module: `torch.nn`\nThe `torch.nn.Linear` class represents a fully connected (or dense) layer, also known as a linear layer. This layer applies a linear transformation to the incoming data using the formula:\n\n$$\ny = xW^T + b\n$$\n\nwhere:\n- $ x $ is the input,\n- $W$ is the weight matrix,\n- $ b$ is the bias vector.\n\n[Look](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) into the API for `torch.nn.linear` to see how you initialize layers for that class.\n\n* **Instructions:**\n  1. Define a class named `LinearRegressionModel` that inherits from `torch.nn.Module`.\n  2. Inside the class, initialize a single `nn.Linear` layer in the `__init__` method, which takes in the number of input features (`input_dim`) and outputs a single value.\n  3. Implement the `forward` method, which defines the forward pass of the model. This method should take an input tensor `x` and return the output of the `nn.Linear` layer.\n\n* **Hint:** The `nn.Linear` layer performs a linear transformation using weights and bias, i.e., `output = x * weight + bias`. By setting the `out_features` argument to 1 in the `nn.Linear` layer, you're ensuring the model outputs a single continuous prediction for each input sample.\n\nWrite your code below","pos":7,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"92f9ef","input":"<div class=\"alert alert-block alert-warning\">\n<b>IMPORTANT</b> If your code is correct the neural network must have an error less than $0.6$.\n</div>\n\nWe could improve the prediction accuracy by increasing the number of epochs, and doing other things. But at this point we will keep the error obtained and we will pass to the validation phase.\n","pos":17,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"9c74f8","input":"## Experiment with Hyperparameters\n\nAnalyze the effect of different learning rates on model performance.\n\n### Question 8\n\n* Change the learning rate to 0.0001 and 0.01, re-train the model, and compare the training and test MSE.\n\n* Discuss the impact of learning rate on model convergence and stability.\n\nWrite your answer below.","pos":20,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"9f4af3","input":"## Train the Model\n\nImplement a training loop over multiple epochs, updating model weights using backpropagation.\n\n### Question 6\n\n* Train the model for 100 epochs.\n\n* Record the training loss at each epoch and plot it at the end.\n\nYou should see something like this (the number may not be the same because we are running the training starting with random assigments of the parameters):\n\n![output3](Output3.png)\n\n\nWrite your answer below","pos":15,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"bfc4ef","input":"### Question 5\n\nAfter defining the loss function and optimizer, write code to:\n\n* Calculate the loss for the predictions from the outputs of `model(X_batch)` (from question 1) against `y_batch`.\n\n* Print the computed loss to verify the setup is correct.\n\nWrite your answer below","pos":13,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"f35998","input":"### Question 1\n\nUse the variables `train_loader` and `test_loader`, to write code to:\n\n* Retrieve the first batch of data from `train_loader`.\n\n* Print the shapes of the feature tensor `X_batch` and target tensor `y_batch` for that batch to verify correctness.\n\nYour output should look like this:\n\n![output1](Output1.png)\n\n\nPlease write your code below.","pos":5,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"fa7e17","input":"### Question 3\n\nAfter defining the `LinearRegressionModel` class, you'll now write code to:\n\n1. **Initialize the Model:** Create an instance of `LinearRegressionModel` called `model` and ensure that it has the correct input dimension.\n   * Use `X.shape[1]` as the input dimension, where `X` is your dataset with features. This ensures the model knows how many features to expect in each sample.\n   \n2. **Verify the Model's Output Shape:** Pass a sample batch of data (e.g., `X_batch` from Question 1) through the model to ensure that the output shape matches what you'd expect.\n   * When you pass `X_batch` (a sample batch of your input data) through the model, it should return predictions with the same number of rows as `X_batch`, but with only one column, since this is a single-output linear regression model.\n   \n3. **Print the Output Shape:** After obtaining the output from the model, use `print` to display the shape of the output tensor to verify it matches the expected shape.\n\nThe output of your code should be something like this (the shape should be the same, the other values not since they correspond to output of the model for the batch input, and those  depend on the random assigment of the weights ):\n\n![output2](Output2.png)\n\nWrite your code below","pos":9,"state":"done","type":"cell"}
{"id":0,"time":1733419149010,"type":"user"}
{"last_load":1732736895383,"type":"file"}