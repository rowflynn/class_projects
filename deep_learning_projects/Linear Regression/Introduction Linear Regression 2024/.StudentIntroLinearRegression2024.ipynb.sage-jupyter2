{"attachments":{"vector.jpeg":{"type":"sha1","value":"45dd8a3de040c243a977f0e5d2292a91a3f83504"}},"cell_type":"markdown","id":"4b273c","input":"## Vectorization for Speed\n\nWhen training our models, we typically want to process whole minibatches of examples simultaneously.\n\n* Doing this efficiently requires that we vectorize the calculations and leverage fast linear algebra libraries rather than writing costly for-loops in Python.\n\n* To see why this matters so much, let's consider two methods for adding vectors. To start, we instantiate two 10,000-dimensional vectors containing all 1s.\n\nIn the first method, we loop over the vectors with a Python for-loop. In the second, we rely on a single call to `+`.","metadata":{"origin_pos":6},"pos":37,"slide":"slide","type":"cell"}
{"backend_state":"running","connection_file":"/tmp/xdg-runtime-user/jupyter/kernel-42f0af50-043f-4b9b-969b-ed40a988690e.json","kernel":"python3","kernel_error":"","kernel_state":"idle","kernel_usage":{"cpu":0,"memory":0},"last_backend_state":1731008335554,"last_ipynb_save":1731010071923,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"required_libs":[]},"trust":true,"type":"settings"}
{"cell_type":"code","end":1731008357542,"exec_count":3,"id":"d23b58","input":"#This Python code imports several modules and sets up the necessary environment for running the code in a Jupyter notebook.\n#The command below specifies that any plots generated by matplotlib should be displayed directly in the notebook.\n%matplotlib inline \nimport math #imports the math module, which provides various mathematical functions.\nimport time #imports the time module, which provides functions for working with time and dates.\nimport numpy as np #imports the numpy module, which provides functions for working with arrays and mathematical operations.\nimport torch #imports the torch module, which provides a framework for working with tensors and building neural networks.\nfrom d2l import torch as d2l #imports the torch module from the d2l library, which is a deep learning library that provides useful functions and utilities for training and testing models.","kernel":"python3","metadata":{"execution":{"iopub.execute_input":"2023-08-18T19:40:22.515904Z","iopub.status.busy":"2023-08-18T19:40:22.515596Z","iopub.status.idle":"2023-08-18T19:40:25.958016Z","shell.execute_reply":"2023-08-18T19:40:25.957007Z"},"origin_pos":3,"tab":["pytorch"]},"pos":38,"slide":"subslide","start":1731008335600,"state":"done","type":"cell"}
{"cell_type":"code","end":1731008425876,"exec_count":4,"id":"4327f4","input":"#We declare the arrays to be added together.\nn = 10000\na = torch.ones(n)\nb = torch.ones(n)","kernel":"python3","metadata":{"execution":{"iopub.execute_input":"2023-08-18T19:40:25.962060Z","iopub.status.busy":"2023-08-18T19:40:25.961577Z","iopub.status.idle":"2023-08-18T19:40:25.986556Z","shell.execute_reply":"2023-08-18T19:40:25.985693Z"},"origin_pos":7,"tab":["pytorch"]},"pos":39,"start":1731008425483,"state":"done","type":"cell"}
{"cell_type":"code","end":1731008478767,"exec_count":9,"id":"4c18b9","input":"c = torch.zeros(n)#initializes an array c with zeros\nt = time.time()#captures the current time\nfor i in range(n):\n    c[i] = a[i] + b[i]\nf'{time.time() - t:.5f} sec'#it calculates the elapsed time by subtracting the initial time captured above from the current time, and formats it as a string with 5 decimal places","kernel":"python3","last":119,"metadata":{"execution":{"iopub.execute_input":"2023-08-18T19:40:25.991235Z","iopub.status.busy":"2023-08-18T19:40:25.990635Z","iopub.status.idle":"2023-08-18T19:40:26.178339Z","shell.execute_reply":"2023-08-18T19:40:26.177087Z"},"origin_pos":9,"tab":["pytorch"]},"output":{"0":{"data":{"text/plain":"'0.11830 sec'"},"exec_count":9}},"pos":41,"start":1731008478638,"state":"done","type":"cell"}
{"cell_type":"code","end":1731008488243,"exec_count":14,"id":"2687f6","input":"t = time.time()#captures the current time\nd = a + b\nf'{time.time() - t:.5f} sec'","kernel":"python3","last":9,"metadata":{"execution":{"iopub.execute_input":"2023-08-18T19:40:26.183124Z","iopub.status.busy":"2023-08-18T19:40:26.182348Z","iopub.status.idle":"2023-08-18T19:40:26.190223Z","shell.execute_reply":"2023-08-18T19:40:26.189016Z"},"origin_pos":13,"tab":["pytorch"]},"output":{"0":{"data":{"text/plain":"'0.00057 sec'"},"exec_count":14}},"pos":43,"start":1731008488239,"state":"done","type":"cell"}
{"cell_type":"code","exec_count":0,"id":"b295b4","input":"","pos":64,"type":"cell"}
{"cell_type":"markdown","id":"02ad1f","input":"\n#### Gradient Descent\n\nThe key technique for optimizing nearly every deep learning model, and which we will call upon throughout this book,consists of iteratively reducing the error by updating the parameters in the direction that incrementally lowers the loss function.\n\n---\n\n> **NOTE**:This algorithm is called **gradient descent**.\n\n---\n![Illustration of Gradient Descent](gradient.png)","pos":25,"slide":"subslide","type":"cell"}
{"cell_type":"markdown","id":"0c5fae","input":"**QUESTION 4**\n\nWhat is the difference between minibatch SGD  and SGD? Write your answer in the cell below.","pos":59,"slide":"subslide","type":"cell"}
{"cell_type":"markdown","id":"0e4796","input":"### Predictions\n\nGiven the model $\\hat{\\mathbf{w}}^\\top \\mathbf{x} + \\hat{b}$, we can now make *predictions* for a new example,\ne.g., predicting the sales price of a previously unseen house given its area $x_1$ and age $x_2$.\n\n* Deep learning practitioners have taken to calling the prediction phase *inference* but this is a bit of a misnomer---*inference* refers broadly to any conclusion reached on the basis of evidence,including both the values of the parameters and the likely label for an unseen instance.\n\n* If anything, in the statistics literature *inference* more often denotes parameter inference and this overloading of terminology creates unnecessary confusion when deep learning practitioners talk to statisticians. In the following we will stick to *prediction* whenever possible.","pos":36,"slide":"slide","type":"cell"}
{"cell_type":"markdown","id":"12bfa9","input":"\nLinear regression flows from a few simple assumptions.\n\n1. First, we assume that there is a relationship between features $\\mathbf{x}=(x_1,x_2,\\ldots,x_d)$ and target $y$ that is approximately **linear**, i.e., that \n$$\ny=w_1x_1+w_2x_2+\\ldots+w_dx_d+b\n$$\n2. We are also given a set of data, or examples for our problem. One can imagine that data set to be a list of inputs-output, or features and target, of the form:\n$$\n(\\mathbf{x}^{(1)}, y^{(1)})=((x_1^1,x_2^1,\\ldots,x_d^1),y^{(1)})\\\\\n(\\mathbf{x}^{(2)}, y^{(2)})=((x_1^2,x_2^2,\\ldots,x_d^2),y^{(2)}),\\\\\n\\ldots,\\\\ \n(\\mathbf{x}^{(n)}, y^{(n)})=((x_1^n,x_2^n,\\ldots,x_d^1),y^{(n)}).\n$$","pos":3,"slide":"subslide","type":"cell"}
{"cell_type":"markdown","id":"2160fb","input":"* The other extreme is to consider only a single example at a time and to take update steps based on one observation at a time. The resulting algorithm, *stochastic gradient descent* (SGD) can be an effective strategy :cite:`Bottou.2010`, even for large datasets. Unfortunately, SGD has drawbacks, both computational and statistical.\n\n* One problem arises from the fact that processors are a lot faster multiplying and adding numbers than they are at moving data from main memory to processor cache. \n\n* It is up to an order of magnitude more efficient to perform a matrix--vector multiplication than a corresponding number of vector--vector operations.This means that it can take a lot longer to process one sample at a time compared to a full batch.","pos":28,"slide":"subslide","type":"cell"}
{"cell_type":"markdown","id":"25b952","input":"> **DATASET**: In this set up, the given dataset is the collection $\\mathbf{X}$ together with the associated  labels $\\mathbf{y}$.","pos":12,"slide":"subslide","type":"cell"}
{"cell_type":"markdown","id":"287157","input":"Here is an example.\n\n---\n**EXAMPLE**\n\nAs a running example, suppose that we wish to estimate the prices ($P$) of houses (in dollars) based on their area ($A$) (in square feet) and age ($T$) (in years). In symbols:\n$$\nP=f(A,T).\n$$\nTo develop a model for predicting house prices,we need to get our hands on data, including the sales price, area, and age for each home.\n\n---\n\nIn the **terminology** of machine learning:\n\n* the dataset is called a *training dataset* or *training set*,\n* and each row (containing the data corresponding to one sale, $(A,T,P)$) is called an *example* (or *data point*, *instance*, *sample*).\n* The thing we are trying to predict (price) is called a *label* (or *target*).\n* The variables (age and area) upon which the predictions are based are called *features* (or *covariates*).","pos":1,"slide":"subslide","type":"cell"}
{"cell_type":"markdown","id":"2915f8","input":"How to solve this problem?\n\n---\n> **SOLUTION**: The solution to both problems is to pick an intermediate strategy: rather than taking a full batch or only a single sample at a time, we take a *minibatch* of observations :cite:`Li.Zhang.Chen.ea.2014`. \n\n---\n* The specific choice of the size of the said minibatch depends on many factors, such as the amount of memory, the number of accelerators, the choice of layers, and the total dataset size. \n\n* Despite all that, a number between 32 and 256, preferably a multiple of a large power of $2$, is a good start.\n\n> This leads us to *minibatch stochastic gradient descent*.","pos":29,"slide":"subslide","type":"cell"}
{"cell_type":"markdown","id":"3335f3","input":"**QUESTION 2**\n\n","pos":55,"slide":"subslide","type":"cell"}
{"cell_type":"markdown","id":"38f9ce","input":":numref:`fig_single_neuron` depicts linear regression as a neural network. The diagram highlights the connectivity pattern, such as how each input is connected to the output, but not the specific values taken by the weights or biases.\n\n![Linear regression is a single-layer neural network.](singleneuron.svg)\n:label:`fig_single_neuron`\n\n* The inputs are $x_1, \\ldots, x_d$. \n\n* We refer to $d$ as the *number of inputs* or the *feature dimensionality* in the input layer. \n\n* The output of the network is $o_1=\\sum_{i=1}^d w_ix_i$. Because we are just trying to predict a single numerical value, we have only one output neuron.\n\n* Note that the input values are all *given*. \n\t* There is just a single *computed* neuron. In summary, we can think of linear regression as a single-layer fully connected neural network. We will encounter networks with far more layers in later chapters.\n","pos":46,"slide":"subslide","type":"cell"}
{"cell_type":"markdown","id":"422f8c","input":"### Biology\n\nBecause linear regression predates computational neuroscience, it might seem anachronistic to describe\nlinear regression in terms of neural networks. \n\n* Nonetheless, they were a natural place to start when the cyberneticists and neurophysiologists Warren McCulloch and Walter Pitts began to develop models of artificial neurons.\n","pos":47,"slide":"subslide","type":"cell"}
{"cell_type":"markdown","id":"438bc6","input":"Alternatively, we rely on the reloaded `+` operator to compute the elementwise sum.\n","metadata":{"origin_pos":12},"pos":42,"slide":"subslide","type":"cell"}
{"cell_type":"markdown","id":"47953d","input":"* Linear regression happens to be a learning problem with a global minimum (whenever $\\mathbf{X}$ is full rank, or equivalently, whenever $\\mathbf{X}^\\top \\mathbf{X}$ is invertible). \n\n* However, the loss surfaces for deep networks contain many saddle points and minima. \n\n\t* Fortunately, we typically do not care about finding an exact set of parameters but merely any set of parameters that leads to accurate predictions (and thus low loss).\n\n\t* In practice, deep learning practitioners seldom struggle to find parameters that minimize the loss *on training.\n\n* The more formidable task is to find parameters that lead to accurate predictions on previously unseen data, a challenge called *generalization*.\n\n* We return to these topics throughout the book.","pos":35,"slide":"subslide","type":"cell"}
{"cell_type":"markdown","id":"48cd01","input":"---\n\n**GOAL**\nFor our example, given the three items listed above ($P=f(A,T)$), our goal is to find a collection of weights $w_1,w_2, \\ldots, w_d,b$ such that when we calculate, for every $1\\leq j\\leq n$, the predicted output for the input $\\mathbf{x}^{(j)}$,\n$$\n\\hat{y}^{(j)}=w_1x_1^j+w_2x_2^j+\\ldots+w_dx_d^j+b+\\epsilon\n$$\n\n* We want the error between the predicted values $(\\hat{y}^{(1)}, \\hat{y}^{(2)}, \\ldots, \\hat{y}^{(n)})$ and the true labels $(y^{(1)},y^{(2)}, \\ldots, y^{(n)})$ to be as small as possible.\n\n![Fitting the data to a linear model](vector.jpeg)\n\n---","pos":5,"slide":"subslide","type":"cell"}
{"cell_type":"markdown","id":"4be46c","input":"**QUESTION 5**\n\nHow can a linear regression model be represented as a neural network?","pos":61,"slide":"subslide","type":"cell"}
{"cell_type":"markdown","id":"50a117","input":"Consider the cartoonish picture of a biological neuron in :numref:`fig_Neuron`, consisting of *dendrites* (input terminals), the *nucleus* (CPU), the *axon* (output wire),and the *axon terminals* (output terminals), enabling connections to other neurons via *synapses*.\n\n![The real neuron (source: \"Anatomy and Physiology\" by the US National Cancer Institute's Surveillance, Epidemiology and End Results (SEER) Program).](neuron.svg)\n:label:`fig_Neuron`\n\n* Information $x_i$ arriving from other neurons (or environmental sensors) is received in the dendrites.\n\n* In particular, that information is weighted by *synaptic weights* $w_i$, determining the effect of the inputs, e.g., activation or inhibition via the product $x_i w_i$.\n\n* The weighted inputs arriving from multiple sources are aggregated in the nucleus as a weighted sum $y = \\sum_i x_i w_i + b$, possibly subject to some nonlinear postprocessing via a function $\\sigma(y)$.","metadata":{"origin_pos":19},"pos":48,"slide":"subslide","type":"cell"}
{"cell_type":"markdown","id":"54b0de","input":"\n## Summary\n\n* In this section, we introduced traditional linear regression, where the parameters of a linear function are chosen to minimize squared loss on the training set. \n\n* After discussing both computational considerations and connections to statistics, we showed how such linear models could be expressed as simple neural networks where the inputs are directly wired to the output(s). \n\n* While we will soon move past linear models altogether, they are sufficient to introduce most of the components that all of our models require:\n\t1. \tparametric forms, \n\t2. differentiable objectives,\n\t3. optimization via minibatch stochastic gradient descent,\n\t4. and ultimately, evaluation on previously unseen data.\n","metadata":{"origin_pos":21,"tab":["pytorch"]},"pos":51,"slide":"slide","type":"cell"}
{"cell_type":"markdown","id":"58da0e","input":"**QUESTION 1**\n\nWhat is the primary goal of linear regression? Write your answer in the cell below.","pos":53,"slide":"subslide","type":"cell"}
{"cell_type":"markdown","id":"5d4130","input":"When our inputs consist of $d$ features, we can assign each an index (between $1$ and $d$) and express our prediction $\\hat{y}$ (in general the \"hat\" symbol denotes an estimate) as\n$$\\hat{y} = w_1  x_1 + \\cdots + w_d  x_d + b.$$\n\nCollecting all features into a vector $\\mathbf{x} \\in \\mathbb{R}^d$ and all weights into a vector $\\mathbf{w} \\in \\mathbb{R}^d$, we can express our model compactly via the dot product\nbetween $\\mathbf{w}$ and $\\mathbf{x}$:\n$$\\hat{y} =  \\mathbf{x}^\\top\\mathbf{w} + b.\\tag{eq_linreg-y}$$\n:eqlabel:`eq_linreg-y`\n\nThis represents the following vector equation:\n$$\n\\hat{y} =[x_1,x_2,\\ldots,x_d]\\begin{bmatrix} w_1\\\\w_2\\\\ \\ldots\\\\ w_d\\end{bmatrix}+b.\n$$\n\nIn :eqref:`eq_linreg-y`, the vector $\\mathbf{x}$ corresponds to the features of a single example. ","pos":10,"slide":"subslide","type":"cell"}
{"cell_type":"markdown","id":"67aa6f","input":"---\n\n> **NOTE**: For regression problems, the most common loss function is the **squared error**.\n\n---","pos":17,"slide":"subslide","type":"cell"}
{"cell_type":"markdown","id":"6a2d62","input":"### Noise\n\nEven if we believe that the best model for predicting $y$ given $\\mathbf{x}$ is linear, we would not expect to find a real-world dataset of $n$ examples where $y^{(i)}$ exactly equals $ (\\mathbf{x}^{(i)})^\\top \\mathbf{w}+b$ for all $1 \\leq i \\leq n$.\n\n\n* For example, whatever instruments we use to observe the features $\\mathbf{X}$ and labels $\\mathbf{y}$, there might be a small amount of measurement error. \n\n* When we use real word data, that data comes already with errors.\n\n* if we generate artificial data from an equation, we will add an error term to this data.\n\n---\n<div class=\"alert alert-block alert-info\">\n<b>NOTE:</b> Even when we are confident that the underlying relationship is linear, we will incorporate a noise term to account for such errors.\n</div>\n\n---","pos":14,"slide":"slide","type":"cell"}
{"cell_type":"markdown","id":"7291bf","input":"In summary, minibatch SGD proceeds as follows:\n\n1. initialize the values of the model parameters, typically at random;\n2. iteratively sample random minibatches from the data, updating the parameters in the direction of the negative gradient.\n\nFor linear regression and a quadratic loss function, and recalling from :numref:`eq_final` that:\n$$l^{(i)}(\\mathbf{w}, b) =\\frac{1}{2}\\left( \\left(\\mathbf{x}^{(i)}\\right)^\\top\\mathbf{w} + b - y^{(i)}\\right)^2,$$\nwe have the following closed-form expansion after deriving:\n\n$$\n\\begin{aligned} \\mathbf{w} & \\leftarrow \\mathbf{w} - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}_t} \\partial_{\\mathbf{w}} l^{(i)}(\\mathbf{w}, b) && = \\mathbf{w} - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}_t} \\mathbf{x}^{(i)} \\left(\\left(\\mathbf{x}^{(i)}\\right)^\\top\\mathbf{w}  + b - y^{(i)}\\right)\\\\ b &\\leftarrow b -  \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}_t} \\partial_b l^{(i)}(\\mathbf{w}, b) &&  = b - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}_t} \\left(\\left(\\mathbf{x}^{(i)}\\right)^\\top\\mathbf{w} + b - y^{(i)}\\right). \\end{aligned} %\\tag{eq_linreg_batch_update}\n$$\n:eqlabel:`eq_linreg_batch_update`","pos":32,"slide":"subslide","type":"cell"}
{"cell_type":"markdown","id":"729428","input":"We can express the update as follows:\n\n$$\n\\text{ Input:} \\\\\\text{ a minibatch } \\mathcal{B}_t \\text{ of training examples,  parameter values } (\\mathbf{w},b)\\text{, and a learning rate } \\eta\\\\\\text{ Process}\\\\ \\text{ for the loss function for the minibatch:}L_{\\mathcal{B}_t}(\\mathbf{w}, b) =\\frac{1}{|\\mathcal{B}|}\\sum_{i \\in \\mathcal{B}_t} l^{(i)}(\\mathbf{w}, b) \\text{ calculate its gradient: } \\\\ \\partial_{(\\mathbf{w},b)} L_{\\mathcal{B}_t}(\\mathbf{w}, b)=\\frac{1}{|\\mathcal{B}|}\\sum_{i \\in \\mathcal{B}_t} \\delta_{(\\mathbf{w},b)}l^{(i)}(\\mathbf{w}, b)  \\\\ \\text{Output: } \\\\ \\text{ update the values of } (\\mathbf{w},b) \\text{ as} \\\\\n(\\mathbf{w},b) \\leftarrow (\\mathbf{w},b) - \\eta\\partial_{(\\mathbf{w},b)} L_{\\mathcal{B}_t}(\\mathbf{w}, b)=\\\\  (\\mathbf{w},b) - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}_t} \\partial_{(\\mathbf{w},b)} l^{(i)}(\\mathbf{w},b).$$","pos":31,"slide":"subslide","type":"cell"}
{"cell_type":"markdown","id":"7a3934","input":"A linear regression model can be represented as a neural network with one input layer and one output layer where the input layer has a variable amount of elements and the output layer has only one.\n","pos":62,"slide":"subslide","type":"cell"}
{"cell_type":"markdown","id":"824588","input":"# Questions","pos":52,"slide":"slide","type":"cell"}
{"cell_type":"markdown","id":"82c236","input":"SDG optimizes based on all inputs at once whereas minibatch SDG optimized smaller chunks at a time which is faster overall.\n","pos":60,"slide":"subslide","type":"cell"}
{"cell_type":"markdown","id":"8709c4","input":"---\n<div class=\"alert alert-block alert-info\">\n<b>NOTE:</b> Large differences between estimates $\\hat{y}^{(i)}$ and targets $y^{(i)}$ lead to even larger contributions to the loss, due to its quadratic form, thus quadraticity can be a double-edge sword; while it encourages the model to avoid large errors it can also lead to excessive sensitivity to anomalous data.\n</div>\n\n---\n","pos":19,"slide":"subslide","type":"cell"}
{"cell_type":"markdown","id":"8dd201","input":"### Example Model\n\n:label:`subsec_linear_model`\n\n> **HYPOTHESIS**: For this example we will assume that there is a **linear** relationship between the prices of houses and the area and age of a house.\n\n* The assumption of linearity means that the expected value of the target (price) can be expressed as a weighted sum of the features (area and age):\n$$\n\\textrm{price} = w_{\\textrm{area}} \\cdot \\textrm{area} + w_{\\textrm{age}} \\cdot \\textrm{age} + b.\\tag{eq_price}\n$$\n:eqlabel:`eq_price-area`\n\nHere $w_{\\textrm{area}}$ and $w_{\\textrm{age}}$ are called *weights*, and $b$ is called a *bias* (or *offset* or *intercept*).\n\n* The weights determine the influence of each feature on our prediction.\n* The bias determines the value of the estimate when all features are zero.\n\n","pos":6,"slide":"slide","type":"cell"}
{"cell_type":"markdown","id":"9201fb","input":"---\n\n<div class=\"alert alert-block alert-info\">\n<b>GOAL:</b> Given our labeled dataset, our goal is to choose the weights $w_{\\textrm{area}},w_{\\textrm{age}}$ and the bias $b$ so that, on average, our model's predictions fit the true prices observed in the data as closely as possible.\n</div>\n\n---","pos":8,"slide":"subslide","type":"cell"}
{"cell_type":"markdown","id":"96e572","input":"When our prediction for an example $i$ is $\\hat{y}^{(i)}$\nand the corresponding true label is $y^{(i)}$,\nthe *squared error* is given by:\n\n$$l^{(i)}(\\mathbf{w}, b) = \\frac{1}{2} \\left(\\hat{y}^{(i)} - y^{(i)}\\right)^2.$$\n\n:eqlabel:`eq_mse`\n\n* The constant $\\frac{1}{2}$ makes no real difference but proves to be notationally convenient, since it cancels out when we take the derivative of the loss.\n\n* Because the training dataset is given to us, and thus is out of our control, the empirical error is only a function of the model parameters.\n\n* In :numref:`fig_fit_linreg`, we visualize the fit of a linear regression model in a problem with one-dimensional inputs.\n\n![Fitting a linear regression model to one-dimensional data.](fit-linreg.svg)\n:label:`fig_fit_linreg`\n\n","pos":18,"slide":"subslide","type":"cell"}
{"cell_type":"markdown","id":"986a3f","input":"#### Minibatching and Stochastic Gradient Descent\n\n* The most naive application of gradient descent consists of taking the gradient (derivative) of the loss function, which is an average of the losses computed on every single example in the dataset.\n\n* In practice, this can be **extremely slow**: we must pass over the entire dataset before making a single update, even if the update steps might be very powerful :cite:`Liu.Nocedal.1989`.\n\n* Even worse, if there is a lot of *redundancy* in the training data, the benefit of a full update is limited.","pos":27,"slide":"subslide","type":"cell"}
{"cell_type":"markdown","id":"9f9036","input":"To measure the quality of a model on the entire dataset of $n$ examples, we simply average (or equivalently, sum) the losses on the training set:\n\n$$\nL(\\mathbf{w}, b) =\\frac{1}{n}\\sum_{i=1}^n l^{(i)}(\\mathbf{w}, b) =  \\frac{1}{n} \\sum_{i=1}^n \\frac{1}{2} \\left(\\hat{y}^{(i)} - y^{(i)}\\right)^2     =\\\\ \\frac{1}{n} \\sum_{i=1}^n \\frac{1}{2}\\left( (\\mathbf{x}^{(i)})^\\top\\mathbf{w} + b - y^{(i)}\\right)^2.\n$$\n:eqlabel:`eq_final`\n\n(Here recall from Equation eq_linreg-y above that for every index $i$: $\\hat{y}^{(i)}=(\\mathbf{x}^{(i)})^\\top\\mathbf{w}+ b$ ).\n\nWhen training the model, we seek parameters ($\\mathbf{w}^*, b^*$)\nthat minimize the total loss across all training examples:\n\n$$\\mathbf{w}^*, b^* = \\operatorname*{argmin}_{\\mathbf{w}, b}\\  L(\\mathbf{w}, b).$$","pos":20,"slide":"subslide","type":"cell"}
{"cell_type":"markdown","id":"a12c74","input":"### Analytic Solution\n\nUnlike most of the models that we will cover, linear regression presents us with a surprisingly easy optimization problem. \n\n* In particular, we can find the optimal parameters (as assessed on the training data) analytically by applying a simple formula as follows. \n\n>The advantage is that we can now compare the solution that we obtain using neural networks , with the exact solution generated by the mathematical formula.","pos":21,"slide":"slide","type":"cell"}
{"cell_type":"markdown","id":"a14452","input":"##### Steps in Minibatch Stochastic Gradient Descent\n\n* In its most basic form, in each iteration $t$, we first randomly sample a minibatch $\\mathcal{B}_t$ consisting of a fixed number $|\\mathcal{B}|$ of training examples. \n* We then compute the derivative (gradient) of the average loss on the minibatch with respect to the model parameters. Finally, we multiply the gradient by a predetermined small positive value $\\eta$, called the *learning rate*, and subtract the resulting term from the current parameter values.","pos":30,"slide":"subslide","type":"cell"}
{"cell_type":"markdown","id":"a19ab2","input":"### Loss Function\n\n:label:`subsec_linear-regression-loss-function`\n\nNaturally, fitting our model to the data requires that we agree on some measure of *fitness* (or, equivalently, of *unfitness*).\n\n* *Loss functions* quantify the distance between the *real* and *predicted* values of the target ($\\mathbf{y}$ and $\\hat{\\mathbf{y}}$). The loss will usually be a nonnegative number where smaller values are better and perfect predictions incur a loss of 0.\n","pos":16,"slide":"slide","type":"cell"}
{"cell_type":"markdown","id":"a318fa","input":"---\n\n<div class=\"alert alert-block alert-info\">\n<b>REFORMULATED GOAL:</b> Under this vectorial presentation of the Linear Regression Problem, the goal is to find the weight vector $\\mathbf{w}$ and the bias term $b$ such that, given the dataset $\\mathbf{X}, \\mathbf{y}$, the error between the predicted labels $\\hat{\\mathbf{y}}$ and the given labels $\\mathbf{y}$ is small.\n</div>\n\n---","pos":13,"slide":"subslide","type":"cell"}
{"cell_type":"markdown","id":"a3dceb","input":"\n![The real neuron (source: \"Anatomy and Physiology\" by the US National Cancer Institute's Surveillance, Epidemiology and End Results (SEER) Program).](neuron.svg)\n:label:`fig_Neuron`\n\n\n* This information is then sent via the axon to the axon terminals, where it reaches its destination(e.g., an actuator such as a muscle) or it is fed into another neuron via its dendrites.\n\n* Certainly, the high-level idea that many such units could be combined, provided they have the correct connectivity and learning algorithm,to produce far more interesting and complex behavior than any one neuron alone could express arises from our study of real biological neural systems.","pos":49,"slide":"subslide","type":"cell"}
{"cell_type":"markdown","id":"a54502","input":"The loss function calculates the error between actual values and calculated values. By minimizing this loss the model can get better at predicting values.\n","pos":56,"slide":"subslide","type":"cell"}
{"cell_type":"markdown","id":"a8cc25","input":"## Linear Regression as a Neural Network\n\nWhile linear models are not sufficiently rich to express the many complicated networks that we will introduce in this book, (artificial) neural networks are rich enough to subsume linear models as networks\nin which every feature is represented by an input neuron, all of which are connected directly to the output.","pos":45,"slide":"slide","type":"cell"}
{"cell_type":"markdown","id":"aeeb72","input":"* Frequently minibatch size and learning rate are user-defined. \n\n* Such tunable parameters that are not updated in the training loop are called *hyperparameters*. They can be tuned automatically by a number of techniques, such as Bayesian optimization :cite:`Frazier.2018`.\n\n* In the end, the quality of the solution is typically assessed on a separate *validation dataset* (or *validation set*).\n\n* After training for some predetermined number of iterations (or until some other stopping criterion is met), we record the estimated model parameters, denoted $\\hat{\\mathbf{w}}, \\hat{b}$.\n\n* Note that even if our function is truly linear and noiseless, these parameters will not be the exact minimizers of the loss, nor even deterministic. \n\n\t* Although the algorithm converges slowly towards the minimizers it typically will not find them exactly in a finite number of steps. \n    \n    * Moreover, the minibatches $\\mathcal{B}$ used for updating the parameters are chosen at random.This breaks determinism.","pos":34,"slide":"subslide","type":"cell"}
{"cell_type":"markdown","id":"b3dbe4","input":"First, we can subsume the bias $b$ into the parameter $\\mathbf{w}$ by appending a column to the design matrix consisting of all 1s.\n$$\n{\\hat{\\mathbf{y}}} = \\mathbf{X} \\mathbf{w} + b=\\begin{bmatrix} x^1_1 & x^1_2 & \\ldots &x^1_d\\\\ x^2_1 & x^2_2 & \\ldots &x^2_d\\\\ \\ldots &\\ldots & \\ldots &\\ldots\\\\x^n_1 & x^n_2 & \\ldots &x^n_d \\end{bmatrix}\\begin{bmatrix} w_1\\\\w_2\\\\\\ldots\\\\w_d\\end{bmatrix}+b=\\begin{bmatrix} x^1_1 & x^1_2 & \\ldots &x^1_d & 1\\\\ x^2_1 & x^2_2 & \\ldots &x^2_d & 1\\\\ \\ldots &\\ldots & \\ldots &\\ldots\\\\x^n_1 & x^n_2 & \\ldots &x^n_d & 1\\end{bmatrix}\\begin{bmatrix} w_1\\\\w_2\\\\\\ldots\\\\w_d\\\\ b\\end{bmatrix}.\n$$\n\nThen our prediction problem is to minimize the sum of all the quadratic errors  $l^{(i)}(\\mathbf{w}, b)$ for each one of the data elements $(i)$ (see equation eq_mse above).","pos":22,"slide":"subslide","type":"cell"}
{"cell_type":"markdown","id":"b78dff","input":"## How to find the parameters $\\mathbf{w},b$ in the Linear Regression Problem?\n\nWe already have:\n1. the **data** for this problem ($\\mathbf{X}, \\mathbf{y}$)\n2. the **model** \n\t$$\\begin{bmatrix} \\hat{y}^{(1)}\\\\ \\hat{y}^{(2)}\\\\ \\ldots \\\\ \\hat{y}^{(n)}\\end{bmatrix}={\\hat{\\mathbf{y}}} = \\mathbf{X} \\mathbf{w} + b=\\begin{bmatrix} x^1_1 & x^1_2 & \\ldots &x^1_d\\\\ x^2_1 & x^2_2 & \\ldots &x^2_d\\\\ \\ldots &\\ldots & \\ldots &\\ldots\\\\x^n_1 & x^n_2 & \\ldots &x^n_d \\end{bmatrix}\\begin{bmatrix} w_1\\\\w_2\\\\\\ldots\\\\w_d\\end{bmatrix}+b,$$\n\nwe will need two more things:\n\n3. a measure of the quality of some given model (**loss function**); \n4. and  a procedure for updating the model to improve its quality (**optimization algorithm**).\n\nWe will discuss these item in the next subsections.\n","pos":15,"slide":"slide","type":"cell"}
{"cell_type":"markdown","id":"c599de","input":"### Vectorization of Linear Regression\n\n* In disciplines where it is common to focus on datasets with just a few features,explicitly expressing models long-form, as in :eqref:`eq_price`, is common.\n\n* In machine learning, we usually work with high-dimensional datasets, where it is more convenient to employ compact linear algebra notation. ","pos":9,"slide":"subslide","type":"cell"}
{"cell_type":"markdown","id":"c61b32","input":"> **NOTE**: Even though we will never see any newly-built homes with precisely zero area, we still need the bias because it allows us to express all linear functions of our features (rather than restricting us to lines that pass through the origin).\n\nWe are also given a dataset of $n$ features and labels of the form:\n$$\n((area^1,age^1), price^{(1)})\\\\\n((area^2,age^2), price^{(2)})\\\\\n\\ldots\\\\\n((area^n,age^n), price^{(n)})\\\\\n$$\n","pos":7,"slide":"subslide","type":"cell"}
{"cell_type":"markdown","id":"c881f6","input":"### Minibatch Stochastic Gradient Descent\n\nFortunately, even in cases where we cannot solve the models analytically, we can still often train models effectively in practice. \n\n* Moreover, for many tasks, those hard-to-optimize models turn out to be so much better that figuring out how to train them ends up being well worth the trouble.\n","pos":24,"slide":"slide","type":"cell"}
{"cell_type":"markdown","id":"d2adf5","input":"**QUESTION 3**\n\nHow does stochastic gradient descent (SGD) help in finding the optimal parameters in linear regression? Write your answer in the cell below.\n\n","pos":57,"slide":"subslide","type":"cell"}
{"cell_type":"markdown","id":"daadb0","input":"> **Conclusion** The second method is dramatically faster than the first.\n\n* Vectorizing code often yields order-of-magnitude speedups. \n* Moreover, we push more of the mathematics to the library so we do not have to write as many calculations ourselves, reducing the potential for errors and increasing portability of the code.","metadata":{"origin_pos":14},"pos":44,"slide":"subslide","type":"cell"}
{"cell_type":"markdown","id":"e31116","input":"\n* Typically, we will use $n$ to denote the number of examples in our dataset. \n* We use superscripts to enumerate samples and targets, and subscripts to index coordinates.\n\n* More concretely, if $\\mathbf{x}^{(i)}=(x_1^i,x_2^i,\\ldots,x_d^i)$ denotes the $i^{\\textrm{th}}$ sample of the data, then $x_j^{(i)}$ denotes its $j^{\\textrm{th}}$ coordinate.","pos":4,"slide":"subslide","type":"cell"}
{"cell_type":"markdown","id":"e391b8","input":"To predict an output value based on a one or more input variables, using a linear equation.\n","pos":54,"slide":"subslide","type":"cell"}
{"cell_type":"markdown","id":"e3fdd6","input":"SDG is an optimization algorithm which minimizes the error in a linear regression problem.\n","pos":58,"slide":"subslide","type":"cell"}
{"cell_type":"markdown","id":"f0937f","input":"## Basics\n\n---\n> **Observation**: *Linear regression* is both the simplest and most popular among the standard tools for tackling regression problems.\n\n---","pos":2,"slide":"slide","type":"cell"}
{"cell_type":"markdown","id":"f189df","input":"\n* As long as the design matrix $\\mathbf{X}$ has full rank (no feature is linearly dependent on the others), then it can be proved that there will be just one critical point on the loss surface and it corresponds to the minimum of the loss over the entire domain.\n\n* Using multidimensional calculus it can be proved that the value of $\\mathbf{w}$ that minimize the quadratic error above is:\n$$\\mathbf{w}^* = (\\mathbf X^\\top \\mathbf X)^{-1}\\mathbf X^\\top \\mathbf{y}$$\nand this value will be **unique** when the matrix $\\mathbf X^\\top \\mathbf X$ is invertible, i.e., when the columns of the design matrix are linearly independent :cite:`Golub.Van-Loan.1996`.\n\n> While simple problems like linear regression may admit analytic solutions, you should not get used to such good fortune. \n\n* Although analytic solutions allow for nice mathematical analysis, the requirement of an analytic solution is so restrictive that it would exclude almost all exciting aspects of deep learning.","pos":23,"slide":"subslide","type":"cell"}
{"cell_type":"markdown","id":"f282a9","input":"\n* Given a loss function $L(\\mathbf{w}, b)$ and a set of parameters $\\mathbf{w}^*,b^*$, we start at the point $\\mathbf{w}^*,b^*$ and calculate the direction (in the plane $\\mathbf{w},b$) in which the loss function decreases the most, by using derivatives to calculate the **gradient vector**. \n\n* We adjust the values of $\\mathbf{w}^*,b^*$ to move in the direction opposite to the gradient, so that the loss function decreases. We iteratively repear this process until we reach a minimun value of the error.\n\nSee [here](https://www.geogebra.org/m/c9n3vdnr) for an animation of the process.","pos":26,"slide":"subslide","type":"cell"}
{"cell_type":"markdown","id":"f52d90","input":"Now we can benchmark the workloads.\nFirst, we add them, one coordinate at a time,\nusing a for-loop.\n","metadata":{"origin_pos":8},"pos":40,"slide":"subslide","type":"cell"}
{"cell_type":"markdown","id":"f63491","input":"**END OF WORKSHEET**\n\nMake sure that you answered all the questions on time. This completed `Jupyter Notebook` will be collected and graded. \n\nOnce the `Jupyter Notebook` is collected it can not be modified.","pos":63,"slide":"slide","type":"cell"}
{"cell_type":"markdown","id":"f9595b","input":"\n\nWe will often find it convenient to refer to features of our entire dataset of $n$ examples\nvia the *design matrix* $\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$.\n$$\n\\mathbf{X}=\\begin{bmatrix} x^1_1 & x^1_2 & \\ldots &x^1_d\\\\ x^2_1 & x^2_2 & \\ldots &x^2_d\\\\ \\ldots &\\ldots & \\ldots &\\ldots\\\\x^n_1 & x^n_2 & \\ldots &x^n_d \\end{bmatrix}=\\begin{bmatrix} \\mathbf{x}^1\\\\\\mathbf{x}^2 \\\\ \\ldots \\\\ \\mathbf{x}^n\\end{bmatrix}\n$$\n\n* Here, $\\mathbf{X}$ contains one row for every example and one column for every feature of our dataset. \n* For a design matrix $\\mathbf{X}$, the predictions $\\hat{\\mathbf{y}} \\in \\mathbb{R}^n$ can be expressed via the matrix--vector product:\n\n$$\\begin{bmatrix} \\hat{y}^{(1)}\\\\ \\hat{y}^{(2)}\\\\ \\ldots \\\\ \\hat{y}^{(n)}\\end{bmatrix}={\\hat{\\mathbf{y}}} = \\mathbf{X} \\mathbf{w} + b=\\begin{bmatrix} x^1_1 & x^1_2 & \\ldots &x^1_d\\\\ x^2_1 & x^2_2 & \\ldots &x^2_d\\\\ \\ldots &\\ldots & \\ldots &\\ldots\\\\x^n_1 & x^n_2 & \\ldots &x^n_d \\end{bmatrix}\\begin{bmatrix} w_1\\\\w_2\\\\\\ldots\\\\w_d\\end{bmatrix}+b,$$\n\n:eqlabel:`eq_linreg-y-vec`","pos":11,"slide":"subslide","type":"cell"}
{"cell_type":"markdown","id":"fb444a","input":"---\n> **NOTE**: Since we pick a minibatch $\\mathcal{B}$ we need to normalize by its size $|\\mathcal{B}|$. \n\n---","pos":33,"slide":"subslide","type":"cell"}
{"cell_type":"markdown","id":"fc8913","input":"* At the same time, most research in deep learning today draws inspiration from a much wider source. \n\n\t* We invoke :citet:`Russell.Norvig.2016` who pointed out that although airplanes might have been *inspired* by birds, ornithology has not been the primary driver of aeronautics innovation for some centuries. \n    * Likewise, inspiration in deep learning these days comes in equal or greater measure from mathematics, linguistics, psychology, statistics, computer science, and many other fields.","pos":50,"slide":"subslide","type":"cell"}
{"cell_type":"markdown","id":"ffe802","input":"# Linear Regression\n\n:label:`sec_linear_regression`\n\n> *Regression* problems pop up whenever we want to predict a numerical value given some input.\n\n* Usually, the format is that you have a vector of inputs $\\mathbf{x}=(x_1,x_2,\\ldots,x_d)$ and a function $y=f(\\mathbf{x})$, so that for every input $\\mathbf{x}$ we get an output $y$.\n* Common examples include predicting prices (of homes, stocks, etc.), predicting the length of stay (for patients in the hospital),forecasting demand (for retail sales), among numerous others.\n\n---\n<div class=\"alert alert-block alert-info\">\n<b>NOTE:</b> Not every prediction problem is one of classical regression. Later on, we will introduce classification problems, where the goal is to predict membership among a set of categories.\n</div>\n\n---","metadata":{"origin_pos":1},"pos":0,"slide":"slide","type":"cell"}
{"id":0,"time":1731007859874,"type":"user"}
{"last_load":1730834361158,"type":"file"}