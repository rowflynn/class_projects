{"args":["--to","cocalc-html"],"start":1733419853388,"state":"done","time":1733419853886,"type":"nbconvert"}
{"backend_state":"running","connection_file":"/tmp/xdg-runtime-user/jupyter/kernel-5cbb48bd-c304-4be5-a4e0-ba7190c56039.json","kernel":"python3","kernel_error":"","kernel_state":"idle","kernel_usage":{"cpu":0,"memory":0},"last_backend_state":1733419791097,"last_ipynb_save":1733441840371,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"trust":true,"type":"settings"}
{"cell_type":"code","end":1733419810914,"exec_count":3,"id":"5570e2","input":"import matplotlib.pyplot as plt  #for plotting our graphs\n%matplotlib inline\nimport random\nimport torch\nfrom torch import nn\n#The torch.nn namespace provides all the building blocks you need to build your own neural network. Every module in PyTorch subclasses the nn.Module. A neural network is a module itself that consists of other modules (layers).\n#from d2l import torch as d2l# the command from d2l import torch as d2l is used to import the torch module from the d2l package/module and refer to it as d2l in the rest of the code for brevity or convenience.\n","kernel":"python3","last":10090,"no_halt":true,"pos":6,"slide":"subslide","start":1733419791323,"state":"done","type":"cell"}
{"cell_type":"code","end":1733419811361,"exec_count":4,"id":"677e33","input":"#True parameters to be obtained after training\nw1 = torch.tensor([2, -3.4])\nb1 = 4.2","kernel":"python3","last":12,"no_halt":true,"pos":10,"start":1733419811226,"state":"done","type":"cell"}
{"cell_type":"code","end":1733419812189,"exec_count":5,"id":"0b9d32","input":"n = 2000  #Size of X\nstandev = 0.01  #Standard deviation of the normal distribution centered at 0.\nX = torch.randn(n, len(w1))  #Creates a random matrix X of size (n by len(w))\nnoise = torch.randn(n, 1) * standev  # creates a random matrix of size (n by 1)\ny = torch.matmul(\n    X, w1.reshape((-1, 1))) + b1 + noise  #w.reshape(-1,1) is w^T, the column vector version of the row vector w.","kernel":"python3","last":344,"no_halt":true,"pos":12,"slide":"subslide","start":1733419811396,"state":"done","type":"cell"}
{"cell_type":"code","end":1733419812497,"exec_count":6,"id":"d01ba4","input":"print('Shape of X:', X.shape)\nprint('Shape of y:', y.shape)\nprint('Shape of w:', w1.shape)\nprint('Some particular values:X[0]=', X[0], '\\ny[0]:', y[0],\n      '\\nnumber of training data:', n)","kernel":"python3","last":275,"no_halt":true,"output":{"0":{"name":"stdout","text":"Shape of X: torch.Size([2000, 2])\nShape of y: torch.Size([2000, 1])\nShape of w: torch.Size([2])\nSome particular values:X[0]= "},"1":{"name":"stdout","text":"tensor([-1.1204, -0.1320]) \ny[0]: tensor([2.4201]) \nnumber of training data: 2000\n"}},"pos":15,"start":1733419812212,"state":"done","type":"cell"}
{"cell_type":"code","end":1733419812541,"exec_count":7,"id":"451884","input":"# we first generate a TensorDataset variable for the input data\n#X,y presented as a list.\ndataset = torch.utils.data.TensorDataset(X, y)\n# now we create the iterator that will extract batches from\n# the data\nbatch_size = 32  # we define the batch size\n#The call below pases the data set, the batch size and request that the batches are shuffled randomly from the whole dataset.\ndataloader = torch.utils.data.DataLoader(dataset, batch_size, shuffle=True)\n#once we create the variable dataloader,we can iterated over\n#all of the batches.\nprint(\"number of batches in dataloader:\", len(dataloader))\n#Note that number of batches in dataloader * batch size is approximately equal\n# to the number of items in the data set.\n#Below is an example of a typical batch\nnewbatchX, newbatchy = next(\n    iter(dataloader)\n)  # call tensorloader function to select a batch of the trainig dataset X and y\nprint('newbatchX shape:', newbatchX.shape, '\\nnewbatchy shape:',\n      newbatchy.shape)\nprint(\"Length of batch:\", len(newbatchy))\nprint(\"first value of X:\", newbatchX[0])\nprint(\"first value of y_hat:\", newbatchy[0])\n#we get the next batch\nnewbatchX, newbatchy = next(iter(dataloader))  # call tensorloader function\nprint('newbatchX shape:', newbatchX.shape, '\\nnewbatchy shape:',\n      newbatchy.shape)\nprint(\"first value of batch X:\", newbatchX[0])\nprint(\"first value of batch y_yhat:\", newbatchy[0])","kernel":"python3","last":109,"no_halt":true,"output":{"0":{"name":"stdout","text":"number of batches in dataloader: 63\nnewbatchX shape: torch.Size([32, 2]) \nnewbatchy shape: torch.Size([32, 1])\nLength of batch: 32\nfirst value of X: tensor([-1.1751,  0.9408])\nfirst value of y_hat: tensor([-1.3597])\nnewbatchX shape: torch.Size([32, 2]) \nnewbatchy shape: torch.Size([32, 1])\nfirst value of batch X: tensor([0.5019, 0.6739])\nfirst value of batch y_yhat: tensor([2.9259])\n"}},"pos":23,"slide":"subslide","start":1733419812515,"state":"done","type":"cell"}
{"cell_type":"code","end":1733419812600,"exec_count":8,"id":"593b23","input":"#we define a linear neural network. We do not need to define the size of the input yet. This neural network will have one layer.\nLinearModel = nn.LazyLinear(1)  #This layer will have output dimension 1\nLinearModel.weight.data.normal_(\n    0, 0.01)  #(We initialize the parameters (weights) of the neural network\n#randomly, from a normal distribution with mean 0 and standard deviation 0.01.\nLinearModel.bias.data.fill_(\n    0)  # We initialize the array of b (the bias) with zeros.\n# We can see the values of the parameters w and b of this neural network\n#by using the method parameters() as shown below\nparameters = LinearModel.parameters()\n# Print the parameters of the neural network\nfor parameter in parameters:\n    print(parameter\n          )  # Note that at this point the parameters have not been initialized","kernel":"python3","last":39,"no_halt":true,"output":{"0":{"name":"stdout","text":"<UninitializedParameter>\n<UninitializedParameter>\n"}},"pos":27,"slide":"subslide","start":1733419812554,"state":"done","type":"cell"}
{"cell_type":"code","end":1733419812637,"exec_count":9,"id":"38e420","input":"LinearModel.eval()  # We make sure that the neural network is in evaluation mode\nprint(\n    \"We evaluate now the output of the neural network for an input of [1.0,0.0]\"\n)\n#input of the  neural network\ninput = torch.tensor([[1.0, 0.0],[1.0, 1.0]])\n#here is how we calculate the output: w_1*x_1+w_2*x_0+b=w_1+b\noutput = LinearModel(input)\nprint(\"Output is=\", output)  #Note that the output of the neural network is as\n# expected w_1+b\n#Now we check the parameters of the neural network\nparameters = LinearModel.parameters()\n# Print the parameters of the neural network\nfor parameter in parameters:\n    print(parameter)  #Prints w and b","kernel":"python3","last":36,"no_halt":true,"output":{"0":{"name":"stdout","text":"We evaluate now the output of the neural network for an input of [1.0,0.0]\nOutput is= tensor([[-0.0030],\n        [-0.1939]], grad_fn=<AddmmBackward0>)\nParameter containing:\ntensor([[ 0.3150, -0.1909]], requires_grad=True)\nParameter containing:\ntensor([-0.3179], requires_grad=True)\n"}},"pos":31,"slide":"subslide","start":1733419812614,"state":"done","type":"cell"}
{"cell_type":"code","end":1733419812812,"exec_count":10,"id":"5c86d0","input":"def loss(y_hat, y):  # Define function loss\n    fn = nn.MSELoss()\n    return fn(y_hat, y)\n\n\n# Here is an example of the calculation of the average loss over a batch\ny2 = torch.tensor([[1.0], [1.0], [1.0]])  # 3 labels in this made up batch\ny2_hat = torch.tensor([[0.5], [0.5], [0.5]\n                       ])  #3 corresponding calculated values for the batch\nprint(\"loss for batch is:\", loss(y2_hat, y2))","kernel":"python3","last":24,"no_halt":true,"output":{"0":{"name":"stdout","text":"loss for batch is: tensor(0.2500)\n"}},"pos":33,"slide":"subslide","start":1733419812685,"state":"done","type":"cell"}
{"cell_type":"code","end":1733419822505,"exec_count":11,"id":"839a20","input":"#We select to optimize the loss function using the SGD method\n# lr is the learning rate. This is the amount by which you multiply each one of the partial derivatives of the gradient to substract from the parameters values.\n#We use the command optim.GEd to optimize the parameters of the linear model.\nLinearOptimizer = torch.optim.SGD(LinearModel.parameters(), lr=0.01)","kernel":"python3","last":4974,"no_halt":true,"pos":37,"slide":"subslide","start":1733419812822,"state":"done","type":"cell"}
{"cell_type":"code","end":1733419824105,"exec_count":12,"id":"440cf6","input":"# Set the model to training mode!\nLinearModel.train()\n# Initialize a list to store the loss values for each epoch\ntotal_loss = []\n#we define the max number of epochs:\nmax_epochs = 5\n#We loop over each epoch\nfor epoch in range(max_epochs):  # Adjust number of epochs as needed\n    #initialize loss_value to 0\n    loss_value = 0.0\n    #for every epoch we iterate over all the batches in dataloader\n    for batch_idx, (batchX, batchy) in enumerate(dataloader):\n        LinearOptimizer.zero_grad()  # Clear gradients before each batch\n        #Forward pass\n        y_hat = LinearModel(batchX)\n        # Calculate loss\n        lossbatch = loss(y_hat, batchy)\n        # Backward pass and update weights\n        lossbatch.backward()\n        LinearOptimizer.step()\n        #Add to loss_value the value of lossbatch\n        loss_value = loss_value + lossbatch\n        #End Loop\n    # We calculate now the average loss for the epoch\n    epoch_loss = loss_value.item() / len(dataloader)\n    #we append the average loss to the list of epoch losses\n    total_loss.append(epoch_loss)\n    #End of Epoch\n# we plot now the graph of total_loss values against the epochs.\nplt.plot(total_loss, label='Training Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n\n# finally, we print the optimized parameters of the linear neural network and compare them with the target parameters.\nparameters = LinearModel.parameters()\n# Print the parameters of the neural network\nprint(\"Final Values of parameters of the linear neural network\")\nfor parameter in parameters:\n    print(parameter)\nprint(\"target value of w:\", w1, \"target value of b:\", b1)","kernel":"python3","last":1011,"no_halt":true,"output":{"0":{"data":{"image/png":"125b30b16a070726b715378ba36c167718246712","text/plain":"<Figure size 1200x700 with 1 Axes>"},"metadata":{"image/png":{"height":602,"width":996}}},"1":{"name":"stdout","text":"Final Values of parameters of the linear neural network\nParameter containing:\ntensor([[ 1.9983, -3.3951]], requires_grad=True)\nParameter containing:\ntensor([4.1925], requires_grad=True)\ntarget value of w: tensor([ 2.0000, -3.4000]) target value of b: 4.2\n"}},"pos":41,"slide":"subslide","start":1733419822522,"state":"done","type":"cell"}
{"cell_type":"code","end":1733419825075,"exec_count":13,"id":"031aa8","input":"# Write your code here\n\n\n#we define a linear neural network. We do not need to define the size of the input yet. This neural network will have one layer.\nLinearModel1 = nn.LazyLinear(1)  #This layer will have output dimension 1\nLinearModel1.weight.data.fill_(0)  #(We initialize the parameters (weights) of the neural network\nLinearModel1.bias.data.fill_(0)  # We initialize the array of b (the bias) with zeros.\n\n    \nLinearOptimizer1 = torch.optim.SGD(LinearModel1.parameters(), lr=0.01)\n    \n# Initialize a list to store the loss values for each epoch\ntotal_loss1 = []\n#we define the max number of epochs:\nmax_epochs = 5\n#We loop over each epoch\nfor epoch in range(max_epochs):  # Adjust number of epochs as needed\n    #initialize loss_value to 0\n    loss_value = 0.0\n    #for every epoch we iterate over all the batches in dataloader\n    for batch_idx, (batchX, batchy) in enumerate(dataloader):\n        LinearOptimizer1.zero_grad()  # Clear gradients before each batch\n        #Forward pass\n        y_hat = LinearModel1(batchX)\n        # Calculate loss\n        lossbatch = loss(y_hat, batchy)\n        # Backward pass and update weights\n        lossbatch.backward()\n        LinearOptimizer1.step()\n        #Add to loss_value the value of lossbatch\n        loss_value = loss_value + lossbatch\n        #End Loop\n    # We calculate now the average loss for the epoch\n    epoch_loss = loss_value.item() / len(dataloader)\n    #we append the average loss to the list of epoch losses\n    total_loss1.append(epoch_loss)\n    #End of Epoch\n# we plot now the graph of total_loss values against the epochs.\nplt.plot(total_loss1, label='Training Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n\n# finally, we print the optimized parameters of the linear neural network and compare them with the target parameters.\nparameters = LinearModel1.parameters()\n# Print the parameters of the neural network\nprint(\"Final Values of parameters of the linear neural network\")\nfor parameter in parameters:\n    print(parameter)\nprint(\"target value of w:\", w1, \"target value of b:\", b1)","kernel":"python3","last":859,"no_halt":true,"output":{"0":{"data":{"image/png":"7a20bffda55396a5ddc4fbf27f55279ed84d9c62","text/plain":"<Figure size 1200x700 with 1 Axes>"},"metadata":{"image/png":{"height":602,"width":996}}},"1":{"name":"stdout","text":"Final Values of parameters of the linear neural network\nParameter containing:\ntensor([[ 1.9987, -3.3937]], requires_grad=True)\nParameter containing:\ntensor([4.1928], requires_grad=True)\ntarget value of w: tensor([ 2.0000, -3.4000]) target value of b: 4.2\n"}},"pos":43,"slide":"subslide","start":1733419824114,"state":"done","type":"cell"}
{"cell_type":"code","end":1733419825956,"exec_count":14,"id":"b40b91","input":"# Write your code here\n\n\n#we define a linear neural network. We do not need to define the size of the input yet. This neural network will have one layer.\nLinearModel2 = nn.LazyLinear(1)  #This layer will have output dimension 1\nLinearModel2.weight.data.fill_(100)  #(We initialize the parameters (weights) of the neural network\nLinearModel2.bias.data.fill_(0)  # We initialize the array of b (the bias) with zeros.\n\n    \nLinearOptimizer2 = torch.optim.SGD(LinearModel1.parameters(), lr=0.01)\n    \n# Initialize a list to store the loss values for each epoch\ntotal_loss2 = []\n#we define the max number of epochs:\nmax_epochs = 5\n#We loop over each epoch\nfor epoch in range(max_epochs):  # Adjust number of epochs as needed\n    #initialize loss_value to 0\n    loss_value = 0.0\n    #for every epoch we iterate over all the batches in dataloader\n    for batch_idx, (batchX, batchy) in enumerate(dataloader):\n        LinearOptimizer2.zero_grad()  # Clear gradients before each batch\n        #Forward pass\n        y_hat = LinearModel2(batchX)\n        # Calculate loss\n        lossbatch = loss(y_hat, batchy)\n        # Backward pass and update weights\n        lossbatch.backward()\n        LinearOptimizer2.step()\n        #Add to loss_value the value of lossbatch\n        loss_value = loss_value + lossbatch\n        #End Loop\n    # We calculate now the average loss for the epoch\n    epoch_loss = loss_value.item() / len(dataloader)\n    #we append the average loss to the list of epoch losses\n    total_loss2.append(epoch_loss)\n    #End of Epoch\n# we plot now the graph of total_loss values against the epochs.\nplt.plot(total_loss2, label='Training Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n\n\n# finally, we print the optimized parameters of the linear neural network and compare them with the target parameters.\nparameters = LinearModel2.parameters()\n# Print the parameters of the neural network\nprint(\"Final Values of parameters of the linear neural network\")\nfor parameter in parameters:\n    print(parameter)\nprint(\"target value of w:\", w1, \"target value of b:\", b1)","kernel":"python3","last":813,"no_halt":true,"output":{"0":{"data":{"image/png":"85aefdaf83ef3a1769a19bd9e841cb147af96c12","text/plain":"<Figure size 1200x700 with 1 Axes>"},"metadata":{"image/png":{"height":602,"width":1019}}},"1":{"name":"stdout","text":"Final Values of parameters of the linear neural network\nParameter containing:\ntensor([[ 0.6955, -0.4565]], requires_grad=True)\nParameter containing:\ntensor([-0.6165], requires_grad=True)\ntarget value of w: tensor([ 2.0000, -3.4000]) target value of b: 4.2\n"}},"pos":43.5,"start":1733419825085,"state":"done","type":"cell"}
{"cell_type":"code","end":1733419826556,"exec_count":15,"id":"a37436","input":"LinearOptimizer3.zero_grad()\n\n\n#we define a linear neural network. We do not need to define the size of the input yet. This neural network will have one layer.\nLinearModel3 = nn.LazyLinear(1)  #This layer will have output dimension 1\nLinearModel3.weight.data.normal_(0,0.1)  #(We initialize the parameters (weights) of the neural network\nLinearModel3.bias.data.fill_(0)  # We initialize the array of b (the bias) with zeros.\n\n    \nLinearOptimizer3 = torch.optim.SGD(LinearModel3.parameters(), lr=1)\n    \n# Initialize a list to store the loss values for each epoch\ntotal_loss3 = []\n#we define the max number of epochs:\nmax_epochs = 40\n#We loop over each epoch\nfor epoch in range(max_epochs):  # Adjust number of epochs as needed\n    #initialize loss_value to 0\n    loss_value = 0.0\n    #for every epoch we iterate over all the batches in dataloader\n    for batch_idx, (batchX, batchy) in enumerate(dataloader):\n        LinearOptimizer3.zero_grad()  # Clear gradients before each batch\n        #Forward pass\n        y_hat = LinearModel3(batchX)\n        # Calculate loss\n        lossbatch = loss(y_hat, batchy)\n        # Backward pass and update weights\n        lossbatch.backward()\n        LinearOptimizer3.step()\n        #Add to loss_value the value of lossbatch\n        loss_value = loss_value + lossbatch\n        #End Loop\n    # We calculate now the average loss for the epoch\n    epoch_loss = loss_value.item() / len(dataloader)\n    #we append the average loss to the list of epoch losses\n    total_loss3.append(epoch_loss)\n    #End of Epoch\n# we plot now the graph of total_loss values against the epochs.\nplt.plot(total_loss3, label='Training Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n\n# finally, we print the optimized parameters of the linear neural network and compare them with the target parameters.\nparameters = LinearModel3.parameters()\n# Print the parameters of the neural network\nprint(\"Final Values of parameters of the linear neural network\")\nfor parameter in parameters:\n    print(parameter)\nprint(\"target value of w:\", w1, \"target value of b:\", b1)\n\n","kernel":"python3","last":99,"no_halt":true,"output":{"0":{"ename":"NameError","evalue":"name 'LinearOptimizer3' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mLinearOptimizer3\u001b[49m\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#we define a linear neural network. We do not need to define the size of the input yet. This neural network will have one layer.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m LinearModel3 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLazyLinear(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m#This layer will have output dimension 1\u001b[39;00m\n","\u001b[0;31mNameError\u001b[0m: name 'LinearOptimizer3' is not defined"]}},"pos":45,"slide":"subslide","start":1733419825963,"state":"done","type":"cell"}
{"cell_type":"code","end":1733419826578,"exec_count":16,"id":"b1ccab","input":"import torch\n\nclass LinearRegressionDataset(torch.utils.data.Dataset):\n    \"\"\"\n  Custom Dataset class for generating linear regression data.\n\n  This class inherits from `torch.utils.data.Dataset` and provides methods\n  to generate data points for training a linear regression model.\n  \"\"\"\n\n    def __init__(self, n, w1, b1, standev=0.01):\n        \"\"\"\n    Initializes the dataset with the following parameters:\n\n    - n (int): Number of data points to generate.\n    - w1 (torch.Tensor): True weight vector for the linear model.\n    - b1 (float): True bias term for the linear model.\n    - standev (float, optional): Standard deviation of the noise added \n                                  to the data (default: 0.01).\n    \"\"\"\n        self.n = n\n        self.w1 = w1\n        self.b1 = b1\n        self.standev = standev\n\n    def __len__(self):\n        \"\"\"\n    Returns the total number of data points in the dataset.\n    \"\"\"\n        return self.n\n\n    def __getitem__(self, idx):\n        \"\"\"\n    Returns a single data point (X, y) at the specified index.\n\n    - idx (int): Index of the data point to be retrieved.\n\n    Returns:\n      - X (torch.Tensor): A tensor representing a single data point (feature vector).\n      - y (torch.Tensor): A tensor representing the corresponding target value.\n    \"\"\"\n        X = torch.randn(1, len(self.w1))  # Generate random feature vector\n        noise = torch.randn(1, 1) * self.standev  # Generate random noise\n        y = torch.matmul(X, self.w1.reshape(\n            (-1, 1))) + self.b1 + noise  # Calculate target value\n        return X, y","kernel":"python3","last":5,"no_halt":true,"pos":48,"slide":"subslide","start":1733419826564,"state":"done","type":"cell"}
{"cell_type":"code","end":1733419826669,"exec_count":17,"id":"fb2983","input":"# write you answer here.\nn = 1000\nw1 = torch.tensor([-2.0,1.5])\nb1 = 0.5\nstandev = 0.01\n\ndataset = LinearRegressionDataset(n,w1,b1,standev)\n\nlength = dataset.__len__()\nprint(\"Length of dataset: \",length)\n\nxitem = dataset.__getitem__(100)\nprint(\"Items at index 100: \",xitem)\n\nfor x in range(dataset.__len__()):\n    xitem = dataset.__getitem__(x)\n","kernel":"python3","last":49,"no_halt":true,"output":{"0":{"name":"stdout","text":"Length of dataset:  1000\nItems at index 100:  (tensor([[-0.3042, -1.2328]]), tensor([[-0.7471]]))\n"}},"pos":53,"slide":"subslide","start":1733419826607,"state":"done","type":"cell"}
{"cell_type":"code","end":1733419826688,"exec_count":18,"id":"90e83b","input":"\nclass LinearRegressionModel(nn.Module):\n    \"\"\"\n    Linear regression model using nn.LazyLinear for lazy parameter initialization.\n    \"\"\"\n\n    def __init__(self, input_features, output_features=1):\n        \"\"\"\n        Initializes the linear regression model.\n\n        Args:\n            input_features (int): Number of input features.\n            output_features (int, optional): Number of output features (default: 1).\n        \"\"\"\n        super().__init__()\n        self.linear = nn.LazyLinear(output_features)\n\n    def forward(self, X):\n        \"\"\"\n        Forward pass through the linear model.\n\n        Args:\n            X (torch.Tensor): Input tensor.\n\n        Returns:\n            torch.Tensor: Output tensor.\n        \"\"\"\n        return self.linear(X)","kernel":"python3","last":10,"no_halt":true,"pos":56,"slide":"subslide","start":1733419826683,"state":"done","type":"cell"}
{"cell_type":"code","end":1733419826710,"exec_count":19,"id":"d10096","input":"#Write your example here.\ninput_features = 2\nmodel = LinearRegressionModel(input_features)\n\nX = torch.randn(10,input_features)\n\ny_hat = model(X)\n\nprint(\"Output:\", y_hat)\nprint(\"Shape of y_hat:\", y_hat.size())\n\nmodel.eval()\n\nweight = model.linear.weight\nbias = model.linear.bias\nprint(\"Weight:\", weight)\nprint(\"Bias:\", bias)\n\nX_new = torch.randn(5, 2)\ny_hat_new = model(X_new)\nprint(\"Output with the new input dimensions:\", y_hat_new)\nprint(\"Shape of y_hat_new:\", y_hat_new.shape)","kernel":"python3","last":12,"no_halt":true,"output":{"0":{"name":"stdout","text":"Output: tensor([[ 0.2590],\n        [-0.2257],\n        [ 0.1091],\n        [-0.6540],\n        [ 0.3766],\n        [ 0.5023],\n        [-0.0625],\n        [ 0.3945],\n        [ 0.0442],\n        [-0.4084]], grad_fn=<AddmmBackward0>)\nShape of y_hat: torch.Size([10, 1])\nWeight: Parameter containing:\ntensor([[-0.3309,  0.2289]], requires_grad=True)\nBias: Parameter containing:\ntensor([-0.0441], requires_grad=True)\nOutput with the new input dimensions: tensor([[ 0.5168],\n        [ 0.0795],\n        [ 0.6253],\n        [-0.6816],\n        [-0.1833]], grad_fn=<AddmmBackward0>)\nShape of y_hat_new: torch.Size([5, 1])\n"}},"pos":62,"slide":"subslide","start":1733419826700,"state":"done","type":"cell"}
{"cell_type":"code","end":1733419826720,"exec_count":20,"id":"b40163","input":"class MSELossWrapper:\n    \"\"\"\n  Wrapper class for nn.MSELoss.\n\n  This class encapsulates the `nn.MSELoss` function, providing a simplified interface\n  and potentially additional functionalities.\n  \"\"\"\n\n    def __init__(self):\n        \"\"\"\n    Initializes the MSELossWrapper.\n    \"\"\"\n        self.criterion = nn.MSELoss()  # Create an instance of nn.MSELoss\n\n    def __call__(self, y_hat, y):\n        \"\"\"\n    Calls the encapsulated MSELoss function.\n\n    Args:\n      y_hat (torch.Tensor): Predicted values.\n      y (torch.Tensor): Target values.\n\n    Returns:\n      torch.Tensor: Calculated mean squared error loss.\n    \"\"\"\n        return self.criterion(y_hat,\n                              y)  # Call the encapsulated MSELoss function","kernel":"python3","last":4,"no_halt":true,"pos":65,"slide":"subslide","start":1733419826715,"state":"done","type":"cell"}
{"cell_type":"code","end":1733419826730,"exec_count":21,"id":"2c4c7f","input":"# Write your answer here\nloss_wrapper = MSELossWrapper()\n\ny_hat = torch.randn(5, 1)\ny = torch.randn(5, 1)\n\nloss = loss_wrapper(y_hat, y)\nprint(\"Calculated loss:\", loss)","kernel":"python3","last":7,"no_halt":true,"output":{"0":{"name":"stdout","text":"Calculated loss: tensor(0.6903)\n"}},"pos":71,"slide":"subslide","start":1733419826724,"state":"done","type":"cell"}
{"cell_type":"code","end":1733419826742,"exec_count":22,"id":"b31542","input":"class SGDOptimizer:\n    \"\"\"\n  Wrapper class for torch.optim.SGD.\n\n  This class encapsulates the `torch.optim.SGD` optimizer, providing a simplified interface\n  and potentially additional functionalities.\n  \"\"\"\n\n    def __init__(self, model_params, lr=0.01):\n        \"\"\"\n    Initializes the SGDOptimizer.\n\n    Args:\n      model_params (iterable): An iterable containing the model parameters to be optimized.\n      lr (float, optional): Learning rate for the optimizer (default: 0.01).\n    \"\"\"\n        self.optimizer = torch.optim.SGD(\n            model_params, lr=lr)  # Create an instance of SGD optimizer\n\n    def zero_grad(self):\n        \"\"\"\n    Zeros the gradients of the optimized parameters.\n    \"\"\"\n        self.optimizer.zero_grad()\n\n    def step(self):\n        \"\"\"\n    Performs a single optimization step, updating the model parameters.\n    \"\"\"\n        self.optimizer.step()","kernel":"python3","last":8,"no_halt":true,"pos":73,"slide":"subslide","start":1733419826737,"state":"done","type":"cell"}
{"cell_type":"code","end":1733419826776,"exec_count":23,"id":"08388d","input":"#Write your code here\n\nnum_epochs = 1000\nbatch_size = 32\nlr = 0.01\n   \ndataloader = torch.utils.data.DataLoader(dataset, batch_size)\nloss_wrapper = MSELossWrapper()\n\nmodel.train()\n    \noptimizer = SGDOptimizer(model.linear.parameters(), lr=0.01)\n    \nbatchX, batchy = next(iter(dataloader))\n    \noptimizer.zero_grad()\n    \ny_hat = model(batchX)\n    \nloss = loss_wrapper(y_hat, batchy)\n\nloss.backward()\n    \noptimizer.step()\n","kernel":"python3","last":10,"no_halt":true,"pos":80,"slide":"subslide","start":1733419826755,"state":"done","type":"cell"}
{"cell_type":"code","end":1733419828268,"exec_count":24,"id":"dad025","input":"\n#####DEFINING PARAMETERS###########################\nn = 2000  #Size of X\n#True parameters to be used to generate the dataset\nw1 = torch.tensor([2, -3.4])\nb1 = 4.2\nbatch_size=32\n###################################################\n##Create dataset##\ndataset = LinearRegressionDataset(n, w1, b1)\n# This line creates a custom dataset class that generates linear regression data points.\n# It takes parameters 'n' (number of data points), 'w1' (true weight vector), and 'b1' (true bias term).\n###################################################\n##Create dataloader**\ndataloader = torch.utils.data.DataLoader(dataset, batch_size)\n# This line creates a data loader that efficiently iterates over the dataset in batches.\n# 'batch_size' specifies the number of data points processed together in each iteration.\n###################################################\n# Create model\nmodel = LinearRegressionModel(len(w1))\n# This line creates the linear regression model using the LinearRegressionModel class.\n# It takes the length of the weight vector ('len(w1)') as input to define the model architecture.\n###################################################\n##Create loss function##\nloss_fn = MSELossWrapper()\n# This line creates an instance of the MSELossWrapper class. This class encapsulates the\n# PyTorch mean squared error loss function ('nn.MSELoss') and provides a simplified interface.\n###################################################\n##Create optimizer##\noptimizer = SGDOptimizer(model.parameters())\n# This line creates an instance of the SGDOptimizer class. This class encapsulates the\n# PyTorch SGD optimizer ('torch.optim.SGD') and provides a simplified interface for updating\n# the model parameters during training. It takes the model parameters retrieved by 'model.parameters()' as input.\n###################################################\n##Training loop##\ntotal_loss = []\nfor epoch in range(max_epochs):\n  loss_value = 0.0\n  for batch_idx, (batchX, batchy) in enumerate(dataloader):\n    optimizer.zero_grad()\n    # Clear the gradients accumulated from the previous iteration\n\n    y_hat = model(batchX)\n    # Perform the forward pass of the model on the current batch (batchX) to get predictions (y_hat)\n\n    loss = loss_fn(y_hat, batchy)\n    # Calculate the loss between the predictions (y_hat) and the true targets (batchy) using the loss function\n\n    loss.backward()\n    # Backpropagate the loss to calculate gradients with respect to the model parameters\n\n    optimizer.step()\n    # Update the model parameters based on the calculated gradients and the learning rate\n\n    loss_value += loss.item()\n  epoch_loss = loss_value / len(dataloader)\n  total_loss.append(epoch_loss)\n\n###################################################\n##Plotting Error**\n# we plot now the graph of total_loss values against the epochs.\nplt.plot(total_loss, label='Training Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n###################################################\n##Print final parameters$$\nprint(\"Final Values of parameters of the linear neural network\")\nfor parameter in model.parameters():\n  print(parameter)\n#We compare tis to the target values of the parameters.\nprint(\"Target Values of the Parameters\\n\")\nprint(\"target value of w:\", w1, \"target value of b:\", b1)\n# Print the final learned values of the model parameters (weights and bias) and compare them \n# to the target values (w1 and b1) used for generating the data.","kernel":"python3","no_halt":true,"output":{"0":{"data":{"image/png":"b24b6df3cda4b45d8b6a7a39c2915ae3e4cc8601","text/plain":"<Figure size 1200x700 with 1 Axes>"},"metadata":{"image/png":{"height":602,"width":996}}},"1":{"name":"stdout","text":"Final Values of parameters of the linear neural network\nParameter containing:\ntensor([[ 1.9965, -3.3948]], requires_grad=True)\nParameter containing:\ntensor([4.1928], requires_grad=True)\nTarget Values of the Parameters\n\ntarget value of w: tensor([ 2.0000, -3.4000]) target value of b: 4.2\n"}},"pos":84,"slide":"subslide","start":1733419826785,"state":"done","type":"cell"}
{"cell_type":"code","end":1733419829539,"exec_count":25,"id":"980dd9","input":"#Write your code here\n\n#####DEFINING PARAMETERS###########################\nn = 2000  #Size of X\n#True parameters to be used to generate the dataset\nw1 = torch.tensor([2, -3.4])\nb1 = 4.2\nbatch_size=32\n###################################################\n##Create dataset##\ndataset = LinearRegressionDataset(n, w1, b1)\n# This line creates a custom dataset class that generates linear regression data points.\n# It takes parameters 'n' (number of data points), 'w1' (true weight vector), and 'b1' (true bias term).\n###################################################\n##Create dataloader**\ndataloader = torch.utils.data.DataLoader(dataset, batch_size)\n# This line creates a data loader that efficiently iterates over the dataset in batches.\n# 'batch_size' specifies the number of data points processed together in each iteration.\n###################################################\n# Create model\nmodel = LinearRegressionModel(len(w1))\n# This line creates the linear regression model using the LinearRegressionModel class.\n# It takes the length of the weight vector ('len(w1)') as input to define the model architecture.\n###################################################\n##Create loss function##\nloss_fn = MSELossWrapper()\n# This line creates an instance of the MSELossWrapper class. This class encapsulates the\n# PyTorch mean squared error loss function ('nn.MSELoss') and provides a simplified interface.\n###################################################\n##Create optimizer##\noptimizer = SGDOptimizer(model.parameters())\n# This line creates an instance of the SGDOptimizer class. This class encapsulates the\n# PyTorch SGD optimizer ('torch.optim.SGD') and provides a simplified interface for updating\n# the model parameters during training. It takes the model parameters retrieved by 'model.parameters()' as input.\n###################################################\n##Training loop##\ntotal_loss = []\nfor epoch in range(max_epochs):\n  loss_value = 0.0\n  for batch_idx, (batchX, batchy) in enumerate(dataloader):\n    optimizer.zero_grad()\n    # Clear the gradients accumulated from the previous iteration\n\n    y_hat = model(batchX)\n    # Perform the forward pass of the model on the current batch (batchX) to get predictions (y_hat)\n\n    loss = loss_fn(y_hat, batchy)\n    # Calculate the loss between the predictions (y_hat) and the true targets (batchy) using the loss function\n\n    loss.backward()\n    # Backpropagate the loss to calculate gradients with respect to the model parameters\n\n    optimizer.step()\n    # Update the model parameters based on the calculated gradients and the learning rate\n\n    loss_value += loss.item()\n  epoch_loss = loss_value / len(dataloader)\n  print(\"Epoch loss: \",epoch_loss)\n  total_loss.append(epoch_loss)\n\n###################################################\n##Plotting Error**\n# we plot now the graph of total_loss values against the epochs.\nplt.plot(total_loss, label='Training Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n###################################################\n##Print final parameters$$\nprint(\"Final Values of parameters of the linear neural network\")\nfor parameter in model.parameters():\n  print(parameter)\n#We compare tis to the target values of the parameters.\nprint(\"Target Values of the Parameters\\n\")\nprint(\"target value of w:\", w1, \"target value of b:\", b1)\n# Print the final learned values of the model parameters (weights and bias) and compare them \n# to the target values (w1 and b1) used for generating the data.","kernel":"python3","last":1116,"no_halt":true,"output":{"0":{"name":"stdout","text":"Epoch loss:  13.934555628943064\nEpoch loss:  1.059441620395297\n"},"1":{"name":"stdout","text":"Epoch loss:  0.08222721017424077\nEpoch loss:  0.006242701999976167\n"},"2":{"name":"stdout","text":"Epoch loss:  0.0006409213367280447\n"},"3":{"data":{"image/png":"79884337502ecd988660165ebb11360177beaf55","text/plain":"<Figure size 1200x700 with 1 Axes>"},"metadata":{"image/png":{"height":602,"width":996}}},"4":{"name":"stdout","text":"Final Values of parameters of the linear neural network\nParameter containing:\ntensor([[ 1.9968, -3.3931]], requires_grad=True)\nParameter containing:\ntensor([4.1915], requires_grad=True)\nTarget Values of the Parameters\n\ntarget value of w: tensor([ 2.0000, -3.4000]) target value of b: 4.2\n"}},"pos":86,"slide":"subslide","start":1733419828277,"state":"done","type":"cell"}
{"cell_type":"code","id":"ada2eb","input":"","pos":88,"type":"cell"}
{"cell_type":"markdown","id":"0580b7","input":"## Key Ingredients to the Training of a Neural Network\n\nHere are the key ingredients:\n1. A neural model $\\mathbf{M}(\\mathbf{w},b)$ containing weights ($\\mathbf{w}$) and bias ($b$).\n\n1. A training data set ($(\\mathbf{X},\\mathbf{y})$).\n\n   - From the training data, a collection of batch data is generated.\n\n2. A loss function $L(\\mathbf{w},b)$ that measures the error between the predicted data $\\hat{\\mathbf{y}}=\\mathbf{X}\\mathbf{w}+b$ and the label data $\\mathbf{y}$.\n\n3. The **Batch Stochastic Gradient Descent** is implemented. \n\n\t- This process uses a batch of the data and then calculates the gradient (derivative) of the batch loss function with respect to the parameters $\\mathbf{w},b$.\n\n   - For this process the values of $\\mathbf{w},b$ are updated by substracting an multiple of the gradient of the loss function. The multiple is the parameter $\\eta$, the learning rate.\n","pos":4,"slide":"slide","state":"done","type":"cell"}
{"cell_type":"markdown","id":"08e0cb","input":"Let's have a look at the tensors in our definition","pos":14,"slide":"subslide","state":"done","type":"cell"}
{"cell_type":"markdown","id":"0af545","input":"# Generating the Training Data\n\nFor this example, we will work in low dimension for succinctness.","pos":7,"slide":"slide","state":"done","type":"cell"}
{"cell_type":"markdown","id":"0c9a92","input":"- We want to generate 1,000 examples with 2-dimensional features drawn from a standard normal distribution. The resulting design matrix $\\mathbf{X}$ belongs to $\\mathbb{R}^{1000 \\times 2}$.\n- We generate each label by applying a _ground truth_ linear function, corrupting them via additive noise $\\boldsymbol{\\epsilon}$, drawn independently and identically for each example:\n\n**$$\\mathbf{y}= \\mathbf{X} \\mathbf{w}_1 + b_1 + \\boldsymbol{\\epsilon}.$$**\n\nFor convenience we assume that $\\boldsymbol{\\epsilon}$ is drawn\nfrom a normal distribution with mean $\\mu= 0$\nand standard deviation $\\sigma = 0.01$.","pos":8,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"0dcfd8","input":"\nClearly, this may be impossible, so we settle by asking to find the values of $\\mathbf{w},b$ that **minimizes** the error between $\\mathbf{y}$(data given) and $$\\mathbf{y}_{hat}=\\mathbf{X} \\mathbf{w} + b$$.\n\n> We will find these values of $\\mathbf{w},b$ by a process that we may call the _training of a neural network_.","pos":3,"slide":"subslide","state":"done","type":"cell"}
{"cell_type":"markdown","id":"10bb8d","input":"> **REMARK** your (synthetic) data for this problem is the pair $(\\mathbf{X},\\mathbf{y})$. The vector $\\mathbf{y}$ is made of the **labels**.","pos":16,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"124a9a","input":"**QUESTION 2** Write a **Pytorch** code snippet that demonstrates how to declare, initialize, and use the methods in the `LinearRegressionDataset` class.","pos":52,"slide":"subslide","state":"done","type":"cell"}
{"cell_type":"markdown","id":"13aa1c","input":"# Defining the Neural Network\n\nOnce we defined the data that will be used to train the neural network, and how we will access that data, we must defined the model to be working with, i.e. the neural network that we will be using.\n\nFor this case, the neural network to be defined is particular class of the neural networks in **Pytorch**.\n\n> Our class for this example will be a `LazyLinear` class.\n\n- This class allows users to specify merely the output dimension, there is no need to additionally asks for how many inputs go into this layer.\n\n- Specifying input shapes is inconvenient and may require nontrivial calculations (such as in convolutional layers). Thus, for simplicity, we will use such \"lazy\" layers whenever we can.\n","pos":25,"slide":"slide","state":"done","type":"cell"}
{"cell_type":"markdown","id":"1bbfc1","input":"Please take a look at the code below:","pos":64,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"1f89c1","input":"\n**Usage in Training Loop**\n\n* Within your training loop:\n\t* Clear gradients using optimizer.zero_grad().\n    * Backpropagate gradients using loss.backward().\n    * Update model parameters using optimizer.step().","pos":81,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"284869","input":"### Optimizer\n\nWe create a wrapper class named `SGDOptimizer` that encapsulates the `torch.optim.SGD optimizer`.\n\nPlease take a look at the code defining this class.","pos":72,"slide":"slide","state":"done","type":"cell"}
{"cell_type":"markdown","id":"2b907f","input":"**QUESTION 0** What would happen if we were to initialize the weights of the model to zero instead of randomly?. Would the training algorithm still work? Write some code that implements this initial set of values of w being zero.","pos":42,"slide":"slide","state":"done","type":"cell"}
{"cell_type":"markdown","id":"2bc60d","input":"Comments on the code below:\n\n`LinearOptimizer.zero_grad()`: This line sets the gradients of all parameters in the optimizer to zero. This is necessary because the gradients accumulate over time, and if they are not reset before each training step, the updates will be based on the gradients from all previous steps, which can lead to unstable training.\n\n`Loss.backward()`: This line computes the gradient of the loss function with respect to the parameters of the model. This is done using a technique called backpropagation, which recursively computes the derivatives of the loss function with respect to all of the parameters in the model.\n\n`LinearOptimizer.step()`: This line updates the parameters of the model based on the gradients that were computed in the previous step. The optimizer uses a specific algorithm to update the parameters, such as stochastic gradient descent (SGD) or Adam. The algorithm takes into account the learning rate, which controls the size of the updates, and any other parameters that are specific to the optimizer.","pos":40,"slide":"subslide","state":"done","type":"cell"}
{"cell_type":"markdown","id":"2e46ee","input":"## Object Oriented Programming Approach to Linear Regression\n\n- Our code for the Linear Regression optimization is an easy one.\n\n- We can take this opportunity to create the same code using OOD.\n\nWe will create the following classes:\n\n| Class Name | Purpose|\n| -------- | ------- |\n| `LinerRegressionDataset` | Data Generation |\n| `LinearRegressionModel` | Linear Model |\n| `MSELossWrapper` | Loss Function |\n| `SGDOptimizer` | Optimizer |","pos":46,"slide":"slide","state":"done","type":"cell"}
{"cell_type":"markdown","id":"32421b","input":"The optimal epochs is 1 there are greatly diminishing returns after that.\n","pos":86.5,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"343f68","input":"## Generating the batch data\n\nRecall that for this training process, we will iteratively execute the _minibatch stochastic gradient descent_ algorithm to minimize our error.\n\n- Recall also that this algorithm is based on taking a minibatch of the training data.\n\n- The specific choice of the size of the said minibatch depends on many factors, such as the amount of memory, the number of accelerators, the choice of layers, and the total dataset size.\n\n- Despite all that, a number between 32 and 256, preferably a multiple of a large power of $2$, is a good start.","pos":17,"slide":"subslide","state":"done","type":"cell"}
{"cell_type":"markdown","id":"354d3e","input":"This neural network is made of a set of weights $\\hat{\\mathbf{w}}$ and a constant $\\hat{b}$ such that for any input data $X^\\prime$,\n\n$$\nX^\\prime=\\begin{pmatrix}\nx_{0} & x_{1}\n\\end{pmatrix}\n$$\n\nthe output of the neural network is going to be scalar $\\hat{y}$:\n\n$$\n\\tag{1}\n\\hat{y}=X^\\prime\\cdot \\hat{\\mathbf{w}}^T+ \\hat{b}=\\\\\n\\begin{pmatrix}\nx_{0} & x_{1}\n\\end{pmatrix}\\cdot \\begin{pmatrix} \\hat{\\mathbf{w}_1}\\\\ \\hat{\\mathbf{w}_2 }\\end{pmatrix}+\\hat{b}\n$$\n\n> The following code defines our linear model in **Pytorch**. Note that despite the model being  defined the random values of the parameters have not been assigned yet.","pos":26,"slide":"subslide","state":"done","type":"cell"}
{"cell_type":"markdown","id":"3f84a8","input":"**NOTE** Up to this points we have the following items for our model:\n\n---\n1. **Data**: The data info `X`, `y` transformed into the `dataloader` iterator that generates our batches.\n2. **The linear model** `LinearModel` defined above\n3. The **loss function** `loss` defined above\n4. The **linear optimizer** `LinearOptimizer` which we selected to be `SGD`.\n5. the **learning rate** `lr=0.01`\n6. The number of **epochs**, `epochs` which the total number of times that we will iterate over all the data on the batches is 5.\n\n---","pos":39,"slide":"subslide","state":"done","type":"cell"}
{"cell_type":"markdown","id":"4296e7","input":"_What is it?_\n\nThe `DataLoader` is an iterable that allows you to load data in batches from a **PyTorch** dataset.\n\n_What does it do?_\n\nThe `DataLoader` does several things:\n\n- It creates a sampler to iterate over the dataset.\n- It splits the dataset into batches according to the batch size.\n- It yields batches of data from the dataset.\n\n_How does it work?_\n\n- The `DataLoader` works by creating a sampler to iterate over the dataset. \n- The sampler can be a random sampler, a sequential sampler, or a custom sampler. \n- The sampler is used to select indices from the dataset. \n- The `DataLoader` then splits the dataset into batches according to the batch size. \n- Finally, the `DataLoader` yields batches of data from the dataset.\n\n","pos":21,"slide":"subslide","state":"done","type":"cell"}
{"cell_type":"markdown","id":"431103","input":"**QUESTION 5**\n\nHow many methods and fields are in the class `MSELossWrapper`? Please answer in the cell below.","pos":67,"slide":"subslide","state":"done","type":"cell"}
{"cell_type":"markdown","id":"44cd68","input":"These ingredients are used in the fitting process for a neural network given a data set by going over the following steps:\n\n- Generate/Obtain the training Data\n- Define the Model\n- Define the loss (fitting) function\n- Define the Optimization process\n- Train\n\n> In this worksheet we will explore how to accomplish this process using **Pytorch**.\n\n* We begin by importing the necessary libraries.","pos":5,"slide":"subslide","state":"done","type":"cell"}
{"cell_type":"markdown","id":"451508","input":"3 methods 0 fields\n","pos":77,"slide":"subslide","state":"done","type":"cell"}
{"cell_type":"markdown","id":"4534c3","input":"# Defining The Optimizer\n\nNow we need to define the optimizing process for the loss function of the neural network.\n\n> As discussed before, all of these methods are based on the _gradient descent_ idea.\n\n- In this particular example, the optimization process will iterate over the values of the parameters of the neural network `LinearModel`.\n\n- **PyTorch** provides several optimization methods for adjusting the parameters of a neural network during training.\n\n- These methods differ in their algorithms and characteristics, making them suitable for different types of models and datasets. Here's an overview of some common optimization methods in torch.optim:\n","pos":34,"slide":"slide","state":"done","type":"cell"}
{"cell_type":"markdown","id":"466a57","input":"\n- **Training mode** is the **default** mode for neural network models. In training mode, the model is constantly learning and updating its weights and biases.\n\n  - This is done using a process called **backpropagation**, which is a method for computing the gradient of the loss function with respect to the weights and biases of the model.\n\n- **Evaluation mode** is used when you want to evaluate the performance of a trained model on a test dataset.\n\n\t- In evaluation mode, the model will not updated its weights and biases. This is because the model has already been trained, and there is no need to continue training it.","pos":29,"slide":"subslide","state":"done","type":"cell"}
{"cell_type":"markdown","id":"472d21","input":"By using a wrapper class, you can:\n\n- Improve code readability: The wrapper class provides a more meaningful name and interface for the optimizer.\n\n- Add flexibility: You can customize the wrapper class to suit your specific needs.\n\n- Maintain modularity: The wrapper class can be reused in different parts of your code.","pos":75,"slide":"subslide","state":"done","type":"cell"}
{"cell_type":"markdown","id":"498493","input":"### Implementing minibatches using Dataloader in Pytorch\n\nWe will use the class `DataLoader ` in the Pytorch module. See [here](https://www.educative.io/answers/what-is-pytorch-dataloader) for a detailed explanation of the use of that class.\n\nThe `DataLoader` class in **PyTorch** is a utility class that is used to load data in batches. It takes a dataset and a batch size as input, and it yields batches of data from the dataset.\n\nHere is a brief overview of the `DataLoader` class:","pos":20,"slide":"subslide","state":"done","type":"cell"}
{"cell_type":"markdown","id":"4a6bcf","input":"","pos":69,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"4f3891","input":"# How to Calculate the Outputs of the Linear Neural Network (_forward pass_)?\n\nGiven any input $X^\\prime$, the neural network `LinearModel` calculates the output given by the equation (1) above.\n\n> **OBSERVATION** In **Pytorch**, neural networks have a particular behavior depending if you are **evaluating it** or **training it**.\n","pos":28,"slide":"subslide","state":"done","type":"cell"}
{"cell_type":"markdown","id":"587c3d","input":"## Model of a Linear Regression Problem\n\nFor a typical linear regression problem we are given a training dataset $(\\mathbf{X},\\mathbf{y})$, where $\\mathbf{X}$ is a matrix $n\\times d$ and $\\mathbf{y}$ is a vector of dimension $n\\times 1$.\n\nWe assume that the data in $\\mathbf{X}$ and $\\mathbf{y}$ are linearly related in the following fashion:\n\n$$\n\\mathbf{y}=\\begin{pmatrix} y_{1} \\\\ y_{2} \\\\ \\ldots \\\\ y_{n}\\end{pmatrix}= \\mathbf{X} \\mathbf{w} + b\\\\\n\\begin{pmatrix}\nx_{1}^{(1)} & x_{2}^{(1)} & \\ldots & x_{d}^{(1)}\\\\\nx_{1}^{(2)} & x_{2}^{(2)} & \\ldots & x_{d}^{(2)}\\\\\n\\ldots\\\\\nx_{1}^{(n)} & x_{1}^{(n)} & \\ldots & x_{d}^{(n)}\n\\end{pmatrix}\\cdot \\begin{pmatrix} w_1 \\\\ w_2 \\\\ \\ldots \\\\ w_{d}\\end{pmatrix}+b\\begin{pmatrix} 1 \\\\ 1 \\\\ \\ldots \\\\ 1\\end{pmatrix}. \\tag{1}\n$$\n\nThe task is to find the values of $\\mathbf{w}$ and $b$ that make the equation (1) true.","pos":2,"slide":"subslide","state":"done","type":"cell"}
{"cell_type":"markdown","id":"62440b","input":"\nThe choice of optimization method depends on several factors, including the complexity of the model, the characteristics of the dataset, and the desired performance.\n\nIt's often beneficial to experiment with different optimization methods to find the one that works best for a specific problem.\n\nFor this example we select SGD.\n\n<details>\n  <summary><b>Detailed Explanation of the SGD code</b></summary>\n\nStochastic Gradient Descent (SGD) is an optimization algorithm commonly used in machine learning to minimize an objective function, especially for large datasets. It's a simplified version of gradient descent that updates the parameters of a neural network in the direction of the steepest descent of the loss function, but instead of using the entire dataset, it uses a small batch of data at each iteration.\n\nAlgorithm:\n\nInitialize the parameters of the neural network: Set the initial values for the weights and biases of the neural network.\n\nIterate over the training data: For each training example, do the following:\n\na. Compute the predicted output: Pass the training example through the neural network to obtain the predicted output.\n\nb. Compute the loss: Compute the loss between the predicted output and the target output.\n\nc. Compute the gradient: Compute the gradient of the loss function with respect to the parameters of the neural network.\n\nd. Update the parameters: Update the parameters of the neural network using the gradient and a learning rate.\n\n</details>\n\nHere is how we define the optimizer for the linear network.","pos":36,"slide":"subslide","state":"done","type":"cell"}
{"cell_type":"markdown","id":"6304c4","input":"# Training a Linear Regression Neural Network\n","pos":0,"slide":"slide","state":"done","type":"cell"}
{"cell_type":"markdown","id":"6482ce","input":"**QUESTION 6**\n\nWrite a **Pytorch** code snippet that demonstrates how to declare, initialize, and use the methods in the `MSELossWrapper` class.","pos":70,"slide":"subslide","state":"done","type":"cell"}
{"cell_type":"markdown","id":"65d00e","input":"**QUESTION 0.0**\n\nWhat will happen if we changed the learning rate? Please run some experiments with this training to see the effects of modifying the learning rate `lr`.\n","pos":44,"slide":"subslide","state":"done","type":"cell"}
{"cell_type":"markdown","id":"6d583b","input":"Answer here\n\n**Fields** \n\n3\n","pos":58,"slide":"subslide","state":"done","type":"cell"}
{"cell_type":"markdown","id":"6e57c9","input":"**QUESTION 7**\n\nHow many methods and fields are in the class `SGDOptimizer`? What do they do? Please answer in the cell below.","pos":76,"slide":"subslide","state":"done","type":"cell"}
{"cell_type":"markdown","id":"721add","input":"**Explanation**\n\n- Encapsulation: The `SGDOptimizer` class encapsulates the `torch.optim.SGD` optimizer within its self.optimizer attribute.\n\n- Simplified Interface: The `zero_grad` and `step` methods provide a simplified interface for using the optimizer.\n\n- Potential for Additional Features: While this basic wrapper class simply calls the encapsulated functions, you could extend it to add features like:\n  - Logging optimization progress\n    - Implementing custom learning rate schedules\n    - Applying gradient clipping\n","pos":74,"slide":"subslide","state":"done","type":"cell"}
{"cell_type":"markdown","id":"72e81c","input":"### Training Loop\n\nNow that we have defined all the important classes for the encapsulation of our linear regression neural network, let us use these classes to run the training of the linear regression neural network with the following parameters:\n\n| Type | Class | Variable |\n|:--------:|:--------:|:--------:|\n|  Data (`X,y`)   |  `LinearRegressionDataSet`  |  `dataset`   |\n| Data Loader | `torch.utils.data.DataLoader` | `dataloader` |\n|  Model   |  `LinearRegressionModel`  |  `model`   |\n|  Loss   |  `MSELossWrapper`   |  `loss_fn`   |\n| SGD Optimization | `SGDOptimizer` | optimizer |\n","pos":82,"slide":"slide","state":"done","type":"cell"}
{"cell_type":"markdown","id":"7bb172","input":"### Loss Function\n\nWe create a wrapper class named `MSELossWrapper` that encapsulates the `nn.MSELoss` function.\n\nWe recall first what is a wrapper.\n\n---\n\n**DEFINITION Python Wrappers**\n\nPython wrappers are functions or classes that can encapsulate, or wrap, the behavior of another function.\n\nWrappers allow an existing function to be modified or extended without changing its core source code.Common applications include monitoring the runtime of function calls or debugging other functions.\n\n---","pos":63,"slide":"slide","state":"done","type":"cell"}
{"cell_type":"markdown","id":"80223f","input":"**Explanation**\n\ndataset.**len**() directly calls the **len** method to get the dataset's length.\n\ndataset.**getitem**(500) directly calls the **getitem** method to retrieve the data point at index 500.\n\nThe for loop iterates over the dataset's length using dataset.**len**() and retrieves data points using dataset.**getitem**(i).\n\n<div class=\"alert alert-block alert-success\">\n<b>NOTE</b> While it's generally more common to use the shorthand syntax (e.g., len(dataset), dataset[i]), directly calling the methods can be useful in specific scenarios, such as when you need to override or customize these methods in a subclass.\n</div>","pos":54,"slide":"subslide","state":"done","type":"cell"}
{"cell_type":"markdown","id":"819b39","input":"**Explanation**\n\n- Encapsulation: The `MSELossWrapper` class encapsulates the `nn.MSELoss` function within its self.criterion attribute.\n\n- Simplified Interface: The `__call__` method provides a simplified interface for using the loss function. You can call it directly on an instance of `MSELossWrapper`, just like calling a function.\n\n- Potential for Additional Features: While this basic wrapper class simply calls the encapsulated function, you could extend it to add features like:\n  - Logging loss values\n    - Applying regularization\n    - Implementing custom reduction strategies\n- By using a wrapper class, you can:\n  - Improve code readability: The wrapper class provides a more meaningful name and interface for the loss function.\n    - Add flexibility: You can customize the wrapper class to suit your specific needs.\n    - Maintain modularity: The wrapper class can be reused in different parts of your code.","pos":66,"slide":"subslide","state":"done","type":"cell"}
{"cell_type":"markdown","id":"87c1f0","input":"- Note that $b_1$ is a scalar, but in the last operation above, **Pytorch** incorporates it as a vector so that the addition in the code above works (broadcasting).","pos":13,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"915075","input":"<div class=\"alert alert-block alert-warning\">\n<b>WARNING!</b> In the PyTorch nn package, the `.eval()` method is used to switch a neural network model from training mode to evaluation mode. This means that the model will no longer perform certain operations that are specifically for training, such as dropout and batch normalization.\n    The `train()` method is used to switch a neural network model from evaluation mode to training mode.\n</div>\nBelow we can see an example of the behavior of the neural network in the evaluation mode.","pos":30,"slide":"subslide","state":"done","type":"cell"}
{"cell_type":"markdown","id":"986178","input":"# Training the Neural Network\n\nThe final step is to **train** our neural network. To do this, we must define first the total number of iterations (**epochs**) to be performed.\n\nFor every **epoch**, we iterate over all the training data in batches:\n\n- For each training batch, we compute the predicted output of the neural network using the forward pass.\n\n- Then, compute the loss between the predicted output and the target output.\n\n- Then, update the parameters of the neural network using the backward pass and the optimizer.","pos":38,"slide":"slide","state":"done","type":"cell"}
{"cell_type":"markdown","id":"9b34a2","input":"Recall that for this example, our training data set is made of $\\mathbf{X},\\mathbf{y}$ with\n\n$$\n\\mathbf{X}=\\begin{pmatrix}\nx_1^{(1)} & x_2^{(1)} \\\\\nx_1^{(2)} & x_2^{(2)} \\\\\n\\ldots\\\\\nx_1^{(2,000)} & x_2^{(2,000)}\n\\end{pmatrix}\n$$\n\nand that the generated (synthetic data) values of $\\mathbf{y}$ will be obtained as:\n\n$$\n\\begin{pmatrix} y^{(1)}\\\\y^{(2)}\\\\ \\ldots \\\\ y^{(2,000)}\\end{pmatrix}=\\mathbf{y}=\\mathbf{X}\\cdot \\mathbf{w_1}^T+ b_1+ noise=\\\\\n\\begin{pmatrix}\nx_1^{(1)} & x_2^{(1)} \\\\\nx_1^{(2)} & x_2^{(2)} \\\\\n\\ldots & \\ldots\\\\\nx_1^{(2,000)} & x_2^{(2,000)}\n\\end{pmatrix}\\cdot \\begin{pmatrix} 2.0\\\\ -3.4 \\end{pmatrix}+(4.2)\\begin{pmatrix} 1 \\\\ 1\\\\ \\ldots \\\\1 \\end{pmatrix}+\\begin{pmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\ldots \\\\ \\epsilon_{2000} \\end{pmatrix}.\n$$\n\nWe define the tensors $\\mathbf{X}$ and $\\mathbf{noise}$, and the result tensor $\\mathbf{y}$ as follows.","pos":11,"slide":"subslide","state":"done","type":"cell"}
{"cell_type":"markdown","id":"9bad11","input":"","pos":78,"slide":"subslide","state":"done","type":"cell"}
{"cell_type":"markdown","id":"9d5474","input":"Write your answer here. 3 methods 4 fields\n","pos":50,"slide":"fragment","state":"done","type":"cell"}
{"cell_type":"markdown","id":"a22756","input":"","pos":51,"slide":"subslide","state":"done","type":"cell"}
{"cell_type":"markdown","id":"a5bf34","input":"**QUESTION 8**\n\nWrite a **Pytorch** code snippet that demonstrates how to declare, initialize, and use all the methods in the  `SGDOptimizer` class.","pos":79,"slide":"subslide","state":"done","type":"cell"}
{"cell_type":"markdown","id":"aa6ac2","input":"*  In this setup we will only make available to the linear regression neural network the values of $\\mathbf{X}$ and $\\mathbf{y}$, called the **training dataset**. \n\n* With this data, the network should generate the values of $\\mathbf{w}_1,b_1$ after **training** with the data.\n\n![image training](training.png)\n\nBelow, we set the true parameters to $\\mathbf{w}_1=[2,3.4]^$ and $b_1=4.2$. Later, we can check our estimated parameters against these ground truth _1values.","pos":9,"slide":"subslide","state":"done","type":"cell"}
{"cell_type":"markdown","id":"aca940","input":"Two methods, no fields.\n","pos":68,"slide":"subslide","state":"done","type":"cell"}
{"cell_type":"markdown","id":"adf45c","input":"## Introduction","pos":1,"slide":"slide","state":"done","type":"cell"}
{"cell_type":"markdown","id":"ae1868","input":"- In its most basic form, in each iteration $t$, we first randomly sample a minibatch $\\mathcal{B}_t$ consisting of a fixed number $|\\mathcal{B}|$ of training examples.\n\n- We then compute the derivative (gradient) of the average loss on the minibatch with respect to the model parameters.\n\n- Finally, we multiply the gradient by a predetermined small positive value $\\eta$, called the _learning rate_, and subtract the resulting term from the current parameter values.\n\n- Recall that we expressed this updated process as:\n  $$(\\mathbf{w},b) \\leftarrow (\\mathbf{w},b) - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}_t} \\partial_{(\\mathbf{w},b)} l^{(i)}(\\mathbf{w},b).$$\n","pos":18,"slide":"subslide","state":"done","type":"cell"}
{"cell_type":"markdown","id":"af6a3c","input":"**QUESTION 3** For this class, how many attributes (also known at fields) are there? How many methods? What does every method do?Please answer in the cell below.","pos":57,"slide":"subslide","state":"done","type":"cell"}
{"cell_type":"markdown","id":"b6f032","input":"**QUESTION 4** Write a **Pytorch** code snippet that demonstrates how to declare, initialize, and use the methods in the `LinearRegressionModel` class.","pos":61,"slide":"subslide","state":"done","type":"cell"}
{"cell_type":"markdown","id":"b7a6d2","input":"In summary, minibatch SGD proceeds as follows:\n\n1. initialize the values of the model parameters, typically at random;\n2. iteratively sample random minibatches from the data, updating the parameters in the direction of the negative gradient.\n\nFor quadratic losses and affine transformations,\nthis has a closed-form expansion:\n$$\\begin{aligned} \\mathbf{w} & \\leftarrow \\mathbf{w} - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}_t} \\partial_{\\mathbf{w}} l^{(i)}(\\mathbf{w}, b) && = \\mathbf{w} - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}_t} \\mathbf{x}^{(i)} \\left(\\mathbf{w}^\\top \\mathbf{x}^{(i)} + b - y^{(i)}\\right)\\\\ b &\\leftarrow b -  \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}_t} \\partial_b l^{(i)}(\\mathbf{w}, b) &&  = b - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}_t} \\left(\\mathbf{w}^\\top \\mathbf{x}^{(i)} + b - y^{(i)}\\right). \\end{aligned}. \\tag{eq\\_linreg\\_batch\\_update}$$\n\nAfter training for some predetermined number of iterations\n(or until some other stopping criterion is met),\nwe record the estimated model parameters,\ndenoted $\\hat{\\mathbf{w}}, \\hat{b}$.","pos":19,"slide":"subslide","state":"done","type":"cell"}
{"cell_type":"markdown","id":"bed94c","input":"Answer here\n\n**Methods** \n\n2\n","pos":59,"slide":"subslide","state":"done","type":"cell"}
{"cell_type":"markdown","id":"bff226","input":"**QUESTION 1** For this class, how many attributes (also know at fields) are there? How many methods? What does every method do?Please answer in the cell below.","pos":49,"slide":"subslide","state":"done","type":"cell"}
{"cell_type":"markdown","id":"cd53f1","input":"**ADDITIONAL EXPLANATION**\n\n1. **Data**: The data info `X`, `y` is transformed into the `dataloader` iterator that generates our batches. This si the same data set as before.\n\n2. the **learning rate** `lr` is the same as before.\n3. The number of **epochs**, `epochs` which the total number of times that we will iterate over all the data on the batches is the same as before.\n\n![image](training2.png)","pos":83,"slide":"subslide","state":"done","type":"cell"}
{"cell_type":"markdown","id":"ce358a","input":"","pos":60,"slide":"subslide","state":"done","type":"cell"}
{"cell_type":"markdown","id":"d7f338","input":"### Linear Model\n\nfor our linear model, we will create a classnamed `LinearRegressionModel` that uses features from `torch.nn.azyLinear` to represent the linear model with a single `nn.LazyLinear` layer.\n\n[Here](https://pytorch.org/docs/stable/generated/torch.nn.LazyLinear.html) is some basic information on the class `nn.LazyLinear layer`.\n\nPlease take a look at the code defining this class.\n\nNote that we directly inherit from `nn.LazyLinear`.\n\nLet's briefly understand `nn.LazyLinear`. It's a lazy module that delays the creation of parameters until the first forward pass. This can be beneficial in certain scenarios, especially when you're unsure about the input dimensions beforehand.","pos":55,"slide":"slide","state":"done","type":"cell"}
{"cell_type":"markdown","id":"e313f8","input":"**QUESTION 9**\n\nWe might ask which number of epochs (`max-epochs`) is the \"optimal\" for our training, in the sense of being the minimal number that obtains a meaningful approximation.\n\nTypically, you want to balance between underfitting (too few epochs) and overfitting (too many epochs). The best epoch number is usually just before the loss starts to plateau or increase due to overfitting.\n\nPlease run the training with the number of epochs from 4 to 10 and plot the final errors. Can you guess from that plot what is an optimal epoch number?","pos":85,"slide":"subslide","state":"done","type":"cell"}
{"cell_type":"markdown","id":"e3792d","input":"Why is it useful?\n\nThe `DataLoader` is useful for loading data in batches because it can improve the performance of training machine learning models. By loading data in batches, you can avoid loading the entire dataset into memory at once. This can save memory and improve the speed of training.\n\nHere is an example of how to use the `DataLoader` class to load data from the `FashionMNIST` dataset:\n\n<details>\n  <summary><b>Example</b></summary>\n\n```Python\nimport torch\nfrom torchvision import datasets, transforms\n\n### Load the FashionMNIST training dataset\ntrain_dataset = datasets.FashionMNIST(root=\"./data\", train=True, download=True)\n\n### Create a transform to convert the images to tensors\ntransform = transforms.ToTensor()\n\n### Create a DataLoader object\ntrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=32, transform=transform)\n\n### Iterate over the DataLoader object and print the first batch of data\nfor batch in train_dataloader:\n    images, labels = batch\n    print(images.shape)\n    print(labels.shape)\n    break\n```\n\n</details>\n\nIn order to run the `DataLoader command`, we need to have defined a training dataset, a batch size and, if necessaty the type of transform because the `DataLoader `object is of type tensor.\n\nThe code below defines the required parameter and the `Dataloader` variable.\n\nThe `TensorDataset` class takes a list of tensors as input, and it stores the tensors in a dataset.","pos":22,"slide":"subslide","state":"done","type":"cell"}
{"cell_type":"markdown","id":"e3ee7d","input":"**END OF WORKSHEET**\n\nMake sure that you answered all the questions on time. This completed `Jupyter Notebook` will be collected and graded. \n\nOnce the `Jupyter Notebook` is collected it can not be modified.","pos":87,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"e41f27","input":"# The Loss Function\n\nIn order to optimize the weights of the neural network, we need to iterate a procedure (the gradient method) that updates the values of the weights and bias in such a way that ot minimizes the error, or loss.\n\nThe **loss function** is calculated by looking at the aggregated difference between the given data ($\\mathbf{y}$) and the corresponding generated data $\\hat{\\mathbf{y}}$, for a given data input $X^\\prime$.\n\nWe calculate this error using a predefined error function in **Pytorch**.\n\n- The `MSELoss` class computes the mean squared error (without the $1/2$ factor in :eqref:`eq_mse`).\n- By default, `MSELoss` returns the average loss over examples.\n\nIt is faster (and easier to use) than implementing our own.\n\nWe will use this in our definition of the loss function.","pos":32,"slide":"slide","state":"done","type":"cell"}
{"cell_type":"markdown","id":"e8643c","input":"> Now that we have the batch of the training data set generated, we move to te second step: define our neural network in **Pytorch**.","pos":24,"slide":"subslide","state":"done","type":"cell"}
{"cell_type":"markdown","id":"f7019c","input":"- **Stochastic Gradient Descent (SGD)**: `SGD` is a widely used optimization method that updates the parameters of the neural network in the direction of the steepest descent of the loss function. It's computationally efficient and works well for simple models and datasets.\n\n- **Momentum** `Momentum` is an extension of `SGD` that adds a momentum term to the weight updates. This helps to smooth out the updates and accelerate convergence, especially in cases where the loss function has a complex landscape.\n\n- **Nesterov Accelerated Gradient Descent (NAG)**: `NAG` is another variant of `SGD` that uses a different momentum scheme to further improve convergence. It often performs better than `SGD` and momentum on convex problems.\n\n- **AdaGrad**: `AdaGrad` is an adaptive learning rate optimizer that adjusts the learning rate for each parameter based on its past gradients. This can help to prevent oscillations and improve convergence for problems with highly variable gradients.\n\n- **RMSProp**: `RMSProp` is another adaptive learning rate optimizer that uses a moving average of the squared gradients to adjust the learning rate. It's similar to `AdaGrad` but often performs better in practice.\n\n- **Adam**: `Adam` is a popular optimization method that combines the adaptive learning rate approach of `AdaGrad` and `RMSProp` with `momentum`. It's generally more stable and faster than `SGD` and can work well for a wide range of models and datasets.\n","pos":35,"slide":"subslide","state":"done","type":"cell"}
{"cell_type":"markdown","id":"fc67ee","input":"### Data Generation\n\nfor our synthetic data generation, we will create a class named `LinearRegressionDataset` that inherits from `torch.utils.data.Dataset` to handle data generation and hold the true parameters (`w1` and `b1`).\n\n[Here](https://pytorch.org/docs/stable/data.html) is some basic information on the class `torch.utils.data.Dataset`.\n\nPlease take a look at the code defining this class.","pos":47,"slide":"slide","state":"done","type":"cell"}
{"id":0,"time":1733419121570,"type":"user"}
{"last_load":1731010157181,"type":"file"}